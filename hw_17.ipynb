{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1hz5V9fiXsawBIupUD_7IAofpEl9RxGVP",
      "authorship_tag": "ABX9TyNzlWwwezemkPvgRIdru/OH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssv273/Neural_Univesity/blob/main/hw_17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4BIn6hKt0i4"
      },
      "source": [
        "from google.colab import files # модуль для загрузки файлов в colab\n",
        "import numpy as np # библиотека для работы с массивами данных\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model # из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input # из кераса загружаем необходимые слои для нейросети\n",
        "from tensorflow.keras.optimizers import RMSprop, Adadelta # из кераса загружаем выбранный оптимизатор\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # загружаем метод ограничения последовательности заданной длиной\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # загружаем токенизатор кераса для обработки текста\n",
        "from tensorflow.keras import utils # загружаем утилиты кераса для one hot кодировки\n",
        "from tensorflow.keras.utils import plot_model # удобный график для визуализации архитектуры модели\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import yaml # импортируем модуль для удобной работы с файлами"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdvN_SMTj7jP",
        "outputId": "b9b35762-595b-46a0-e6cf-dfd147870915"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/Neural_Univesity/hw_17/rus.zip\" -d /content/rus"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Neural_Univesity/hw_17/rus.zip\n",
            "  inflating: /content/rus/rus.txt    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1w9cHqyu-pE"
      },
      "source": [
        "data_path = '/content/rus/rus.txt'\n",
        "num_samples = 30000\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "#     w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(preprocess_sentence(input_text))\n",
        "    target_texts.append(preprocess_sentence(target_text))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJGJA7JfmF0k",
        "outputId": "96043e5c-4504-4f83-f2c5-6ec6574abe42"
      },
      "source": [
        "len(input_texts), len(target_texts)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 30000)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONU-EgbL5TvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af24dc1-248c-46c6-faab-a13d99044e93"
      },
      "source": [
        "input_texts[10], target_texts[10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<start> Run . <end>', '<start> Беги ! <end>')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jESVipX-5TlN"
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = pad_sequences(tensor, padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoyxCcT1ldaV"
      },
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
        "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3bMtv2lliFX",
        "outputId": "75a7ee8c-e871-43c5-87fe-ac1b7eef9496"
      },
      "source": [
        "input_tensor.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9vefEOY5UTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a65f46-df1c-420e-d448-4d349ba283e9"
      },
      "source": [
        "len(inp_lang_tokenizer.word_index)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3645"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x8BKFUJpQ4V"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmXrFCA6mgHo"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ing9GeOVnSd0"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "    \n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ-vvVOhnehP"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "    \n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "    \n",
        "    \n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEUxOa51nkIC"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                 look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFls-myVno2H"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W95aOtWonsVT"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
        "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y7-rryinyLB"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynk5YFRLn3pj"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "# learning_rate = 0.0001\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "  \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIJsH58qn7uI"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2ix3sXGn_4Z"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpFuXEC6oFJ2"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx9GUD0RoM1f"
      },
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "  \n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8uWNNSaokNM"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor)\n",
        "BATCH_SIZE = 512\n",
        "steps_per_epoch = len(input_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hvumKBxoSN9",
        "outputId": "6b6a02ed-1122-4ffb-e858-6de0bb81b9bc"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "for epoch in range(700):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        train_step(inp, tar)\n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 9.2251 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 9.1084 Accuracy 0.0181\n",
            "Epoch 1 Loss 9.0814 Accuracy 0.0252\n",
            "Epoch 2 Batch 0 Loss 8.8483 Accuracy 0.0772\n",
            "Epoch 2 Batch 50 Loss 8.6724 Accuracy 0.0769\n",
            "Epoch 2 Loss 8.6503 Accuracy 0.0769\n",
            "Epoch 3 Batch 0 Loss 8.4677 Accuracy 0.0769\n",
            "Epoch 3 Batch 50 Loss 8.2725 Accuracy 0.0769\n",
            "Epoch 3 Loss 8.2417 Accuracy 0.0769\n",
            "Epoch 4 Batch 0 Loss 7.9695 Accuracy 0.0771\n",
            "Epoch 4 Batch 50 Loss 7.6797 Accuracy 0.1111\n",
            "Epoch 4 Loss 7.6382 Accuracy 0.1145\n",
            "Epoch 5 Batch 0 Loss 7.2848 Accuracy 0.1403\n",
            "Epoch 5 Batch 50 Loss 6.9703 Accuracy 0.1401\n",
            "Epoch 5 Loss 6.9268 Accuracy 0.1401\n",
            "Epoch 6 Batch 0 Loss 6.5565 Accuracy 0.1400\n",
            "Epoch 6 Batch 50 Loss 6.2223 Accuracy 0.1402\n",
            "Epoch 6 Loss 6.1780 Accuracy 0.1402\n",
            "Epoch 7 Batch 0 Loss 5.8436 Accuracy 0.1388\n",
            "Epoch 7 Batch 50 Loss 5.4983 Accuracy 0.1471\n",
            "Epoch 7 Loss 5.4593 Accuracy 0.1488\n",
            "Epoch 8 Batch 0 Loss 5.1314 Accuracy 0.1603\n",
            "Epoch 8 Batch 50 Loss 4.8818 Accuracy 0.1661\n",
            "Epoch 8 Loss 4.8488 Accuracy 0.1667\n",
            "Epoch 9 Batch 0 Loss 4.5309 Accuracy 0.1729\n",
            "Epoch 9 Batch 50 Loss 4.3484 Accuracy 0.1763\n",
            "Epoch 9 Loss 4.3198 Accuracy 0.1770\n",
            "Epoch 10 Batch 0 Loss 4.0434 Accuracy 0.1833\n",
            "Epoch 10 Batch 50 Loss 3.9189 Accuracy 0.1887\n",
            "Epoch 10 Loss 3.8999 Accuracy 0.1890\n",
            "Epoch 11 Batch 0 Loss 3.6519 Accuracy 0.1949\n",
            "Epoch 11 Batch 50 Loss 3.6063 Accuracy 0.1951\n",
            "Epoch 11 Loss 3.5962 Accuracy 0.1954\n",
            "Epoch 12 Batch 0 Loss 3.4809 Accuracy 0.1968\n",
            "Epoch 12 Batch 50 Loss 3.3805 Accuracy 0.2010\n",
            "Epoch 12 Loss 3.3697 Accuracy 0.2014\n",
            "Epoch 13 Batch 0 Loss 3.2406 Accuracy 0.2045\n",
            "Epoch 13 Batch 50 Loss 3.1926 Accuracy 0.2076\n",
            "Epoch 13 Loss 3.1851 Accuracy 0.2079\n",
            "Epoch 14 Batch 0 Loss 3.1153 Accuracy 0.2079\n",
            "Epoch 14 Batch 50 Loss 3.0278 Accuracy 0.2131\n",
            "Epoch 14 Loss 3.0220 Accuracy 0.2133\n",
            "Epoch 15 Batch 0 Loss 2.9047 Accuracy 0.2175\n",
            "Epoch 15 Batch 50 Loss 2.8840 Accuracy 0.2167\n",
            "Epoch 15 Loss 2.8768 Accuracy 0.2171\n",
            "Epoch 16 Batch 0 Loss 2.7392 Accuracy 0.2228\n",
            "Epoch 16 Batch 50 Loss 2.7505 Accuracy 0.2212\n",
            "Epoch 16 Loss 2.7462 Accuracy 0.2212\n",
            "Epoch 17 Batch 0 Loss 2.7144 Accuracy 0.2227\n",
            "Epoch 17 Batch 50 Loss 2.6208 Accuracy 0.2258\n",
            "Epoch 17 Loss 2.6201 Accuracy 0.2258\n",
            "Epoch 18 Batch 0 Loss 2.5788 Accuracy 0.2308\n",
            "Epoch 18 Batch 50 Loss 2.4977 Accuracy 0.2302\n",
            "Epoch 18 Loss 2.4979 Accuracy 0.2301\n",
            "Epoch 19 Batch 0 Loss 2.3802 Accuracy 0.2341\n",
            "Epoch 19 Batch 50 Loss 2.3812 Accuracy 0.2340\n",
            "Epoch 19 Loss 2.3776 Accuracy 0.2341\n",
            "Epoch 20 Batch 0 Loss 2.3335 Accuracy 0.2327\n",
            "Epoch 20 Batch 50 Loss 2.2678 Accuracy 0.2374\n",
            "Epoch 20 Loss 2.2625 Accuracy 0.2376\n",
            "Epoch 21 Batch 0 Loss 2.1723 Accuracy 0.2381\n",
            "Epoch 21 Batch 50 Loss 2.1546 Accuracy 0.2419\n",
            "Epoch 21 Loss 2.1511 Accuracy 0.2417\n",
            "Epoch 22 Batch 0 Loss 2.0460 Accuracy 0.2411\n",
            "Epoch 22 Batch 50 Loss 2.0504 Accuracy 0.2447\n",
            "Epoch 22 Loss 2.0470 Accuracy 0.2449\n",
            "Epoch 23 Batch 0 Loss 1.9996 Accuracy 0.2443\n",
            "Epoch 23 Batch 50 Loss 1.9480 Accuracy 0.2482\n",
            "Epoch 23 Loss 1.9445 Accuracy 0.2483\n",
            "Epoch 24 Batch 0 Loss 1.8745 Accuracy 0.2523\n",
            "Epoch 24 Batch 50 Loss 1.8432 Accuracy 0.2526\n",
            "Epoch 24 Loss 1.8417 Accuracy 0.2525\n",
            "Epoch 25 Batch 0 Loss 1.7720 Accuracy 0.2553\n",
            "Epoch 25 Batch 50 Loss 1.7495 Accuracy 0.2560\n",
            "Epoch 25 Loss 1.7470 Accuracy 0.2562\n",
            "Epoch 26 Batch 0 Loss 1.6717 Accuracy 0.2614\n",
            "Epoch 26 Batch 50 Loss 1.6440 Accuracy 0.2607\n",
            "Epoch 26 Loss 1.6450 Accuracy 0.2605\n",
            "Epoch 27 Batch 0 Loss 1.6164 Accuracy 0.2641\n",
            "Epoch 27 Batch 50 Loss 1.5495 Accuracy 0.2650\n",
            "Epoch 27 Loss 1.5490 Accuracy 0.2650\n",
            "Epoch 28 Batch 0 Loss 1.5234 Accuracy 0.2698\n",
            "Epoch 28 Batch 50 Loss 1.4583 Accuracy 0.2693\n",
            "Epoch 28 Loss 1.4556 Accuracy 0.2693\n",
            "Epoch 29 Batch 0 Loss 1.3319 Accuracy 0.2743\n",
            "Epoch 29 Batch 50 Loss 1.3646 Accuracy 0.2737\n",
            "Epoch 29 Loss 1.3623 Accuracy 0.2741\n",
            "Epoch 30 Batch 0 Loss 1.2439 Accuracy 0.2820\n",
            "Epoch 30 Batch 50 Loss 1.2763 Accuracy 0.2782\n",
            "Epoch 30 Loss 1.2786 Accuracy 0.2783\n",
            "Epoch 31 Batch 0 Loss 1.1533 Accuracy 0.2852\n",
            "Epoch 31 Batch 50 Loss 1.1917 Accuracy 0.2823\n",
            "Epoch 31 Loss 1.1930 Accuracy 0.2824\n",
            "Epoch 32 Batch 0 Loss 1.0682 Accuracy 0.2943\n",
            "Epoch 32 Batch 50 Loss 1.1106 Accuracy 0.2869\n",
            "Epoch 32 Loss 1.1130 Accuracy 0.2868\n",
            "Epoch 33 Batch 0 Loss 1.0084 Accuracy 0.2906\n",
            "Epoch 33 Batch 50 Loss 1.0316 Accuracy 0.2913\n",
            "Epoch 33 Loss 1.0332 Accuracy 0.2910\n",
            "Epoch 34 Batch 0 Loss 0.9634 Accuracy 0.2964\n",
            "Epoch 34 Batch 50 Loss 0.9557 Accuracy 0.2954\n",
            "Epoch 34 Loss 0.9593 Accuracy 0.2948\n",
            "Epoch 35 Batch 0 Loss 0.8694 Accuracy 0.3042\n",
            "Epoch 35 Batch 50 Loss 0.8938 Accuracy 0.2981\n",
            "Epoch 35 Loss 0.8947 Accuracy 0.2979\n",
            "Epoch 36 Batch 0 Loss 0.7845 Accuracy 0.3072\n",
            "Epoch 36 Batch 50 Loss 0.8291 Accuracy 0.3015\n",
            "Epoch 36 Loss 0.8282 Accuracy 0.3013\n",
            "Epoch 37 Batch 0 Loss 0.7327 Accuracy 0.3123\n",
            "Epoch 37 Batch 50 Loss 0.7714 Accuracy 0.3047\n",
            "Epoch 37 Loss 0.7754 Accuracy 0.3041\n",
            "Epoch 38 Batch 0 Loss 0.7195 Accuracy 0.3136\n",
            "Epoch 38 Batch 50 Loss 0.7147 Accuracy 0.3074\n",
            "Epoch 38 Loss 0.7194 Accuracy 0.3066\n",
            "Epoch 39 Batch 0 Loss 0.6268 Accuracy 0.3154\n",
            "Epoch 39 Batch 50 Loss 0.6649 Accuracy 0.3099\n",
            "Epoch 39 Loss 0.6694 Accuracy 0.3094\n",
            "Epoch 40 Batch 0 Loss 0.6015 Accuracy 0.3196\n",
            "Epoch 40 Batch 50 Loss 0.6234 Accuracy 0.3121\n",
            "Epoch 40 Loss 0.6268 Accuracy 0.3116\n",
            "Epoch 41 Batch 0 Loss 0.5581 Accuracy 0.3211\n",
            "Epoch 41 Batch 50 Loss 0.5868 Accuracy 0.3136\n",
            "Epoch 41 Loss 0.5904 Accuracy 0.3132\n",
            "Epoch 42 Batch 0 Loss 0.5130 Accuracy 0.3215\n",
            "Epoch 42 Batch 50 Loss 0.5503 Accuracy 0.3160\n",
            "Epoch 42 Loss 0.5549 Accuracy 0.3153\n",
            "Epoch 43 Batch 0 Loss 0.5115 Accuracy 0.3205\n",
            "Epoch 43 Batch 50 Loss 0.5221 Accuracy 0.3175\n",
            "Epoch 43 Loss 0.5258 Accuracy 0.3167\n",
            "Epoch 44 Batch 0 Loss 0.4716 Accuracy 0.3235\n",
            "Epoch 44 Batch 50 Loss 0.4942 Accuracy 0.3190\n",
            "Epoch 44 Loss 0.4994 Accuracy 0.3182\n",
            "Epoch 45 Batch 0 Loss 0.4203 Accuracy 0.3283\n",
            "Epoch 45 Batch 50 Loss 0.4697 Accuracy 0.3206\n",
            "Epoch 45 Loss 0.4741 Accuracy 0.3198\n",
            "Epoch 46 Batch 0 Loss 0.4043 Accuracy 0.3316\n",
            "Epoch 46 Batch 50 Loss 0.4489 Accuracy 0.3214\n",
            "Epoch 46 Loss 0.4541 Accuracy 0.3206\n",
            "Epoch 47 Batch 0 Loss 0.3939 Accuracy 0.3277\n",
            "Epoch 47 Batch 50 Loss 0.4335 Accuracy 0.3221\n",
            "Epoch 47 Loss 0.4404 Accuracy 0.3212\n",
            "Epoch 48 Batch 0 Loss 0.3935 Accuracy 0.3256\n",
            "Epoch 48 Batch 50 Loss 0.4198 Accuracy 0.3223\n",
            "Epoch 48 Loss 0.4236 Accuracy 0.3218\n",
            "Epoch 49 Batch 0 Loss 0.3612 Accuracy 0.3253\n",
            "Epoch 49 Batch 50 Loss 0.4113 Accuracy 0.3226\n",
            "Epoch 49 Loss 0.4164 Accuracy 0.3221\n",
            "Epoch 50 Batch 0 Loss 0.3479 Accuracy 0.3283\n",
            "Epoch 50 Batch 50 Loss 0.4017 Accuracy 0.3230\n",
            "Epoch 50 Loss 0.4080 Accuracy 0.3223\n",
            "Epoch 51 Batch 0 Loss 0.3319 Accuracy 0.3334\n",
            "Epoch 51 Batch 50 Loss 0.3910 Accuracy 0.3234\n",
            "Epoch 51 Loss 0.3964 Accuracy 0.3230\n",
            "Epoch 52 Batch 0 Loss 0.3294 Accuracy 0.3259\n",
            "Epoch 52 Batch 50 Loss 0.3854 Accuracy 0.3244\n",
            "Epoch 52 Loss 0.3916 Accuracy 0.3233\n",
            "Epoch 53 Batch 0 Loss 0.3487 Accuracy 0.3245\n",
            "Epoch 53 Batch 50 Loss 0.3831 Accuracy 0.3238\n",
            "Epoch 53 Loss 0.3889 Accuracy 0.3233\n",
            "Epoch 54 Batch 0 Loss 0.3286 Accuracy 0.3262\n",
            "Epoch 54 Batch 50 Loss 0.3752 Accuracy 0.3248\n",
            "Epoch 54 Loss 0.3806 Accuracy 0.3240\n",
            "Epoch 55 Batch 0 Loss 0.3191 Accuracy 0.3326\n",
            "Epoch 55 Batch 50 Loss 0.3728 Accuracy 0.3244\n",
            "Epoch 55 Loss 0.3800 Accuracy 0.3236\n",
            "Epoch 56 Batch 0 Loss 0.3409 Accuracy 0.3239\n",
            "Epoch 56 Batch 50 Loss 0.3719 Accuracy 0.3251\n",
            "Epoch 56 Loss 0.3775 Accuracy 0.3242\n",
            "Epoch 57 Batch 0 Loss 0.3118 Accuracy 0.3307\n",
            "Epoch 57 Batch 50 Loss 0.3670 Accuracy 0.3251\n",
            "Epoch 57 Loss 0.3731 Accuracy 0.3244\n",
            "Epoch 58 Batch 0 Loss 0.3201 Accuracy 0.3314\n",
            "Epoch 58 Batch 50 Loss 0.3603 Accuracy 0.3255\n",
            "Epoch 58 Loss 0.3669 Accuracy 0.3249\n",
            "Epoch 59 Batch 0 Loss 0.3125 Accuracy 0.3359\n",
            "Epoch 59 Batch 50 Loss 0.3625 Accuracy 0.3254\n",
            "Epoch 59 Loss 0.3676 Accuracy 0.3247\n",
            "Epoch 60 Batch 0 Loss 0.3139 Accuracy 0.3281\n",
            "Epoch 60 Batch 50 Loss 0.3604 Accuracy 0.3253\n",
            "Epoch 60 Loss 0.3668 Accuracy 0.3244\n",
            "Epoch 61 Batch 0 Loss 0.3278 Accuracy 0.3283\n",
            "Epoch 61 Batch 50 Loss 0.3552 Accuracy 0.3261\n",
            "Epoch 61 Loss 0.3614 Accuracy 0.3256\n",
            "Epoch 62 Batch 0 Loss 0.3051 Accuracy 0.3335\n",
            "Epoch 62 Batch 50 Loss 0.3511 Accuracy 0.3263\n",
            "Epoch 62 Loss 0.3568 Accuracy 0.3256\n",
            "Epoch 63 Batch 0 Loss 0.3112 Accuracy 0.3322\n",
            "Epoch 63 Batch 50 Loss 0.3556 Accuracy 0.3259\n",
            "Epoch 63 Loss 0.3611 Accuracy 0.3253\n",
            "Epoch 64 Batch 0 Loss 0.3216 Accuracy 0.3292\n",
            "Epoch 64 Batch 50 Loss 0.3556 Accuracy 0.3260\n",
            "Epoch 64 Loss 0.3611 Accuracy 0.3256\n",
            "Epoch 65 Batch 0 Loss 0.3040 Accuracy 0.3316\n",
            "Epoch 65 Batch 50 Loss 0.3503 Accuracy 0.3266\n",
            "Epoch 65 Loss 0.3572 Accuracy 0.3258\n",
            "Epoch 66 Batch 0 Loss 0.3074 Accuracy 0.3296\n",
            "Epoch 66 Batch 50 Loss 0.3494 Accuracy 0.3264\n",
            "Epoch 66 Loss 0.3552 Accuracy 0.3258\n",
            "Epoch 67 Batch 0 Loss 0.2935 Accuracy 0.3290\n",
            "Epoch 67 Batch 50 Loss 0.3497 Accuracy 0.3266\n",
            "Epoch 67 Loss 0.3553 Accuracy 0.3261\n",
            "Epoch 68 Batch 0 Loss 0.3026 Accuracy 0.3296\n",
            "Epoch 68 Batch 50 Loss 0.3474 Accuracy 0.3268\n",
            "Epoch 68 Loss 0.3536 Accuracy 0.3263\n",
            "Epoch 69 Batch 0 Loss 0.3012 Accuracy 0.3341\n",
            "Epoch 69 Batch 50 Loss 0.3506 Accuracy 0.3266\n",
            "Epoch 69 Loss 0.3569 Accuracy 0.3258\n",
            "Epoch 70 Batch 0 Loss 0.3063 Accuracy 0.3331\n",
            "Epoch 70 Batch 50 Loss 0.3457 Accuracy 0.3269\n",
            "Epoch 70 Loss 0.3516 Accuracy 0.3262\n",
            "Epoch 71 Batch 0 Loss 0.3118 Accuracy 0.3281\n",
            "Epoch 71 Batch 50 Loss 0.3426 Accuracy 0.3272\n",
            "Epoch 71 Loss 0.3491 Accuracy 0.3266\n",
            "Epoch 72 Batch 0 Loss 0.2850 Accuracy 0.3343\n",
            "Epoch 72 Batch 50 Loss 0.3375 Accuracy 0.3276\n",
            "Epoch 72 Loss 0.3439 Accuracy 0.3269\n",
            "Epoch 73 Batch 0 Loss 0.2761 Accuracy 0.3329\n",
            "Epoch 73 Batch 50 Loss 0.3351 Accuracy 0.3279\n",
            "Epoch 73 Loss 0.3416 Accuracy 0.3274\n",
            "Epoch 74 Batch 0 Loss 0.2975 Accuracy 0.3305\n",
            "Epoch 74 Batch 50 Loss 0.3315 Accuracy 0.3284\n",
            "Epoch 74 Loss 0.3377 Accuracy 0.3274\n",
            "Epoch 75 Batch 0 Loss 0.2863 Accuracy 0.3349\n",
            "Epoch 75 Batch 50 Loss 0.3303 Accuracy 0.3282\n",
            "Epoch 75 Loss 0.3367 Accuracy 0.3275\n",
            "Epoch 76 Batch 0 Loss 0.3005 Accuracy 0.3268\n",
            "Epoch 76 Batch 50 Loss 0.3277 Accuracy 0.3286\n",
            "Epoch 76 Loss 0.3323 Accuracy 0.3282\n",
            "Epoch 77 Batch 0 Loss 0.2680 Accuracy 0.3386\n",
            "Epoch 77 Batch 50 Loss 0.3256 Accuracy 0.3288\n",
            "Epoch 77 Loss 0.3306 Accuracy 0.3282\n",
            "Epoch 78 Batch 0 Loss 0.3061 Accuracy 0.3244\n",
            "Epoch 78 Batch 50 Loss 0.3205 Accuracy 0.3290\n",
            "Epoch 78 Loss 0.3262 Accuracy 0.3285\n",
            "Epoch 79 Batch 0 Loss 0.2644 Accuracy 0.3410\n",
            "Epoch 79 Batch 50 Loss 0.3176 Accuracy 0.3294\n",
            "Epoch 79 Loss 0.3224 Accuracy 0.3288\n",
            "Epoch 80 Batch 0 Loss 0.2813 Accuracy 0.3304\n",
            "Epoch 80 Batch 50 Loss 0.3152 Accuracy 0.3297\n",
            "Epoch 80 Loss 0.3202 Accuracy 0.3291\n",
            "Epoch 81 Batch 0 Loss 0.2660 Accuracy 0.3368\n",
            "Epoch 81 Batch 50 Loss 0.3123 Accuracy 0.3298\n",
            "Epoch 81 Loss 0.3172 Accuracy 0.3293\n",
            "Epoch 82 Batch 0 Loss 0.2787 Accuracy 0.3323\n",
            "Epoch 82 Batch 50 Loss 0.3112 Accuracy 0.3294\n",
            "Epoch 82 Loss 0.3157 Accuracy 0.3287\n",
            "Epoch 83 Batch 0 Loss 0.2580 Accuracy 0.3374\n",
            "Epoch 83 Batch 50 Loss 0.3109 Accuracy 0.3298\n",
            "Epoch 83 Loss 0.3152 Accuracy 0.3291\n",
            "Epoch 84 Batch 0 Loss 0.2630 Accuracy 0.3404\n",
            "Epoch 84 Batch 50 Loss 0.3061 Accuracy 0.3302\n",
            "Epoch 84 Loss 0.3115 Accuracy 0.3296\n",
            "Epoch 85 Batch 0 Loss 0.2830 Accuracy 0.3308\n",
            "Epoch 85 Batch 50 Loss 0.3072 Accuracy 0.3298\n",
            "Epoch 85 Loss 0.3121 Accuracy 0.3292\n",
            "Epoch 86 Batch 0 Loss 0.2618 Accuracy 0.3371\n",
            "Epoch 86 Batch 50 Loss 0.3024 Accuracy 0.3303\n",
            "Epoch 86 Loss 0.3073 Accuracy 0.3296\n",
            "Epoch 87 Batch 0 Loss 0.2505 Accuracy 0.3347\n",
            "Epoch 87 Batch 50 Loss 0.3024 Accuracy 0.3304\n",
            "Epoch 87 Loss 0.3066 Accuracy 0.3297\n",
            "Epoch 88 Batch 0 Loss 0.2670 Accuracy 0.3364\n",
            "Epoch 88 Batch 50 Loss 0.3003 Accuracy 0.3301\n",
            "Epoch 88 Loss 0.3052 Accuracy 0.3297\n",
            "Epoch 89 Batch 0 Loss 0.2400 Accuracy 0.3355\n",
            "Epoch 89 Batch 50 Loss 0.2982 Accuracy 0.3306\n",
            "Epoch 89 Loss 0.3028 Accuracy 0.3297\n",
            "Epoch 90 Batch 0 Loss 0.2680 Accuracy 0.3287\n",
            "Epoch 90 Batch 50 Loss 0.2933 Accuracy 0.3312\n",
            "Epoch 90 Loss 0.2995 Accuracy 0.3303\n",
            "Epoch 91 Batch 0 Loss 0.2455 Accuracy 0.3301\n",
            "Epoch 91 Batch 50 Loss 0.2924 Accuracy 0.3311\n",
            "Epoch 91 Loss 0.2987 Accuracy 0.3305\n",
            "Epoch 92 Batch 0 Loss 0.2320 Accuracy 0.3389\n",
            "Epoch 92 Batch 50 Loss 0.2878 Accuracy 0.3314\n",
            "Epoch 92 Loss 0.2937 Accuracy 0.3304\n",
            "Epoch 93 Batch 0 Loss 0.2432 Accuracy 0.3374\n",
            "Epoch 93 Batch 50 Loss 0.2892 Accuracy 0.3316\n",
            "Epoch 93 Loss 0.2949 Accuracy 0.3308\n",
            "Epoch 94 Batch 0 Loss 0.2468 Accuracy 0.3388\n",
            "Epoch 94 Batch 50 Loss 0.2883 Accuracy 0.3308\n",
            "Epoch 94 Loss 0.2928 Accuracy 0.3305\n",
            "Epoch 95 Batch 0 Loss 0.2326 Accuracy 0.3401\n",
            "Epoch 95 Batch 50 Loss 0.2876 Accuracy 0.3314\n",
            "Epoch 95 Loss 0.2925 Accuracy 0.3305\n",
            "Epoch 96 Batch 0 Loss 0.2549 Accuracy 0.3370\n",
            "Epoch 96 Batch 50 Loss 0.2843 Accuracy 0.3315\n",
            "Epoch 96 Loss 0.2899 Accuracy 0.3308\n",
            "Epoch 97 Batch 0 Loss 0.2464 Accuracy 0.3353\n",
            "Epoch 97 Batch 50 Loss 0.2842 Accuracy 0.3313\n",
            "Epoch 97 Loss 0.2900 Accuracy 0.3306\n",
            "Epoch 98 Batch 0 Loss 0.2277 Accuracy 0.3486\n",
            "Epoch 98 Batch 50 Loss 0.2813 Accuracy 0.3317\n",
            "Epoch 98 Loss 0.2875 Accuracy 0.3309\n",
            "Epoch 99 Batch 0 Loss 0.2390 Accuracy 0.3377\n",
            "Epoch 99 Batch 50 Loss 0.2786 Accuracy 0.3317\n",
            "Epoch 99 Loss 0.2846 Accuracy 0.3311\n",
            "Epoch 100 Batch 0 Loss 0.2435 Accuracy 0.3425\n",
            "Epoch 100 Batch 50 Loss 0.2795 Accuracy 0.3316\n",
            "Epoch 100 Loss 0.2847 Accuracy 0.3311\n",
            "Epoch 101 Batch 0 Loss 0.2341 Accuracy 0.3394\n",
            "Epoch 101 Batch 50 Loss 0.2777 Accuracy 0.3317\n",
            "Epoch 101 Loss 0.2833 Accuracy 0.3311\n",
            "Epoch 102 Batch 0 Loss 0.2183 Accuracy 0.3389\n",
            "Epoch 102 Batch 50 Loss 0.2782 Accuracy 0.3320\n",
            "Epoch 102 Loss 0.2828 Accuracy 0.3311\n",
            "Epoch 103 Batch 0 Loss 0.2225 Accuracy 0.3332\n",
            "Epoch 103 Batch 50 Loss 0.2745 Accuracy 0.3322\n",
            "Epoch 103 Loss 0.2801 Accuracy 0.3314\n",
            "Epoch 104 Batch 0 Loss 0.2481 Accuracy 0.3364\n",
            "Epoch 104 Batch 50 Loss 0.2748 Accuracy 0.3322\n",
            "Epoch 104 Loss 0.2802 Accuracy 0.3313\n",
            "Epoch 105 Batch 0 Loss 0.2520 Accuracy 0.3353\n",
            "Epoch 105 Batch 50 Loss 0.2734 Accuracy 0.3320\n",
            "Epoch 105 Loss 0.2790 Accuracy 0.3311\n",
            "Epoch 106 Batch 0 Loss 0.2443 Accuracy 0.3331\n",
            "Epoch 106 Batch 50 Loss 0.2710 Accuracy 0.3322\n",
            "Epoch 106 Loss 0.2753 Accuracy 0.3317\n",
            "Epoch 107 Batch 0 Loss 0.2304 Accuracy 0.3388\n",
            "Epoch 107 Batch 50 Loss 0.2714 Accuracy 0.3319\n",
            "Epoch 107 Loss 0.2767 Accuracy 0.3313\n",
            "Epoch 108 Batch 0 Loss 0.2334 Accuracy 0.3416\n",
            "Epoch 108 Batch 50 Loss 0.2714 Accuracy 0.3321\n",
            "Epoch 108 Loss 0.2773 Accuracy 0.3311\n",
            "Epoch 109 Batch 0 Loss 0.2279 Accuracy 0.3377\n",
            "Epoch 109 Batch 50 Loss 0.2690 Accuracy 0.3325\n",
            "Epoch 109 Loss 0.2742 Accuracy 0.3318\n",
            "Epoch 110 Batch 0 Loss 0.2319 Accuracy 0.3364\n",
            "Epoch 110 Batch 50 Loss 0.2697 Accuracy 0.3324\n",
            "Epoch 110 Loss 0.2747 Accuracy 0.3316\n",
            "Epoch 111 Batch 0 Loss 0.2315 Accuracy 0.3403\n",
            "Epoch 111 Batch 50 Loss 0.2675 Accuracy 0.3320\n",
            "Epoch 111 Loss 0.2735 Accuracy 0.3314\n",
            "Epoch 112 Batch 0 Loss 0.2194 Accuracy 0.3419\n",
            "Epoch 112 Batch 50 Loss 0.2668 Accuracy 0.3324\n",
            "Epoch 112 Loss 0.2716 Accuracy 0.3317\n",
            "Epoch 113 Batch 0 Loss 0.2304 Accuracy 0.3392\n",
            "Epoch 113 Batch 50 Loss 0.2665 Accuracy 0.3321\n",
            "Epoch 113 Loss 0.2714 Accuracy 0.3316\n",
            "Epoch 114 Batch 0 Loss 0.2311 Accuracy 0.3388\n",
            "Epoch 114 Batch 50 Loss 0.2635 Accuracy 0.3325\n",
            "Epoch 114 Loss 0.2682 Accuracy 0.3318\n",
            "Epoch 115 Batch 0 Loss 0.2306 Accuracy 0.3296\n",
            "Epoch 115 Batch 50 Loss 0.2633 Accuracy 0.3322\n",
            "Epoch 115 Loss 0.2684 Accuracy 0.3317\n",
            "Epoch 116 Batch 0 Loss 0.2185 Accuracy 0.3433\n",
            "Epoch 116 Batch 50 Loss 0.2621 Accuracy 0.3325\n",
            "Epoch 116 Loss 0.2668 Accuracy 0.3318\n",
            "Epoch 117 Batch 0 Loss 0.2191 Accuracy 0.3471\n",
            "Epoch 117 Batch 50 Loss 0.2611 Accuracy 0.3332\n",
            "Epoch 117 Loss 0.2656 Accuracy 0.3323\n",
            "Epoch 118 Batch 0 Loss 0.2412 Accuracy 0.3371\n",
            "Epoch 118 Batch 50 Loss 0.2610 Accuracy 0.3326\n",
            "Epoch 118 Loss 0.2661 Accuracy 0.3318\n",
            "Epoch 119 Batch 0 Loss 0.2203 Accuracy 0.3401\n",
            "Epoch 119 Batch 50 Loss 0.2616 Accuracy 0.3321\n",
            "Epoch 119 Loss 0.2655 Accuracy 0.3318\n",
            "Epoch 120 Batch 0 Loss 0.2149 Accuracy 0.3438\n",
            "Epoch 120 Batch 50 Loss 0.2583 Accuracy 0.3328\n",
            "Epoch 120 Loss 0.2635 Accuracy 0.3321\n",
            "Epoch 121 Batch 0 Loss 0.2194 Accuracy 0.3386\n",
            "Epoch 121 Batch 50 Loss 0.2572 Accuracy 0.3327\n",
            "Epoch 121 Loss 0.2632 Accuracy 0.3320\n",
            "Epoch 122 Batch 0 Loss 0.2426 Accuracy 0.3371\n",
            "Epoch 122 Batch 50 Loss 0.2564 Accuracy 0.3326\n",
            "Epoch 122 Loss 0.2611 Accuracy 0.3319\n",
            "Epoch 123 Batch 0 Loss 0.1971 Accuracy 0.3450\n",
            "Epoch 123 Batch 50 Loss 0.2551 Accuracy 0.3326\n",
            "Epoch 123 Loss 0.2612 Accuracy 0.3320\n",
            "Epoch 124 Batch 0 Loss 0.2178 Accuracy 0.3359\n",
            "Epoch 124 Batch 50 Loss 0.2566 Accuracy 0.3327\n",
            "Epoch 124 Loss 0.2613 Accuracy 0.3321\n",
            "Epoch 125 Batch 0 Loss 0.2090 Accuracy 0.3460\n",
            "Epoch 125 Batch 50 Loss 0.2544 Accuracy 0.3330\n",
            "Epoch 125 Loss 0.2591 Accuracy 0.3323\n",
            "Epoch 126 Batch 0 Loss 0.2203 Accuracy 0.3325\n",
            "Epoch 126 Batch 50 Loss 0.2531 Accuracy 0.3328\n",
            "Epoch 126 Loss 0.2583 Accuracy 0.3323\n",
            "Epoch 127 Batch 0 Loss 0.2172 Accuracy 0.3392\n",
            "Epoch 127 Batch 50 Loss 0.2531 Accuracy 0.3328\n",
            "Epoch 127 Loss 0.2576 Accuracy 0.3321\n",
            "Epoch 128 Batch 0 Loss 0.2169 Accuracy 0.3413\n",
            "Epoch 128 Batch 50 Loss 0.2510 Accuracy 0.3332\n",
            "Epoch 128 Loss 0.2556 Accuracy 0.3326\n",
            "Epoch 129 Batch 0 Loss 0.2167 Accuracy 0.3337\n",
            "Epoch 129 Batch 50 Loss 0.2521 Accuracy 0.3332\n",
            "Epoch 129 Loss 0.2571 Accuracy 0.3323\n",
            "Epoch 130 Batch 0 Loss 0.2254 Accuracy 0.3433\n",
            "Epoch 130 Batch 50 Loss 0.2523 Accuracy 0.3331\n",
            "Epoch 130 Loss 0.2574 Accuracy 0.3323\n",
            "Epoch 131 Batch 0 Loss 0.2257 Accuracy 0.3349\n",
            "Epoch 131 Batch 50 Loss 0.2510 Accuracy 0.3331\n",
            "Epoch 131 Loss 0.2554 Accuracy 0.3326\n",
            "Epoch 132 Batch 0 Loss 0.2361 Accuracy 0.3320\n",
            "Epoch 132 Batch 50 Loss 0.2506 Accuracy 0.3328\n",
            "Epoch 132 Loss 0.2555 Accuracy 0.3322\n",
            "Epoch 133 Batch 0 Loss 0.2122 Accuracy 0.3395\n",
            "Epoch 133 Batch 50 Loss 0.2482 Accuracy 0.3333\n",
            "Epoch 133 Loss 0.2534 Accuracy 0.3326\n",
            "Epoch 134 Batch 0 Loss 0.2186 Accuracy 0.3343\n",
            "Epoch 134 Batch 50 Loss 0.2496 Accuracy 0.3329\n",
            "Epoch 134 Loss 0.2540 Accuracy 0.3324\n",
            "Epoch 135 Batch 0 Loss 0.2125 Accuracy 0.3361\n",
            "Epoch 135 Batch 50 Loss 0.2483 Accuracy 0.3330\n",
            "Epoch 135 Loss 0.2528 Accuracy 0.3324\n",
            "Epoch 136 Batch 0 Loss 0.2117 Accuracy 0.3406\n",
            "Epoch 136 Batch 50 Loss 0.2481 Accuracy 0.3333\n",
            "Epoch 136 Loss 0.2527 Accuracy 0.3325\n",
            "Epoch 137 Batch 0 Loss 0.2124 Accuracy 0.3364\n",
            "Epoch 137 Batch 50 Loss 0.2461 Accuracy 0.3334\n",
            "Epoch 137 Loss 0.2510 Accuracy 0.3327\n",
            "Epoch 138 Batch 0 Loss 0.2189 Accuracy 0.3346\n",
            "Epoch 138 Batch 50 Loss 0.2463 Accuracy 0.3334\n",
            "Epoch 138 Loss 0.2521 Accuracy 0.3327\n",
            "Epoch 139 Batch 0 Loss 0.2148 Accuracy 0.3368\n",
            "Epoch 139 Batch 50 Loss 0.2457 Accuracy 0.3334\n",
            "Epoch 139 Loss 0.2501 Accuracy 0.3326\n",
            "Epoch 140 Batch 0 Loss 0.2091 Accuracy 0.3404\n",
            "Epoch 140 Batch 50 Loss 0.2445 Accuracy 0.3333\n",
            "Epoch 140 Loss 0.2489 Accuracy 0.3327\n",
            "Epoch 141 Batch 0 Loss 0.2110 Accuracy 0.3406\n",
            "Epoch 141 Batch 50 Loss 0.2445 Accuracy 0.3338\n",
            "Epoch 141 Loss 0.2487 Accuracy 0.3329\n",
            "Epoch 142 Batch 0 Loss 0.2246 Accuracy 0.3379\n",
            "Epoch 142 Batch 50 Loss 0.2435 Accuracy 0.3337\n",
            "Epoch 142 Loss 0.2482 Accuracy 0.3329\n",
            "Epoch 143 Batch 0 Loss 0.2082 Accuracy 0.3412\n",
            "Epoch 143 Batch 50 Loss 0.2433 Accuracy 0.3336\n",
            "Epoch 143 Loss 0.2485 Accuracy 0.3328\n",
            "Epoch 144 Batch 0 Loss 0.1997 Accuracy 0.3421\n",
            "Epoch 144 Batch 50 Loss 0.2432 Accuracy 0.3334\n",
            "Epoch 144 Loss 0.2476 Accuracy 0.3329\n",
            "Epoch 145 Batch 0 Loss 0.2106 Accuracy 0.3416\n",
            "Epoch 145 Batch 50 Loss 0.2436 Accuracy 0.3335\n",
            "Epoch 145 Loss 0.2478 Accuracy 0.3327\n",
            "Epoch 146 Batch 0 Loss 0.2097 Accuracy 0.3349\n",
            "Epoch 146 Batch 50 Loss 0.2418 Accuracy 0.3336\n",
            "Epoch 146 Loss 0.2474 Accuracy 0.3327\n",
            "Epoch 147 Batch 0 Loss 0.2152 Accuracy 0.3341\n",
            "Epoch 147 Batch 50 Loss 0.2402 Accuracy 0.3338\n",
            "Epoch 147 Loss 0.2449 Accuracy 0.3327\n",
            "Epoch 148 Batch 0 Loss 0.2164 Accuracy 0.3403\n",
            "Epoch 148 Batch 50 Loss 0.2421 Accuracy 0.3332\n",
            "Epoch 148 Loss 0.2458 Accuracy 0.3328\n",
            "Epoch 149 Batch 0 Loss 0.2090 Accuracy 0.3391\n",
            "Epoch 149 Batch 50 Loss 0.2404 Accuracy 0.3333\n",
            "Epoch 149 Loss 0.2455 Accuracy 0.3328\n",
            "Epoch 150 Batch 0 Loss 0.2154 Accuracy 0.3361\n",
            "Epoch 150 Batch 50 Loss 0.2394 Accuracy 0.3334\n",
            "Epoch 150 Loss 0.2441 Accuracy 0.3327\n",
            "Epoch 151 Batch 0 Loss 0.2157 Accuracy 0.3383\n",
            "Epoch 151 Batch 50 Loss 0.2392 Accuracy 0.3334\n",
            "Epoch 151 Loss 0.2439 Accuracy 0.3327\n",
            "Epoch 152 Batch 0 Loss 0.2011 Accuracy 0.3489\n",
            "Epoch 152 Batch 50 Loss 0.2395 Accuracy 0.3337\n",
            "Epoch 152 Loss 0.2436 Accuracy 0.3330\n",
            "Epoch 153 Batch 0 Loss 0.2040 Accuracy 0.3418\n",
            "Epoch 153 Batch 50 Loss 0.2397 Accuracy 0.3335\n",
            "Epoch 153 Loss 0.2437 Accuracy 0.3328\n",
            "Epoch 154 Batch 0 Loss 0.2224 Accuracy 0.3335\n",
            "Epoch 154 Batch 50 Loss 0.2396 Accuracy 0.3334\n",
            "Epoch 154 Loss 0.2441 Accuracy 0.3329\n",
            "Epoch 155 Batch 0 Loss 0.2171 Accuracy 0.3374\n",
            "Epoch 155 Batch 50 Loss 0.2377 Accuracy 0.3333\n",
            "Epoch 155 Loss 0.2426 Accuracy 0.3326\n",
            "Epoch 156 Batch 0 Loss 0.1933 Accuracy 0.3379\n",
            "Epoch 156 Batch 50 Loss 0.2382 Accuracy 0.3336\n",
            "Epoch 156 Loss 0.2427 Accuracy 0.3329\n",
            "Epoch 157 Batch 0 Loss 0.2154 Accuracy 0.3386\n",
            "Epoch 157 Batch 50 Loss 0.2375 Accuracy 0.3337\n",
            "Epoch 157 Loss 0.2419 Accuracy 0.3329\n",
            "Epoch 158 Batch 0 Loss 0.2060 Accuracy 0.3368\n",
            "Epoch 158 Batch 50 Loss 0.2361 Accuracy 0.3337\n",
            "Epoch 158 Loss 0.2404 Accuracy 0.3330\n",
            "Epoch 159 Batch 0 Loss 0.2211 Accuracy 0.3280\n",
            "Epoch 159 Batch 50 Loss 0.2368 Accuracy 0.3333\n",
            "Epoch 159 Loss 0.2409 Accuracy 0.3329\n",
            "Epoch 160 Batch 0 Loss 0.1938 Accuracy 0.3424\n",
            "Epoch 160 Batch 50 Loss 0.2353 Accuracy 0.3341\n",
            "Epoch 160 Loss 0.2402 Accuracy 0.3330\n",
            "Epoch 161 Batch 0 Loss 0.2151 Accuracy 0.3347\n",
            "Epoch 161 Batch 50 Loss 0.2359 Accuracy 0.3339\n",
            "Epoch 161 Loss 0.2403 Accuracy 0.3331\n",
            "Epoch 162 Batch 0 Loss 0.2088 Accuracy 0.3359\n",
            "Epoch 162 Batch 50 Loss 0.2345 Accuracy 0.3335\n",
            "Epoch 162 Loss 0.2401 Accuracy 0.3326\n",
            "Epoch 163 Batch 0 Loss 0.2139 Accuracy 0.3383\n",
            "Epoch 163 Batch 50 Loss 0.2345 Accuracy 0.3337\n",
            "Epoch 163 Loss 0.2398 Accuracy 0.3328\n",
            "Epoch 164 Batch 0 Loss 0.2142 Accuracy 0.3356\n",
            "Epoch 164 Batch 50 Loss 0.2353 Accuracy 0.3338\n",
            "Epoch 164 Loss 0.2388 Accuracy 0.3331\n",
            "Epoch 165 Batch 0 Loss 0.2018 Accuracy 0.3404\n",
            "Epoch 165 Batch 50 Loss 0.2332 Accuracy 0.3338\n",
            "Epoch 165 Loss 0.2374 Accuracy 0.3331\n",
            "Epoch 166 Batch 0 Loss 0.2085 Accuracy 0.3371\n",
            "Epoch 166 Batch 50 Loss 0.2332 Accuracy 0.3336\n",
            "Epoch 166 Loss 0.2375 Accuracy 0.3329\n",
            "Epoch 167 Batch 0 Loss 0.2050 Accuracy 0.3325\n",
            "Epoch 167 Batch 50 Loss 0.2353 Accuracy 0.3333\n",
            "Epoch 167 Loss 0.2397 Accuracy 0.3327\n",
            "Epoch 168 Batch 0 Loss 0.1997 Accuracy 0.3418\n",
            "Epoch 168 Batch 50 Loss 0.2330 Accuracy 0.3341\n",
            "Epoch 168 Loss 0.2375 Accuracy 0.3331\n",
            "Epoch 169 Batch 0 Loss 0.2029 Accuracy 0.3395\n",
            "Epoch 169 Batch 50 Loss 0.2313 Accuracy 0.3338\n",
            "Epoch 169 Loss 0.2365 Accuracy 0.3330\n",
            "Epoch 170 Batch 0 Loss 0.1968 Accuracy 0.3404\n",
            "Epoch 170 Batch 50 Loss 0.2331 Accuracy 0.3332\n",
            "Epoch 170 Loss 0.2373 Accuracy 0.3328\n",
            "Epoch 171 Batch 0 Loss 0.2018 Accuracy 0.3332\n",
            "Epoch 171 Batch 50 Loss 0.2323 Accuracy 0.3335\n",
            "Epoch 171 Loss 0.2358 Accuracy 0.3330\n",
            "Epoch 172 Batch 0 Loss 0.2086 Accuracy 0.3431\n",
            "Epoch 172 Batch 50 Loss 0.2308 Accuracy 0.3341\n",
            "Epoch 172 Loss 0.2356 Accuracy 0.3332\n",
            "Epoch 173 Batch 0 Loss 0.2029 Accuracy 0.3407\n",
            "Epoch 173 Batch 50 Loss 0.2308 Accuracy 0.3338\n",
            "Epoch 173 Loss 0.2344 Accuracy 0.3333\n",
            "Epoch 174 Batch 0 Loss 0.2035 Accuracy 0.3359\n",
            "Epoch 174 Batch 50 Loss 0.2311 Accuracy 0.3337\n",
            "Epoch 174 Loss 0.2346 Accuracy 0.3329\n",
            "Epoch 175 Batch 0 Loss 0.2067 Accuracy 0.3344\n",
            "Epoch 175 Batch 50 Loss 0.2296 Accuracy 0.3340\n",
            "Epoch 175 Loss 0.2343 Accuracy 0.3330\n",
            "Epoch 176 Batch 0 Loss 0.2037 Accuracy 0.3382\n",
            "Epoch 176 Batch 50 Loss 0.2307 Accuracy 0.3339\n",
            "Epoch 176 Loss 0.2341 Accuracy 0.3330\n",
            "Epoch 177 Batch 0 Loss 0.1981 Accuracy 0.3350\n",
            "Epoch 177 Batch 50 Loss 0.2291 Accuracy 0.3340\n",
            "Epoch 177 Loss 0.2339 Accuracy 0.3332\n",
            "Epoch 178 Batch 0 Loss 0.1901 Accuracy 0.3416\n",
            "Epoch 178 Batch 50 Loss 0.2295 Accuracy 0.3338\n",
            "Epoch 178 Loss 0.2339 Accuracy 0.3329\n",
            "Epoch 179 Batch 0 Loss 0.1841 Accuracy 0.3489\n",
            "Epoch 179 Batch 50 Loss 0.2285 Accuracy 0.3340\n",
            "Epoch 179 Loss 0.2329 Accuracy 0.3331\n",
            "Epoch 180 Batch 0 Loss 0.2012 Accuracy 0.3383\n",
            "Epoch 180 Batch 50 Loss 0.2283 Accuracy 0.3341\n",
            "Epoch 180 Loss 0.2328 Accuracy 0.3332\n",
            "Epoch 181 Batch 0 Loss 0.1913 Accuracy 0.3383\n",
            "Epoch 181 Batch 50 Loss 0.2287 Accuracy 0.3340\n",
            "Epoch 181 Loss 0.2327 Accuracy 0.3331\n",
            "Epoch 182 Batch 0 Loss 0.2094 Accuracy 0.3347\n",
            "Epoch 182 Batch 50 Loss 0.2274 Accuracy 0.3344\n",
            "Epoch 182 Loss 0.2320 Accuracy 0.3334\n",
            "Epoch 183 Batch 0 Loss 0.2052 Accuracy 0.3334\n",
            "Epoch 183 Batch 50 Loss 0.2281 Accuracy 0.3342\n",
            "Epoch 183 Loss 0.2319 Accuracy 0.3333\n",
            "Epoch 184 Batch 0 Loss 0.2133 Accuracy 0.3377\n",
            "Epoch 184 Batch 50 Loss 0.2263 Accuracy 0.3343\n",
            "Epoch 184 Loss 0.2310 Accuracy 0.3333\n",
            "Epoch 185 Batch 0 Loss 0.1887 Accuracy 0.3382\n",
            "Epoch 185 Batch 50 Loss 0.2259 Accuracy 0.3343\n",
            "Epoch 185 Loss 0.2303 Accuracy 0.3336\n",
            "Epoch 186 Batch 0 Loss 0.2086 Accuracy 0.3313\n",
            "Epoch 186 Batch 50 Loss 0.2270 Accuracy 0.3339\n",
            "Epoch 186 Loss 0.2310 Accuracy 0.3333\n",
            "Epoch 187 Batch 0 Loss 0.1982 Accuracy 0.3385\n",
            "Epoch 187 Batch 50 Loss 0.2260 Accuracy 0.3342\n",
            "Epoch 187 Loss 0.2311 Accuracy 0.3334\n",
            "Epoch 188 Batch 0 Loss 0.1901 Accuracy 0.3400\n",
            "Epoch 188 Batch 50 Loss 0.2265 Accuracy 0.3338\n",
            "Epoch 188 Loss 0.2301 Accuracy 0.3331\n",
            "Epoch 189 Batch 0 Loss 0.2121 Accuracy 0.3364\n",
            "Epoch 189 Batch 50 Loss 0.2271 Accuracy 0.3339\n",
            "Epoch 189 Loss 0.2306 Accuracy 0.3331\n",
            "Epoch 190 Batch 0 Loss 0.2018 Accuracy 0.3356\n",
            "Epoch 190 Batch 50 Loss 0.2259 Accuracy 0.3342\n",
            "Epoch 190 Loss 0.2300 Accuracy 0.3333\n",
            "Epoch 191 Batch 0 Loss 0.1898 Accuracy 0.3477\n",
            "Epoch 191 Batch 50 Loss 0.2249 Accuracy 0.3345\n",
            "Epoch 191 Loss 0.2293 Accuracy 0.3335\n",
            "Epoch 192 Batch 0 Loss 0.1912 Accuracy 0.3379\n",
            "Epoch 192 Batch 50 Loss 0.2241 Accuracy 0.3341\n",
            "Epoch 192 Loss 0.2281 Accuracy 0.3334\n",
            "Epoch 193 Batch 0 Loss 0.1878 Accuracy 0.3445\n",
            "Epoch 193 Batch 50 Loss 0.2241 Accuracy 0.3345\n",
            "Epoch 193 Loss 0.2284 Accuracy 0.3335\n",
            "Epoch 194 Batch 0 Loss 0.1787 Accuracy 0.3454\n",
            "Epoch 194 Batch 50 Loss 0.2235 Accuracy 0.3339\n",
            "Epoch 194 Loss 0.2277 Accuracy 0.3332\n",
            "Epoch 195 Batch 0 Loss 0.1881 Accuracy 0.3407\n",
            "Epoch 195 Batch 50 Loss 0.2241 Accuracy 0.3342\n",
            "Epoch 195 Loss 0.2292 Accuracy 0.3329\n",
            "Epoch 196 Batch 0 Loss 0.1985 Accuracy 0.3385\n",
            "Epoch 196 Batch 50 Loss 0.2249 Accuracy 0.3342\n",
            "Epoch 196 Loss 0.2281 Accuracy 0.3335\n",
            "Epoch 197 Batch 0 Loss 0.1898 Accuracy 0.3370\n",
            "Epoch 197 Batch 50 Loss 0.2239 Accuracy 0.3341\n",
            "Epoch 197 Loss 0.2280 Accuracy 0.3334\n",
            "Epoch 198 Batch 0 Loss 0.1990 Accuracy 0.3406\n",
            "Epoch 198 Batch 50 Loss 0.2228 Accuracy 0.3342\n",
            "Epoch 198 Loss 0.2270 Accuracy 0.3336\n",
            "Epoch 199 Batch 0 Loss 0.1957 Accuracy 0.3391\n",
            "Epoch 199 Batch 50 Loss 0.2237 Accuracy 0.3341\n",
            "Epoch 199 Loss 0.2271 Accuracy 0.3336\n",
            "Epoch 200 Batch 0 Loss 0.2104 Accuracy 0.3356\n",
            "Epoch 200 Batch 50 Loss 0.2223 Accuracy 0.3345\n",
            "Epoch 200 Loss 0.2266 Accuracy 0.3335\n",
            "Epoch 201 Batch 0 Loss 0.1994 Accuracy 0.3383\n",
            "Epoch 201 Batch 50 Loss 0.2217 Accuracy 0.3342\n",
            "Epoch 201 Loss 0.2266 Accuracy 0.3332\n",
            "Epoch 202 Batch 0 Loss 0.1848 Accuracy 0.3450\n",
            "Epoch 202 Batch 50 Loss 0.2224 Accuracy 0.3343\n",
            "Epoch 202 Loss 0.2262 Accuracy 0.3334\n",
            "Epoch 203 Batch 0 Loss 0.2000 Accuracy 0.3410\n",
            "Epoch 203 Batch 50 Loss 0.2230 Accuracy 0.3341\n",
            "Epoch 203 Loss 0.2266 Accuracy 0.3333\n",
            "Epoch 204 Batch 0 Loss 0.1940 Accuracy 0.3388\n",
            "Epoch 204 Batch 50 Loss 0.2215 Accuracy 0.3341\n",
            "Epoch 204 Loss 0.2265 Accuracy 0.3332\n",
            "Epoch 205 Batch 0 Loss 0.1814 Accuracy 0.3407\n",
            "Epoch 205 Batch 50 Loss 0.2213 Accuracy 0.3343\n",
            "Epoch 205 Loss 0.2256 Accuracy 0.3333\n",
            "Epoch 206 Batch 0 Loss 0.1881 Accuracy 0.3410\n",
            "Epoch 206 Batch 50 Loss 0.2208 Accuracy 0.3341\n",
            "Epoch 206 Loss 0.2243 Accuracy 0.3336\n",
            "Epoch 207 Batch 0 Loss 0.1967 Accuracy 0.3425\n",
            "Epoch 207 Batch 50 Loss 0.2216 Accuracy 0.3340\n",
            "Epoch 207 Loss 0.2252 Accuracy 0.3334\n",
            "Epoch 208 Batch 0 Loss 0.1959 Accuracy 0.3370\n",
            "Epoch 208 Batch 50 Loss 0.2211 Accuracy 0.3344\n",
            "Epoch 208 Loss 0.2256 Accuracy 0.3335\n",
            "Epoch 209 Batch 0 Loss 0.1999 Accuracy 0.3419\n",
            "Epoch 209 Batch 50 Loss 0.2210 Accuracy 0.3340\n",
            "Epoch 209 Loss 0.2256 Accuracy 0.3333\n",
            "Epoch 210 Batch 0 Loss 0.1951 Accuracy 0.3395\n",
            "Epoch 210 Batch 50 Loss 0.2191 Accuracy 0.3343\n",
            "Epoch 210 Loss 0.2234 Accuracy 0.3334\n",
            "Epoch 211 Batch 0 Loss 0.1909 Accuracy 0.3407\n",
            "Epoch 211 Batch 50 Loss 0.2197 Accuracy 0.3344\n",
            "Epoch 211 Loss 0.2240 Accuracy 0.3336\n",
            "Epoch 212 Batch 0 Loss 0.1992 Accuracy 0.3403\n",
            "Epoch 212 Batch 50 Loss 0.2191 Accuracy 0.3340\n",
            "Epoch 212 Loss 0.2228 Accuracy 0.3333\n",
            "Epoch 213 Batch 0 Loss 0.1950 Accuracy 0.3395\n",
            "Epoch 213 Batch 50 Loss 0.2195 Accuracy 0.3345\n",
            "Epoch 213 Loss 0.2242 Accuracy 0.3335\n",
            "Epoch 214 Batch 0 Loss 0.2056 Accuracy 0.3352\n",
            "Epoch 214 Batch 50 Loss 0.2206 Accuracy 0.3344\n",
            "Epoch 214 Loss 0.2247 Accuracy 0.3334\n",
            "Epoch 215 Batch 0 Loss 0.1919 Accuracy 0.3370\n",
            "Epoch 215 Batch 50 Loss 0.2191 Accuracy 0.3342\n",
            "Epoch 215 Loss 0.2227 Accuracy 0.3336\n",
            "Epoch 216 Batch 0 Loss 0.1847 Accuracy 0.3431\n",
            "Epoch 216 Batch 50 Loss 0.2192 Accuracy 0.3342\n",
            "Epoch 216 Loss 0.2223 Accuracy 0.3334\n",
            "Epoch 217 Batch 0 Loss 0.1953 Accuracy 0.3374\n",
            "Epoch 217 Batch 50 Loss 0.2194 Accuracy 0.3338\n",
            "Epoch 217 Loss 0.2235 Accuracy 0.3332\n",
            "Epoch 218 Batch 0 Loss 0.1991 Accuracy 0.3388\n",
            "Epoch 218 Batch 50 Loss 0.2184 Accuracy 0.3339\n",
            "Epoch 218 Loss 0.2224 Accuracy 0.3331\n",
            "Epoch 219 Batch 0 Loss 0.2082 Accuracy 0.3352\n",
            "Epoch 219 Batch 50 Loss 0.2183 Accuracy 0.3340\n",
            "Epoch 219 Loss 0.2216 Accuracy 0.3336\n",
            "Epoch 220 Batch 0 Loss 0.1913 Accuracy 0.3424\n",
            "Epoch 220 Batch 50 Loss 0.2192 Accuracy 0.3343\n",
            "Epoch 220 Loss 0.2224 Accuracy 0.3334\n",
            "Epoch 221 Batch 0 Loss 0.1910 Accuracy 0.3406\n",
            "Epoch 221 Batch 50 Loss 0.2191 Accuracy 0.3340\n",
            "Epoch 221 Loss 0.2227 Accuracy 0.3334\n",
            "Epoch 222 Batch 0 Loss 0.1776 Accuracy 0.3392\n",
            "Epoch 222 Batch 50 Loss 0.2183 Accuracy 0.3344\n",
            "Epoch 222 Loss 0.2216 Accuracy 0.3337\n",
            "Epoch 223 Batch 0 Loss 0.1965 Accuracy 0.3395\n",
            "Epoch 223 Batch 50 Loss 0.2187 Accuracy 0.3340\n",
            "Epoch 223 Loss 0.2218 Accuracy 0.3333\n",
            "Epoch 224 Batch 0 Loss 0.1927 Accuracy 0.3364\n",
            "Epoch 224 Batch 50 Loss 0.2162 Accuracy 0.3343\n",
            "Epoch 224 Loss 0.2207 Accuracy 0.3336\n",
            "Epoch 225 Batch 0 Loss 0.1859 Accuracy 0.3421\n",
            "Epoch 225 Batch 50 Loss 0.2175 Accuracy 0.3338\n",
            "Epoch 225 Loss 0.2218 Accuracy 0.3331\n",
            "Epoch 226 Batch 0 Loss 0.2024 Accuracy 0.3397\n",
            "Epoch 226 Batch 50 Loss 0.2166 Accuracy 0.3345\n",
            "Epoch 226 Loss 0.2205 Accuracy 0.3336\n",
            "Epoch 227 Batch 0 Loss 0.1777 Accuracy 0.3424\n",
            "Epoch 227 Batch 50 Loss 0.2169 Accuracy 0.3343\n",
            "Epoch 227 Loss 0.2209 Accuracy 0.3336\n",
            "Epoch 228 Batch 0 Loss 0.1940 Accuracy 0.3319\n",
            "Epoch 228 Batch 50 Loss 0.2164 Accuracy 0.3344\n",
            "Epoch 228 Loss 0.2203 Accuracy 0.3335\n",
            "Epoch 229 Batch 0 Loss 0.1895 Accuracy 0.3419\n",
            "Epoch 229 Batch 50 Loss 0.2172 Accuracy 0.3340\n",
            "Epoch 229 Loss 0.2210 Accuracy 0.3333\n",
            "Epoch 230 Batch 0 Loss 0.1872 Accuracy 0.3428\n",
            "Epoch 230 Batch 50 Loss 0.2163 Accuracy 0.3346\n",
            "Epoch 230 Loss 0.2200 Accuracy 0.3339\n",
            "Epoch 231 Batch 0 Loss 0.1935 Accuracy 0.3415\n",
            "Epoch 231 Batch 50 Loss 0.2168 Accuracy 0.3343\n",
            "Epoch 231 Loss 0.2206 Accuracy 0.3335\n",
            "Epoch 232 Batch 0 Loss 0.1907 Accuracy 0.3421\n",
            "Epoch 232 Batch 50 Loss 0.2164 Accuracy 0.3344\n",
            "Epoch 232 Loss 0.2203 Accuracy 0.3335\n",
            "Epoch 233 Batch 0 Loss 0.1684 Accuracy 0.3427\n",
            "Epoch 233 Batch 50 Loss 0.2163 Accuracy 0.3345\n",
            "Epoch 233 Loss 0.2204 Accuracy 0.3337\n",
            "Epoch 234 Batch 0 Loss 0.1967 Accuracy 0.3370\n",
            "Epoch 234 Batch 50 Loss 0.2157 Accuracy 0.3347\n",
            "Epoch 234 Loss 0.2193 Accuracy 0.3338\n",
            "Epoch 235 Batch 0 Loss 0.1905 Accuracy 0.3395\n",
            "Epoch 235 Batch 50 Loss 0.2149 Accuracy 0.3347\n",
            "Epoch 235 Loss 0.2194 Accuracy 0.3336\n",
            "Epoch 236 Batch 0 Loss 0.1992 Accuracy 0.3374\n",
            "Epoch 236 Batch 50 Loss 0.2150 Accuracy 0.3347\n",
            "Epoch 236 Loss 0.2188 Accuracy 0.3338\n",
            "Epoch 237 Batch 0 Loss 0.1975 Accuracy 0.3358\n",
            "Epoch 237 Batch 50 Loss 0.2152 Accuracy 0.3348\n",
            "Epoch 237 Loss 0.2189 Accuracy 0.3339\n",
            "Epoch 238 Batch 0 Loss 0.1759 Accuracy 0.3430\n",
            "Epoch 238 Batch 50 Loss 0.2159 Accuracy 0.3344\n",
            "Epoch 238 Loss 0.2194 Accuracy 0.3336\n",
            "Epoch 239 Batch 0 Loss 0.1781 Accuracy 0.3424\n",
            "Epoch 239 Batch 50 Loss 0.2147 Accuracy 0.3342\n",
            "Epoch 239 Loss 0.2181 Accuracy 0.3335\n",
            "Epoch 240 Batch 0 Loss 0.1785 Accuracy 0.3380\n",
            "Epoch 240 Batch 50 Loss 0.2144 Accuracy 0.3344\n",
            "Epoch 240 Loss 0.2182 Accuracy 0.3334\n",
            "Epoch 241 Batch 0 Loss 0.1858 Accuracy 0.3367\n",
            "Epoch 241 Batch 50 Loss 0.2148 Accuracy 0.3344\n",
            "Epoch 241 Loss 0.2190 Accuracy 0.3336\n",
            "Epoch 242 Batch 0 Loss 0.1875 Accuracy 0.3448\n",
            "Epoch 242 Batch 50 Loss 0.2140 Accuracy 0.3344\n",
            "Epoch 242 Loss 0.2176 Accuracy 0.3337\n",
            "Epoch 243 Batch 0 Loss 0.1815 Accuracy 0.3460\n",
            "Epoch 243 Batch 50 Loss 0.2143 Accuracy 0.3345\n",
            "Epoch 243 Loss 0.2174 Accuracy 0.3337\n",
            "Epoch 244 Batch 0 Loss 0.1889 Accuracy 0.3427\n",
            "Epoch 244 Batch 50 Loss 0.2131 Accuracy 0.3345\n",
            "Epoch 244 Loss 0.2176 Accuracy 0.3336\n",
            "Epoch 245 Batch 0 Loss 0.1872 Accuracy 0.3367\n",
            "Epoch 245 Batch 50 Loss 0.2132 Accuracy 0.3346\n",
            "Epoch 245 Loss 0.2168 Accuracy 0.3338\n",
            "Epoch 246 Batch 0 Loss 0.1859 Accuracy 0.3385\n",
            "Epoch 246 Batch 50 Loss 0.2115 Accuracy 0.3349\n",
            "Epoch 246 Loss 0.2156 Accuracy 0.3339\n",
            "Epoch 247 Batch 0 Loss 0.1954 Accuracy 0.3356\n",
            "Epoch 247 Batch 50 Loss 0.2145 Accuracy 0.3343\n",
            "Epoch 247 Loss 0.2176 Accuracy 0.3335\n",
            "Epoch 248 Batch 0 Loss 0.1798 Accuracy 0.3367\n",
            "Epoch 248 Batch 50 Loss 0.2129 Accuracy 0.3347\n",
            "Epoch 248 Loss 0.2165 Accuracy 0.3338\n",
            "Epoch 249 Batch 0 Loss 0.1658 Accuracy 0.3544\n",
            "Epoch 249 Batch 50 Loss 0.2128 Accuracy 0.3347\n",
            "Epoch 249 Loss 0.2168 Accuracy 0.3336\n",
            "Epoch 250 Batch 0 Loss 0.1795 Accuracy 0.3456\n",
            "Epoch 250 Batch 50 Loss 0.2131 Accuracy 0.3343\n",
            "Epoch 250 Loss 0.2169 Accuracy 0.3336\n",
            "Epoch 251 Batch 0 Loss 0.1891 Accuracy 0.3350\n",
            "Epoch 251 Batch 50 Loss 0.2116 Accuracy 0.3346\n",
            "Epoch 251 Loss 0.2156 Accuracy 0.3340\n",
            "Epoch 252 Batch 0 Loss 0.1918 Accuracy 0.3430\n",
            "Epoch 252 Batch 50 Loss 0.2122 Accuracy 0.3345\n",
            "Epoch 252 Loss 0.2160 Accuracy 0.3338\n",
            "Epoch 253 Batch 0 Loss 0.1925 Accuracy 0.3380\n",
            "Epoch 253 Batch 50 Loss 0.2127 Accuracy 0.3347\n",
            "Epoch 253 Loss 0.2162 Accuracy 0.3337\n",
            "Epoch 254 Batch 0 Loss 0.1874 Accuracy 0.3465\n",
            "Epoch 254 Batch 50 Loss 0.2127 Accuracy 0.3342\n",
            "Epoch 254 Loss 0.2161 Accuracy 0.3336\n",
            "Epoch 255 Batch 0 Loss 0.1916 Accuracy 0.3385\n",
            "Epoch 255 Batch 50 Loss 0.2112 Accuracy 0.3347\n",
            "Epoch 255 Loss 0.2156 Accuracy 0.3336\n",
            "Epoch 256 Batch 0 Loss 0.1843 Accuracy 0.3445\n",
            "Epoch 256 Batch 50 Loss 0.2114 Accuracy 0.3345\n",
            "Epoch 256 Loss 0.2151 Accuracy 0.3338\n",
            "Epoch 257 Batch 0 Loss 0.1891 Accuracy 0.3386\n",
            "Epoch 257 Batch 50 Loss 0.2115 Accuracy 0.3342\n",
            "Epoch 257 Loss 0.2148 Accuracy 0.3336\n",
            "Epoch 258 Batch 0 Loss 0.1745 Accuracy 0.3415\n",
            "Epoch 258 Batch 50 Loss 0.2113 Accuracy 0.3346\n",
            "Epoch 258 Loss 0.2150 Accuracy 0.3339\n",
            "Epoch 259 Batch 0 Loss 0.1811 Accuracy 0.3415\n",
            "Epoch 259 Batch 50 Loss 0.2122 Accuracy 0.3343\n",
            "Epoch 259 Loss 0.2155 Accuracy 0.3335\n",
            "Epoch 260 Batch 0 Loss 0.1850 Accuracy 0.3385\n",
            "Epoch 260 Batch 50 Loss 0.2107 Accuracy 0.3348\n",
            "Epoch 260 Loss 0.2147 Accuracy 0.3338\n",
            "Epoch 261 Batch 0 Loss 0.1908 Accuracy 0.3398\n",
            "Epoch 261 Batch 50 Loss 0.2108 Accuracy 0.3342\n",
            "Epoch 261 Loss 0.2144 Accuracy 0.3336\n",
            "Epoch 262 Batch 0 Loss 0.1913 Accuracy 0.3380\n",
            "Epoch 262 Batch 50 Loss 0.2115 Accuracy 0.3342\n",
            "Epoch 262 Loss 0.2146 Accuracy 0.3337\n",
            "Epoch 263 Batch 0 Loss 0.1866 Accuracy 0.3403\n",
            "Epoch 263 Batch 50 Loss 0.2109 Accuracy 0.3343\n",
            "Epoch 263 Loss 0.2148 Accuracy 0.3336\n",
            "Epoch 264 Batch 0 Loss 0.1868 Accuracy 0.3355\n",
            "Epoch 264 Batch 50 Loss 0.2103 Accuracy 0.3346\n",
            "Epoch 264 Loss 0.2143 Accuracy 0.3337\n",
            "Epoch 265 Batch 0 Loss 0.2038 Accuracy 0.3323\n",
            "Epoch 265 Batch 50 Loss 0.2112 Accuracy 0.3345\n",
            "Epoch 265 Loss 0.2148 Accuracy 0.3337\n",
            "Epoch 266 Batch 0 Loss 0.1793 Accuracy 0.3407\n",
            "Epoch 266 Batch 50 Loss 0.2112 Accuracy 0.3344\n",
            "Epoch 266 Loss 0.2141 Accuracy 0.3337\n",
            "Epoch 267 Batch 0 Loss 0.1872 Accuracy 0.3361\n",
            "Epoch 267 Batch 50 Loss 0.2105 Accuracy 0.3346\n",
            "Epoch 267 Loss 0.2143 Accuracy 0.3338\n",
            "Epoch 268 Batch 0 Loss 0.1707 Accuracy 0.3433\n",
            "Epoch 268 Batch 50 Loss 0.2100 Accuracy 0.3347\n",
            "Epoch 268 Loss 0.2143 Accuracy 0.3336\n",
            "Epoch 269 Batch 0 Loss 0.1862 Accuracy 0.3433\n",
            "Epoch 269 Batch 50 Loss 0.2108 Accuracy 0.3343\n",
            "Epoch 269 Loss 0.2141 Accuracy 0.3337\n",
            "Epoch 270 Batch 0 Loss 0.1938 Accuracy 0.3386\n",
            "Epoch 270 Batch 50 Loss 0.2101 Accuracy 0.3345\n",
            "Epoch 270 Loss 0.2133 Accuracy 0.3339\n",
            "Epoch 271 Batch 0 Loss 0.1893 Accuracy 0.3358\n",
            "Epoch 271 Batch 50 Loss 0.2111 Accuracy 0.3344\n",
            "Epoch 271 Loss 0.2143 Accuracy 0.3337\n",
            "Epoch 272 Batch 0 Loss 0.1828 Accuracy 0.3409\n",
            "Epoch 272 Batch 50 Loss 0.2101 Accuracy 0.3344\n",
            "Epoch 272 Loss 0.2133 Accuracy 0.3337\n",
            "Epoch 273 Batch 0 Loss 0.1804 Accuracy 0.3415\n",
            "Epoch 273 Batch 50 Loss 0.2089 Accuracy 0.3351\n",
            "Epoch 273 Loss 0.2131 Accuracy 0.3341\n",
            "Epoch 274 Batch 0 Loss 0.1836 Accuracy 0.3395\n",
            "Epoch 274 Batch 50 Loss 0.2085 Accuracy 0.3346\n",
            "Epoch 274 Loss 0.2129 Accuracy 0.3336\n",
            "Epoch 275 Batch 0 Loss 0.1858 Accuracy 0.3383\n",
            "Epoch 275 Batch 50 Loss 0.2100 Accuracy 0.3346\n",
            "Epoch 275 Loss 0.2128 Accuracy 0.3339\n",
            "Epoch 276 Batch 0 Loss 0.1698 Accuracy 0.3480\n",
            "Epoch 276 Batch 50 Loss 0.2095 Accuracy 0.3348\n",
            "Epoch 276 Loss 0.2128 Accuracy 0.3337\n",
            "Epoch 277 Batch 0 Loss 0.1935 Accuracy 0.3377\n",
            "Epoch 277 Batch 50 Loss 0.2089 Accuracy 0.3346\n",
            "Epoch 277 Loss 0.2125 Accuracy 0.3339\n",
            "Epoch 278 Batch 0 Loss 0.1960 Accuracy 0.3397\n",
            "Epoch 278 Batch 50 Loss 0.2091 Accuracy 0.3348\n",
            "Epoch 278 Loss 0.2126 Accuracy 0.3338\n",
            "Epoch 279 Batch 0 Loss 0.1885 Accuracy 0.3398\n",
            "Epoch 279 Batch 50 Loss 0.2075 Accuracy 0.3350\n",
            "Epoch 279 Loss 0.2119 Accuracy 0.3340\n",
            "Epoch 280 Batch 0 Loss 0.1882 Accuracy 0.3394\n",
            "Epoch 280 Batch 50 Loss 0.2077 Accuracy 0.3349\n",
            "Epoch 280 Loss 0.2112 Accuracy 0.3341\n",
            "Epoch 281 Batch 0 Loss 0.1919 Accuracy 0.3353\n",
            "Epoch 281 Batch 50 Loss 0.2085 Accuracy 0.3346\n",
            "Epoch 281 Loss 0.2121 Accuracy 0.3338\n",
            "Epoch 282 Batch 0 Loss 0.1740 Accuracy 0.3453\n",
            "Epoch 282 Batch 50 Loss 0.2079 Accuracy 0.3344\n",
            "Epoch 282 Loss 0.2113 Accuracy 0.3338\n",
            "Epoch 283 Batch 0 Loss 0.1994 Accuracy 0.3346\n",
            "Epoch 283 Batch 50 Loss 0.2088 Accuracy 0.3345\n",
            "Epoch 283 Loss 0.2124 Accuracy 0.3337\n",
            "Epoch 284 Batch 0 Loss 0.1907 Accuracy 0.3365\n",
            "Epoch 284 Batch 50 Loss 0.2082 Accuracy 0.3345\n",
            "Epoch 284 Loss 0.2120 Accuracy 0.3338\n",
            "Epoch 285 Batch 0 Loss 0.1922 Accuracy 0.3344\n",
            "Epoch 285 Batch 50 Loss 0.2085 Accuracy 0.3346\n",
            "Epoch 285 Loss 0.2114 Accuracy 0.3338\n",
            "Epoch 286 Batch 0 Loss 0.1715 Accuracy 0.3439\n",
            "Epoch 286 Batch 50 Loss 0.2085 Accuracy 0.3347\n",
            "Epoch 286 Loss 0.2116 Accuracy 0.3343\n",
            "Epoch 287 Batch 0 Loss 0.1940 Accuracy 0.3352\n",
            "Epoch 287 Batch 50 Loss 0.2086 Accuracy 0.3344\n",
            "Epoch 287 Loss 0.2117 Accuracy 0.3337\n",
            "Epoch 288 Batch 0 Loss 0.1955 Accuracy 0.3418\n",
            "Epoch 288 Batch 50 Loss 0.2078 Accuracy 0.3347\n",
            "Epoch 288 Loss 0.2113 Accuracy 0.3338\n",
            "Epoch 289 Batch 0 Loss 0.1856 Accuracy 0.3382\n",
            "Epoch 289 Batch 50 Loss 0.2075 Accuracy 0.3349\n",
            "Epoch 289 Loss 0.2112 Accuracy 0.3341\n",
            "Epoch 290 Batch 0 Loss 0.1823 Accuracy 0.3400\n",
            "Epoch 290 Batch 50 Loss 0.2071 Accuracy 0.3348\n",
            "Epoch 290 Loss 0.2110 Accuracy 0.3340\n",
            "Epoch 291 Batch 0 Loss 0.1882 Accuracy 0.3373\n",
            "Epoch 291 Batch 50 Loss 0.2067 Accuracy 0.3349\n",
            "Epoch 291 Loss 0.2105 Accuracy 0.3341\n",
            "Epoch 292 Batch 0 Loss 0.1896 Accuracy 0.3394\n",
            "Epoch 292 Batch 50 Loss 0.2062 Accuracy 0.3343\n",
            "Epoch 292 Loss 0.2096 Accuracy 0.3338\n",
            "Epoch 293 Batch 0 Loss 0.1894 Accuracy 0.3434\n",
            "Epoch 293 Batch 50 Loss 0.2078 Accuracy 0.3346\n",
            "Epoch 293 Loss 0.2106 Accuracy 0.3339\n",
            "Epoch 294 Batch 0 Loss 0.1706 Accuracy 0.3453\n",
            "Epoch 294 Batch 50 Loss 0.2071 Accuracy 0.3346\n",
            "Epoch 294 Loss 0.2104 Accuracy 0.3338\n",
            "Epoch 295 Batch 0 Loss 0.1794 Accuracy 0.3419\n",
            "Epoch 295 Batch 50 Loss 0.2078 Accuracy 0.3342\n",
            "Epoch 295 Loss 0.2113 Accuracy 0.3337\n",
            "Epoch 296 Batch 0 Loss 0.1867 Accuracy 0.3391\n",
            "Epoch 296 Batch 50 Loss 0.2060 Accuracy 0.3349\n",
            "Epoch 296 Loss 0.2095 Accuracy 0.3340\n",
            "Epoch 297 Batch 0 Loss 0.1812 Accuracy 0.3397\n",
            "Epoch 297 Batch 50 Loss 0.2054 Accuracy 0.3350\n",
            "Epoch 297 Loss 0.2090 Accuracy 0.3341\n",
            "Epoch 298 Batch 0 Loss 0.1851 Accuracy 0.3418\n",
            "Epoch 298 Batch 50 Loss 0.2071 Accuracy 0.3347\n",
            "Epoch 298 Loss 0.2107 Accuracy 0.3338\n",
            "Epoch 299 Batch 0 Loss 0.1806 Accuracy 0.3382\n",
            "Epoch 299 Batch 50 Loss 0.2062 Accuracy 0.3346\n",
            "Epoch 299 Loss 0.2101 Accuracy 0.3338\n",
            "Epoch 300 Batch 0 Loss 0.1873 Accuracy 0.3397\n",
            "Epoch 300 Batch 50 Loss 0.2054 Accuracy 0.3347\n",
            "Epoch 300 Loss 0.2091 Accuracy 0.3341\n",
            "Epoch 301 Batch 0 Loss 0.2064 Accuracy 0.3335\n",
            "Epoch 301 Batch 50 Loss 0.2062 Accuracy 0.3349\n",
            "Epoch 301 Loss 0.2098 Accuracy 0.3341\n",
            "Epoch 302 Batch 0 Loss 0.1899 Accuracy 0.3349\n",
            "Epoch 302 Batch 50 Loss 0.2064 Accuracy 0.3345\n",
            "Epoch 302 Loss 0.2095 Accuracy 0.3340\n",
            "Epoch 303 Batch 0 Loss 0.1871 Accuracy 0.3451\n",
            "Epoch 303 Batch 50 Loss 0.2058 Accuracy 0.3347\n",
            "Epoch 303 Loss 0.2088 Accuracy 0.3341\n",
            "Epoch 304 Batch 0 Loss 0.1891 Accuracy 0.3409\n",
            "Epoch 304 Batch 50 Loss 0.2058 Accuracy 0.3345\n",
            "Epoch 304 Loss 0.2094 Accuracy 0.3337\n",
            "Epoch 305 Batch 0 Loss 0.1814 Accuracy 0.3388\n",
            "Epoch 305 Batch 50 Loss 0.2062 Accuracy 0.3347\n",
            "Epoch 305 Loss 0.2092 Accuracy 0.3342\n",
            "Epoch 306 Batch 0 Loss 0.1878 Accuracy 0.3340\n",
            "Epoch 306 Batch 50 Loss 0.2068 Accuracy 0.3346\n",
            "Epoch 306 Loss 0.2093 Accuracy 0.3339\n",
            "Epoch 307 Batch 0 Loss 0.1848 Accuracy 0.3391\n",
            "Epoch 307 Batch 50 Loss 0.2058 Accuracy 0.3349\n",
            "Epoch 307 Loss 0.2090 Accuracy 0.3341\n",
            "Epoch 308 Batch 0 Loss 0.1828 Accuracy 0.3404\n",
            "Epoch 308 Batch 50 Loss 0.2056 Accuracy 0.3346\n",
            "Epoch 308 Loss 0.2090 Accuracy 0.3337\n",
            "Epoch 309 Batch 0 Loss 0.1845 Accuracy 0.3365\n",
            "Epoch 309 Batch 50 Loss 0.2061 Accuracy 0.3347\n",
            "Epoch 309 Loss 0.2093 Accuracy 0.3343\n",
            "Epoch 310 Batch 0 Loss 0.1694 Accuracy 0.3419\n",
            "Epoch 310 Batch 50 Loss 0.2057 Accuracy 0.3347\n",
            "Epoch 310 Loss 0.2091 Accuracy 0.3338\n",
            "Epoch 311 Batch 0 Loss 0.1841 Accuracy 0.3433\n",
            "Epoch 311 Batch 50 Loss 0.2058 Accuracy 0.3346\n",
            "Epoch 311 Loss 0.2090 Accuracy 0.3340\n",
            "Epoch 312 Batch 0 Loss 0.1848 Accuracy 0.3448\n",
            "Epoch 312 Batch 50 Loss 0.2049 Accuracy 0.3351\n",
            "Epoch 312 Loss 0.2088 Accuracy 0.3342\n",
            "Epoch 313 Batch 0 Loss 0.1882 Accuracy 0.3365\n",
            "Epoch 313 Batch 50 Loss 0.2048 Accuracy 0.3349\n",
            "Epoch 313 Loss 0.2087 Accuracy 0.3342\n",
            "Epoch 314 Batch 0 Loss 0.1795 Accuracy 0.3430\n",
            "Epoch 314 Batch 50 Loss 0.2054 Accuracy 0.3351\n",
            "Epoch 314 Loss 0.2085 Accuracy 0.3344\n",
            "Epoch 315 Batch 0 Loss 0.1733 Accuracy 0.3424\n",
            "Epoch 315 Batch 50 Loss 0.2039 Accuracy 0.3347\n",
            "Epoch 315 Loss 0.2070 Accuracy 0.3340\n",
            "Epoch 316 Batch 0 Loss 0.1834 Accuracy 0.3376\n",
            "Epoch 316 Batch 50 Loss 0.2046 Accuracy 0.3344\n",
            "Epoch 316 Loss 0.2076 Accuracy 0.3338\n",
            "Epoch 317 Batch 0 Loss 0.1834 Accuracy 0.3415\n",
            "Epoch 317 Batch 50 Loss 0.2037 Accuracy 0.3348\n",
            "Epoch 317 Loss 0.2070 Accuracy 0.3344\n",
            "Epoch 318 Batch 0 Loss 0.1957 Accuracy 0.3355\n",
            "Epoch 318 Batch 50 Loss 0.2043 Accuracy 0.3344\n",
            "Epoch 318 Loss 0.2076 Accuracy 0.3337\n",
            "Epoch 319 Batch 0 Loss 0.2026 Accuracy 0.3334\n",
            "Epoch 319 Batch 50 Loss 0.2038 Accuracy 0.3346\n",
            "Epoch 319 Loss 0.2068 Accuracy 0.3341\n",
            "Epoch 320 Batch 0 Loss 0.1797 Accuracy 0.3412\n",
            "Epoch 320 Batch 50 Loss 0.2046 Accuracy 0.3346\n",
            "Epoch 320 Loss 0.2079 Accuracy 0.3339\n",
            "Epoch 321 Batch 0 Loss 0.1824 Accuracy 0.3350\n",
            "Epoch 321 Batch 50 Loss 0.2042 Accuracy 0.3349\n",
            "Epoch 321 Loss 0.2081 Accuracy 0.3340\n",
            "Epoch 322 Batch 0 Loss 0.1895 Accuracy 0.3373\n",
            "Epoch 322 Batch 50 Loss 0.2041 Accuracy 0.3353\n",
            "Epoch 322 Loss 0.2077 Accuracy 0.3341\n",
            "Epoch 323 Batch 0 Loss 0.1781 Accuracy 0.3465\n",
            "Epoch 323 Batch 50 Loss 0.2044 Accuracy 0.3347\n",
            "Epoch 323 Loss 0.2070 Accuracy 0.3341\n",
            "Epoch 324 Batch 0 Loss 0.1776 Accuracy 0.3453\n",
            "Epoch 324 Batch 50 Loss 0.2046 Accuracy 0.3345\n",
            "Epoch 324 Loss 0.2074 Accuracy 0.3339\n",
            "Epoch 325 Batch 0 Loss 0.1848 Accuracy 0.3368\n",
            "Epoch 325 Batch 50 Loss 0.2036 Accuracy 0.3351\n",
            "Epoch 325 Loss 0.2074 Accuracy 0.3342\n",
            "Epoch 326 Batch 0 Loss 0.1800 Accuracy 0.3389\n",
            "Epoch 326 Batch 50 Loss 0.2034 Accuracy 0.3348\n",
            "Epoch 326 Loss 0.2066 Accuracy 0.3343\n",
            "Epoch 327 Batch 0 Loss 0.1760 Accuracy 0.3427\n",
            "Epoch 327 Batch 50 Loss 0.2043 Accuracy 0.3348\n",
            "Epoch 327 Loss 0.2073 Accuracy 0.3342\n",
            "Epoch 328 Batch 0 Loss 0.1898 Accuracy 0.3371\n",
            "Epoch 328 Batch 50 Loss 0.2041 Accuracy 0.3349\n",
            "Epoch 328 Loss 0.2068 Accuracy 0.3342\n",
            "Epoch 329 Batch 0 Loss 0.1794 Accuracy 0.3389\n",
            "Epoch 329 Batch 50 Loss 0.2028 Accuracy 0.3352\n",
            "Epoch 329 Loss 0.2066 Accuracy 0.3343\n",
            "Epoch 330 Batch 0 Loss 0.1845 Accuracy 0.3401\n",
            "Epoch 330 Batch 50 Loss 0.2031 Accuracy 0.3350\n",
            "Epoch 330 Loss 0.2066 Accuracy 0.3341\n",
            "Epoch 331 Batch 0 Loss 0.1960 Accuracy 0.3376\n",
            "Epoch 331 Batch 50 Loss 0.2029 Accuracy 0.3347\n",
            "Epoch 331 Loss 0.2066 Accuracy 0.3339\n",
            "Epoch 332 Batch 0 Loss 0.1787 Accuracy 0.3410\n",
            "Epoch 332 Batch 50 Loss 0.2030 Accuracy 0.3349\n",
            "Epoch 332 Loss 0.2060 Accuracy 0.3340\n",
            "Epoch 333 Batch 0 Loss 0.1768 Accuracy 0.3374\n",
            "Epoch 333 Batch 50 Loss 0.2032 Accuracy 0.3348\n",
            "Epoch 333 Loss 0.2065 Accuracy 0.3339\n",
            "Epoch 334 Batch 0 Loss 0.1842 Accuracy 0.3403\n",
            "Epoch 334 Batch 50 Loss 0.2019 Accuracy 0.3350\n",
            "Epoch 334 Loss 0.2060 Accuracy 0.3342\n",
            "Epoch 335 Batch 0 Loss 0.1808 Accuracy 0.3407\n",
            "Epoch 335 Batch 50 Loss 0.2040 Accuracy 0.3347\n",
            "Epoch 335 Loss 0.2072 Accuracy 0.3338\n",
            "Epoch 336 Batch 0 Loss 0.1752 Accuracy 0.3385\n",
            "Epoch 336 Batch 50 Loss 0.2036 Accuracy 0.3348\n",
            "Epoch 336 Loss 0.2071 Accuracy 0.3341\n",
            "Epoch 337 Batch 0 Loss 0.1801 Accuracy 0.3356\n",
            "Epoch 337 Batch 50 Loss 0.2034 Accuracy 0.3343\n",
            "Epoch 337 Loss 0.2061 Accuracy 0.3340\n",
            "Epoch 338 Batch 0 Loss 0.1699 Accuracy 0.3436\n",
            "Epoch 338 Batch 50 Loss 0.2029 Accuracy 0.3344\n",
            "Epoch 338 Loss 0.2061 Accuracy 0.3338\n",
            "Epoch 339 Batch 0 Loss 0.1883 Accuracy 0.3353\n",
            "Epoch 339 Batch 50 Loss 0.2027 Accuracy 0.3348\n",
            "Epoch 339 Loss 0.2058 Accuracy 0.3340\n",
            "Epoch 340 Batch 0 Loss 0.1712 Accuracy 0.3457\n",
            "Epoch 340 Batch 50 Loss 0.2014 Accuracy 0.3348\n",
            "Epoch 340 Loss 0.2051 Accuracy 0.3340\n",
            "Epoch 341 Batch 0 Loss 0.1821 Accuracy 0.3361\n",
            "Epoch 341 Batch 50 Loss 0.2035 Accuracy 0.3345\n",
            "Epoch 341 Loss 0.2062 Accuracy 0.3340\n",
            "Epoch 342 Batch 0 Loss 0.1697 Accuracy 0.3438\n",
            "Epoch 342 Batch 50 Loss 0.2014 Accuracy 0.3353\n",
            "Epoch 342 Loss 0.2052 Accuracy 0.3343\n",
            "Epoch 343 Batch 0 Loss 0.1861 Accuracy 0.3398\n",
            "Epoch 343 Batch 50 Loss 0.2018 Accuracy 0.3349\n",
            "Epoch 343 Loss 0.2058 Accuracy 0.3343\n",
            "Epoch 344 Batch 0 Loss 0.1741 Accuracy 0.3450\n",
            "Epoch 344 Batch 50 Loss 0.2040 Accuracy 0.3345\n",
            "Epoch 344 Loss 0.2063 Accuracy 0.3340\n",
            "Epoch 345 Batch 0 Loss 0.1720 Accuracy 0.3456\n",
            "Epoch 345 Batch 50 Loss 0.2018 Accuracy 0.3348\n",
            "Epoch 345 Loss 0.2047 Accuracy 0.3342\n",
            "Epoch 346 Batch 0 Loss 0.1795 Accuracy 0.3465\n",
            "Epoch 346 Batch 50 Loss 0.2021 Accuracy 0.3348\n",
            "Epoch 346 Loss 0.2057 Accuracy 0.3339\n",
            "Epoch 347 Batch 0 Loss 0.1757 Accuracy 0.3385\n",
            "Epoch 347 Batch 50 Loss 0.2019 Accuracy 0.3348\n",
            "Epoch 347 Loss 0.2056 Accuracy 0.3340\n",
            "Epoch 348 Batch 0 Loss 0.1847 Accuracy 0.3385\n",
            "Epoch 348 Batch 50 Loss 0.2018 Accuracy 0.3348\n",
            "Epoch 348 Loss 0.2050 Accuracy 0.3339\n",
            "Epoch 349 Batch 0 Loss 0.1874 Accuracy 0.3353\n",
            "Epoch 349 Batch 50 Loss 0.2022 Accuracy 0.3350\n",
            "Epoch 349 Loss 0.2052 Accuracy 0.3343\n",
            "Epoch 350 Batch 0 Loss 0.1811 Accuracy 0.3389\n",
            "Epoch 350 Batch 50 Loss 0.2016 Accuracy 0.3351\n",
            "Epoch 350 Loss 0.2042 Accuracy 0.3345\n",
            "Epoch 351 Batch 0 Loss 0.1840 Accuracy 0.3343\n",
            "Epoch 351 Batch 50 Loss 0.2014 Accuracy 0.3350\n",
            "Epoch 351 Loss 0.2048 Accuracy 0.3342\n",
            "Epoch 352 Batch 0 Loss 0.1851 Accuracy 0.3391\n",
            "Epoch 352 Batch 50 Loss 0.2019 Accuracy 0.3347\n",
            "Epoch 352 Loss 0.2053 Accuracy 0.3339\n",
            "Epoch 353 Batch 0 Loss 0.1966 Accuracy 0.3392\n",
            "Epoch 353 Batch 50 Loss 0.2019 Accuracy 0.3349\n",
            "Epoch 353 Loss 0.2056 Accuracy 0.3339\n",
            "Epoch 354 Batch 0 Loss 0.1701 Accuracy 0.3389\n",
            "Epoch 354 Batch 50 Loss 0.2007 Accuracy 0.3348\n",
            "Epoch 354 Loss 0.2038 Accuracy 0.3341\n",
            "Epoch 355 Batch 0 Loss 0.1706 Accuracy 0.3397\n",
            "Epoch 355 Batch 50 Loss 0.2010 Accuracy 0.3348\n",
            "Epoch 355 Loss 0.2050 Accuracy 0.3337\n",
            "Epoch 356 Batch 0 Loss 0.1714 Accuracy 0.3371\n",
            "Epoch 356 Batch 50 Loss 0.2019 Accuracy 0.3349\n",
            "Epoch 356 Loss 0.2045 Accuracy 0.3342\n",
            "Epoch 357 Batch 0 Loss 0.1752 Accuracy 0.3395\n",
            "Epoch 357 Batch 50 Loss 0.2012 Accuracy 0.3349\n",
            "Epoch 357 Loss 0.2042 Accuracy 0.3343\n",
            "Epoch 358 Batch 0 Loss 0.1710 Accuracy 0.3419\n",
            "Epoch 358 Batch 50 Loss 0.2013 Accuracy 0.3349\n",
            "Epoch 358 Loss 0.2048 Accuracy 0.3339\n",
            "Epoch 359 Batch 0 Loss 0.1729 Accuracy 0.3478\n",
            "Epoch 359 Batch 50 Loss 0.2010 Accuracy 0.3351\n",
            "Epoch 359 Loss 0.2038 Accuracy 0.3343\n",
            "Epoch 360 Batch 0 Loss 0.1762 Accuracy 0.3434\n",
            "Epoch 360 Batch 50 Loss 0.2002 Accuracy 0.3355\n",
            "Epoch 360 Loss 0.2035 Accuracy 0.3344\n",
            "Epoch 361 Batch 0 Loss 0.1896 Accuracy 0.3373\n",
            "Epoch 361 Batch 50 Loss 0.2011 Accuracy 0.3349\n",
            "Epoch 361 Loss 0.2044 Accuracy 0.3340\n",
            "Epoch 362 Batch 0 Loss 0.1894 Accuracy 0.3398\n",
            "Epoch 362 Batch 50 Loss 0.2001 Accuracy 0.3348\n",
            "Epoch 362 Loss 0.2035 Accuracy 0.3341\n",
            "Epoch 363 Batch 0 Loss 0.1796 Accuracy 0.3421\n",
            "Epoch 363 Batch 50 Loss 0.2017 Accuracy 0.3350\n",
            "Epoch 363 Loss 0.2044 Accuracy 0.3343\n",
            "Epoch 364 Batch 0 Loss 0.1781 Accuracy 0.3377\n",
            "Epoch 364 Batch 50 Loss 0.2012 Accuracy 0.3349\n",
            "Epoch 364 Loss 0.2043 Accuracy 0.3342\n",
            "Epoch 365 Batch 0 Loss 0.1758 Accuracy 0.3424\n",
            "Epoch 365 Batch 50 Loss 0.1996 Accuracy 0.3353\n",
            "Epoch 365 Loss 0.2032 Accuracy 0.3343\n",
            "Epoch 366 Batch 0 Loss 0.1696 Accuracy 0.3450\n",
            "Epoch 366 Batch 50 Loss 0.2014 Accuracy 0.3347\n",
            "Epoch 366 Loss 0.2040 Accuracy 0.3341\n",
            "Epoch 367 Batch 0 Loss 0.1817 Accuracy 0.3391\n",
            "Epoch 367 Batch 50 Loss 0.2001 Accuracy 0.3351\n",
            "Epoch 367 Loss 0.2035 Accuracy 0.3342\n",
            "Epoch 368 Batch 0 Loss 0.1840 Accuracy 0.3398\n",
            "Epoch 368 Batch 50 Loss 0.2014 Accuracy 0.3346\n",
            "Epoch 368 Loss 0.2038 Accuracy 0.3339\n",
            "Epoch 369 Batch 0 Loss 0.1918 Accuracy 0.3368\n",
            "Epoch 369 Batch 50 Loss 0.1998 Accuracy 0.3352\n",
            "Epoch 369 Loss 0.2031 Accuracy 0.3341\n",
            "Epoch 370 Batch 0 Loss 0.1921 Accuracy 0.3340\n",
            "Epoch 370 Batch 50 Loss 0.2000 Accuracy 0.3353\n",
            "Epoch 370 Loss 0.2032 Accuracy 0.3343\n",
            "Epoch 371 Batch 0 Loss 0.1817 Accuracy 0.3335\n",
            "Epoch 371 Batch 50 Loss 0.2007 Accuracy 0.3347\n",
            "Epoch 371 Loss 0.2030 Accuracy 0.3341\n",
            "Epoch 372 Batch 0 Loss 0.1751 Accuracy 0.3419\n",
            "Epoch 372 Batch 50 Loss 0.2003 Accuracy 0.3347\n",
            "Epoch 372 Loss 0.2036 Accuracy 0.3339\n",
            "Epoch 373 Batch 0 Loss 0.1819 Accuracy 0.3415\n",
            "Epoch 373 Batch 50 Loss 0.2003 Accuracy 0.3347\n",
            "Epoch 373 Loss 0.2034 Accuracy 0.3340\n",
            "Epoch 374 Batch 0 Loss 0.1902 Accuracy 0.3353\n",
            "Epoch 374 Batch 50 Loss 0.1997 Accuracy 0.3354\n",
            "Epoch 374 Loss 0.2030 Accuracy 0.3344\n",
            "Epoch 375 Batch 0 Loss 0.1809 Accuracy 0.3373\n",
            "Epoch 375 Batch 50 Loss 0.2000 Accuracy 0.3347\n",
            "Epoch 375 Loss 0.2033 Accuracy 0.3340\n",
            "Epoch 376 Batch 0 Loss 0.1818 Accuracy 0.3427\n",
            "Epoch 376 Batch 50 Loss 0.2003 Accuracy 0.3350\n",
            "Epoch 376 Loss 0.2031 Accuracy 0.3343\n",
            "Epoch 377 Batch 0 Loss 0.1843 Accuracy 0.3419\n",
            "Epoch 377 Batch 50 Loss 0.1994 Accuracy 0.3351\n",
            "Epoch 377 Loss 0.2022 Accuracy 0.3344\n",
            "Epoch 378 Batch 0 Loss 0.1842 Accuracy 0.3368\n",
            "Epoch 378 Batch 50 Loss 0.1997 Accuracy 0.3347\n",
            "Epoch 378 Loss 0.2031 Accuracy 0.3340\n",
            "Epoch 379 Batch 0 Loss 0.1976 Accuracy 0.3298\n",
            "Epoch 379 Batch 50 Loss 0.2003 Accuracy 0.3352\n",
            "Epoch 379 Loss 0.2030 Accuracy 0.3342\n",
            "Epoch 380 Batch 0 Loss 0.1690 Accuracy 0.3490\n",
            "Epoch 380 Batch 50 Loss 0.2003 Accuracy 0.3349\n",
            "Epoch 380 Loss 0.2025 Accuracy 0.3342\n",
            "Epoch 381 Batch 0 Loss 0.1836 Accuracy 0.3421\n",
            "Epoch 381 Batch 50 Loss 0.2003 Accuracy 0.3347\n",
            "Epoch 381 Loss 0.2030 Accuracy 0.3339\n",
            "Epoch 382 Batch 0 Loss 0.1876 Accuracy 0.3400\n",
            "Epoch 382 Batch 50 Loss 0.1978 Accuracy 0.3347\n",
            "Epoch 382 Loss 0.2009 Accuracy 0.3341\n",
            "Epoch 383 Batch 0 Loss 0.1729 Accuracy 0.3404\n",
            "Epoch 383 Batch 50 Loss 0.1999 Accuracy 0.3349\n",
            "Epoch 383 Loss 0.2027 Accuracy 0.3346\n",
            "Epoch 384 Batch 0 Loss 0.1877 Accuracy 0.3382\n",
            "Epoch 384 Batch 50 Loss 0.1997 Accuracy 0.3347\n",
            "Epoch 384 Loss 0.2027 Accuracy 0.3341\n",
            "Epoch 385 Batch 0 Loss 0.1763 Accuracy 0.3388\n",
            "Epoch 385 Batch 50 Loss 0.1987 Accuracy 0.3349\n",
            "Epoch 385 Loss 0.2018 Accuracy 0.3341\n",
            "Epoch 386 Batch 0 Loss 0.1784 Accuracy 0.3391\n",
            "Epoch 386 Batch 50 Loss 0.2002 Accuracy 0.3352\n",
            "Epoch 386 Loss 0.2032 Accuracy 0.3341\n",
            "Epoch 387 Batch 0 Loss 0.1683 Accuracy 0.3413\n",
            "Epoch 387 Batch 50 Loss 0.1988 Accuracy 0.3351\n",
            "Epoch 387 Loss 0.2014 Accuracy 0.3345\n",
            "Epoch 388 Batch 0 Loss 0.1737 Accuracy 0.3418\n",
            "Epoch 388 Batch 50 Loss 0.1989 Accuracy 0.3352\n",
            "Epoch 388 Loss 0.2026 Accuracy 0.3342\n",
            "Epoch 389 Batch 0 Loss 0.1731 Accuracy 0.3427\n",
            "Epoch 389 Batch 50 Loss 0.1983 Accuracy 0.3351\n",
            "Epoch 389 Loss 0.2021 Accuracy 0.3345\n",
            "Epoch 390 Batch 0 Loss 0.1852 Accuracy 0.3370\n",
            "Epoch 390 Batch 50 Loss 0.1986 Accuracy 0.3349\n",
            "Epoch 390 Loss 0.2016 Accuracy 0.3342\n",
            "Epoch 391 Batch 0 Loss 0.1712 Accuracy 0.3380\n",
            "Epoch 391 Batch 50 Loss 0.1995 Accuracy 0.3352\n",
            "Epoch 391 Loss 0.2022 Accuracy 0.3344\n",
            "Epoch 392 Batch 0 Loss 0.1776 Accuracy 0.3391\n",
            "Epoch 392 Batch 50 Loss 0.1979 Accuracy 0.3351\n",
            "Epoch 392 Loss 0.2014 Accuracy 0.3342\n",
            "Epoch 393 Batch 0 Loss 0.1686 Accuracy 0.3451\n",
            "Epoch 393 Batch 50 Loss 0.1986 Accuracy 0.3351\n",
            "Epoch 393 Loss 0.2022 Accuracy 0.3342\n",
            "Epoch 394 Batch 0 Loss 0.1731 Accuracy 0.3397\n",
            "Epoch 394 Batch 50 Loss 0.1985 Accuracy 0.3346\n",
            "Epoch 394 Loss 0.2013 Accuracy 0.3339\n",
            "Epoch 395 Batch 0 Loss 0.1699 Accuracy 0.3365\n",
            "Epoch 395 Batch 50 Loss 0.1989 Accuracy 0.3353\n",
            "Epoch 395 Loss 0.2016 Accuracy 0.3345\n",
            "Epoch 396 Batch 0 Loss 0.1788 Accuracy 0.3398\n",
            "Epoch 396 Batch 50 Loss 0.1983 Accuracy 0.3350\n",
            "Epoch 396 Loss 0.2016 Accuracy 0.3341\n",
            "Epoch 397 Batch 0 Loss 0.1940 Accuracy 0.3383\n",
            "Epoch 397 Batch 50 Loss 0.1995 Accuracy 0.3349\n",
            "Epoch 397 Loss 0.2013 Accuracy 0.3346\n",
            "Epoch 398 Batch 0 Loss 0.1806 Accuracy 0.3358\n",
            "Epoch 398 Batch 50 Loss 0.1991 Accuracy 0.3345\n",
            "Epoch 398 Loss 0.2015 Accuracy 0.3342\n",
            "Epoch 399 Batch 0 Loss 0.1774 Accuracy 0.3453\n",
            "Epoch 399 Batch 50 Loss 0.1990 Accuracy 0.3350\n",
            "Epoch 399 Loss 0.2013 Accuracy 0.3343\n",
            "Epoch 400 Batch 0 Loss 0.1748 Accuracy 0.3376\n",
            "Epoch 400 Batch 50 Loss 0.1975 Accuracy 0.3349\n",
            "Epoch 400 Loss 0.2004 Accuracy 0.3343\n",
            "Epoch 401 Batch 0 Loss 0.1826 Accuracy 0.3422\n",
            "Epoch 401 Batch 50 Loss 0.1987 Accuracy 0.3352\n",
            "Epoch 401 Loss 0.2015 Accuracy 0.3344\n",
            "Epoch 402 Batch 0 Loss 0.1783 Accuracy 0.3376\n",
            "Epoch 402 Batch 50 Loss 0.1981 Accuracy 0.3350\n",
            "Epoch 402 Loss 0.2010 Accuracy 0.3343\n",
            "Epoch 403 Batch 0 Loss 0.1703 Accuracy 0.3451\n",
            "Epoch 403 Batch 50 Loss 0.1971 Accuracy 0.3353\n",
            "Epoch 403 Loss 0.1999 Accuracy 0.3344\n",
            "Epoch 404 Batch 0 Loss 0.1893 Accuracy 0.3401\n",
            "Epoch 404 Batch 50 Loss 0.1984 Accuracy 0.3347\n",
            "Epoch 404 Loss 0.2009 Accuracy 0.3341\n",
            "Epoch 405 Batch 0 Loss 0.1759 Accuracy 0.3403\n",
            "Epoch 405 Batch 50 Loss 0.1978 Accuracy 0.3351\n",
            "Epoch 405 Loss 0.2005 Accuracy 0.3345\n",
            "Epoch 406 Batch 0 Loss 0.1717 Accuracy 0.3404\n",
            "Epoch 406 Batch 50 Loss 0.1978 Accuracy 0.3348\n",
            "Epoch 406 Loss 0.2008 Accuracy 0.3341\n",
            "Epoch 407 Batch 0 Loss 0.1834 Accuracy 0.3433\n",
            "Epoch 407 Batch 50 Loss 0.1979 Accuracy 0.3349\n",
            "Epoch 407 Loss 0.2007 Accuracy 0.3344\n",
            "Epoch 408 Batch 0 Loss 0.1791 Accuracy 0.3398\n",
            "Epoch 408 Batch 50 Loss 0.1978 Accuracy 0.3351\n",
            "Epoch 408 Loss 0.2008 Accuracy 0.3343\n",
            "Epoch 409 Batch 0 Loss 0.1938 Accuracy 0.3377\n",
            "Epoch 409 Batch 50 Loss 0.1963 Accuracy 0.3354\n",
            "Epoch 409 Loss 0.1999 Accuracy 0.3346\n",
            "Epoch 410 Batch 0 Loss 0.1877 Accuracy 0.3391\n",
            "Epoch 410 Batch 50 Loss 0.1967 Accuracy 0.3354\n",
            "Epoch 410 Loss 0.2003 Accuracy 0.3343\n",
            "Epoch 411 Batch 0 Loss 0.1779 Accuracy 0.3386\n",
            "Epoch 411 Batch 50 Loss 0.1973 Accuracy 0.3353\n",
            "Epoch 411 Loss 0.2007 Accuracy 0.3344\n",
            "Epoch 412 Batch 0 Loss 0.1625 Accuracy 0.3456\n",
            "Epoch 412 Batch 50 Loss 0.1980 Accuracy 0.3349\n",
            "Epoch 412 Loss 0.2004 Accuracy 0.3344\n",
            "Epoch 413 Batch 0 Loss 0.1688 Accuracy 0.3403\n",
            "Epoch 413 Batch 50 Loss 0.1973 Accuracy 0.3349\n",
            "Epoch 413 Loss 0.2003 Accuracy 0.3343\n",
            "Epoch 414 Batch 0 Loss 0.1850 Accuracy 0.3427\n",
            "Epoch 414 Batch 50 Loss 0.1972 Accuracy 0.3354\n",
            "Epoch 414 Loss 0.2004 Accuracy 0.3344\n",
            "Epoch 415 Batch 0 Loss 0.1793 Accuracy 0.3370\n",
            "Epoch 415 Batch 50 Loss 0.1974 Accuracy 0.3352\n",
            "Epoch 415 Loss 0.2005 Accuracy 0.3344\n",
            "Epoch 416 Batch 0 Loss 0.1756 Accuracy 0.3406\n",
            "Epoch 416 Batch 50 Loss 0.1970 Accuracy 0.3353\n",
            "Epoch 416 Loss 0.1997 Accuracy 0.3346\n",
            "Epoch 417 Batch 0 Loss 0.1603 Accuracy 0.3404\n",
            "Epoch 417 Batch 50 Loss 0.1974 Accuracy 0.3351\n",
            "Epoch 417 Loss 0.2001 Accuracy 0.3345\n",
            "Epoch 418 Batch 0 Loss 0.1638 Accuracy 0.3465\n",
            "Epoch 418 Batch 50 Loss 0.1972 Accuracy 0.3352\n",
            "Epoch 418 Loss 0.2001 Accuracy 0.3345\n",
            "Epoch 419 Batch 0 Loss 0.1797 Accuracy 0.3377\n",
            "Epoch 419 Batch 50 Loss 0.1977 Accuracy 0.3349\n",
            "Epoch 419 Loss 0.2004 Accuracy 0.3342\n",
            "Epoch 420 Batch 0 Loss 0.1745 Accuracy 0.3395\n",
            "Epoch 420 Batch 50 Loss 0.1976 Accuracy 0.3349\n",
            "Epoch 420 Loss 0.2000 Accuracy 0.3343\n",
            "Epoch 421 Batch 0 Loss 0.1768 Accuracy 0.3380\n",
            "Epoch 421 Batch 50 Loss 0.1964 Accuracy 0.3353\n",
            "Epoch 421 Loss 0.1996 Accuracy 0.3343\n",
            "Epoch 422 Batch 0 Loss 0.1823 Accuracy 0.3377\n",
            "Epoch 422 Batch 50 Loss 0.1978 Accuracy 0.3350\n",
            "Epoch 422 Loss 0.2006 Accuracy 0.3343\n",
            "Epoch 423 Batch 0 Loss 0.1775 Accuracy 0.3410\n",
            "Epoch 423 Batch 50 Loss 0.1966 Accuracy 0.3352\n",
            "Epoch 423 Loss 0.1993 Accuracy 0.3344\n",
            "Epoch 424 Batch 0 Loss 0.1843 Accuracy 0.3308\n",
            "Epoch 424 Batch 50 Loss 0.1972 Accuracy 0.3352\n",
            "Epoch 424 Loss 0.2003 Accuracy 0.3344\n",
            "Epoch 425 Batch 0 Loss 0.1740 Accuracy 0.3454\n",
            "Epoch 425 Batch 50 Loss 0.1969 Accuracy 0.3351\n",
            "Epoch 425 Loss 0.1993 Accuracy 0.3343\n",
            "Epoch 426 Batch 0 Loss 0.2019 Accuracy 0.3320\n",
            "Epoch 426 Batch 50 Loss 0.1966 Accuracy 0.3349\n",
            "Epoch 426 Loss 0.1994 Accuracy 0.3344\n",
            "Epoch 427 Batch 0 Loss 0.1721 Accuracy 0.3388\n",
            "Epoch 427 Batch 50 Loss 0.1963 Accuracy 0.3354\n",
            "Epoch 427 Loss 0.1997 Accuracy 0.3346\n",
            "Epoch 428 Batch 0 Loss 0.1826 Accuracy 0.3338\n",
            "Epoch 428 Batch 50 Loss 0.1957 Accuracy 0.3353\n",
            "Epoch 428 Loss 0.1987 Accuracy 0.3346\n",
            "Epoch 429 Batch 0 Loss 0.1707 Accuracy 0.3433\n",
            "Epoch 429 Batch 50 Loss 0.1966 Accuracy 0.3352\n",
            "Epoch 429 Loss 0.2000 Accuracy 0.3344\n",
            "Epoch 430 Batch 0 Loss 0.1704 Accuracy 0.3463\n",
            "Epoch 430 Batch 50 Loss 0.1965 Accuracy 0.3358\n",
            "Epoch 430 Loss 0.1990 Accuracy 0.3347\n",
            "Epoch 431 Batch 0 Loss 0.1660 Accuracy 0.3475\n",
            "Epoch 431 Batch 50 Loss 0.1959 Accuracy 0.3352\n",
            "Epoch 431 Loss 0.1988 Accuracy 0.3342\n",
            "Epoch 432 Batch 0 Loss 0.1794 Accuracy 0.3406\n",
            "Epoch 432 Batch 50 Loss 0.1965 Accuracy 0.3353\n",
            "Epoch 432 Loss 0.1994 Accuracy 0.3345\n",
            "Epoch 433 Batch 0 Loss 0.1818 Accuracy 0.3401\n",
            "Epoch 433 Batch 50 Loss 0.1956 Accuracy 0.3349\n",
            "Epoch 433 Loss 0.1993 Accuracy 0.3341\n",
            "Epoch 434 Batch 0 Loss 0.1759 Accuracy 0.3404\n",
            "Epoch 434 Batch 50 Loss 0.1960 Accuracy 0.3350\n",
            "Epoch 434 Loss 0.1986 Accuracy 0.3343\n",
            "Epoch 435 Batch 0 Loss 0.1735 Accuracy 0.3377\n",
            "Epoch 435 Batch 50 Loss 0.1962 Accuracy 0.3352\n",
            "Epoch 435 Loss 0.1991 Accuracy 0.3343\n",
            "Epoch 436 Batch 0 Loss 0.1666 Accuracy 0.3401\n",
            "Epoch 436 Batch 50 Loss 0.1955 Accuracy 0.3350\n",
            "Epoch 436 Loss 0.1992 Accuracy 0.3343\n",
            "Epoch 437 Batch 0 Loss 0.1787 Accuracy 0.3383\n",
            "Epoch 437 Batch 50 Loss 0.1960 Accuracy 0.3352\n",
            "Epoch 437 Loss 0.1990 Accuracy 0.3345\n",
            "Epoch 438 Batch 0 Loss 0.1931 Accuracy 0.3334\n",
            "Epoch 438 Batch 50 Loss 0.1955 Accuracy 0.3348\n",
            "Epoch 438 Loss 0.1985 Accuracy 0.3343\n",
            "Epoch 439 Batch 0 Loss 0.1609 Accuracy 0.3517\n",
            "Epoch 439 Batch 50 Loss 0.1962 Accuracy 0.3348\n",
            "Epoch 439 Loss 0.1991 Accuracy 0.3343\n",
            "Epoch 440 Batch 0 Loss 0.1795 Accuracy 0.3418\n",
            "Epoch 440 Batch 50 Loss 0.1955 Accuracy 0.3351\n",
            "Epoch 440 Loss 0.1987 Accuracy 0.3343\n",
            "Epoch 441 Batch 0 Loss 0.1749 Accuracy 0.3401\n",
            "Epoch 441 Batch 50 Loss 0.1957 Accuracy 0.3357\n",
            "Epoch 441 Loss 0.1990 Accuracy 0.3345\n",
            "Epoch 442 Batch 0 Loss 0.1707 Accuracy 0.3436\n",
            "Epoch 442 Batch 50 Loss 0.1962 Accuracy 0.3353\n",
            "Epoch 442 Loss 0.1983 Accuracy 0.3348\n",
            "Epoch 443 Batch 0 Loss 0.1828 Accuracy 0.3456\n",
            "Epoch 443 Batch 50 Loss 0.1965 Accuracy 0.3348\n",
            "Epoch 443 Loss 0.1992 Accuracy 0.3343\n",
            "Epoch 444 Batch 0 Loss 0.1757 Accuracy 0.3419\n",
            "Epoch 444 Batch 50 Loss 0.1961 Accuracy 0.3350\n",
            "Epoch 444 Loss 0.1990 Accuracy 0.3341\n",
            "Epoch 445 Batch 0 Loss 0.1820 Accuracy 0.3412\n",
            "Epoch 445 Batch 50 Loss 0.1964 Accuracy 0.3347\n",
            "Epoch 445 Loss 0.1987 Accuracy 0.3343\n",
            "Epoch 446 Batch 0 Loss 0.1616 Accuracy 0.3419\n",
            "Epoch 446 Batch 50 Loss 0.1946 Accuracy 0.3356\n",
            "Epoch 446 Loss 0.1977 Accuracy 0.3348\n",
            "Epoch 447 Batch 0 Loss 0.1806 Accuracy 0.3398\n",
            "Epoch 447 Batch 50 Loss 0.1950 Accuracy 0.3353\n",
            "Epoch 447 Loss 0.1977 Accuracy 0.3348\n",
            "Epoch 448 Batch 0 Loss 0.1814 Accuracy 0.3416\n",
            "Epoch 448 Batch 50 Loss 0.1954 Accuracy 0.3356\n",
            "Epoch 448 Loss 0.1982 Accuracy 0.3349\n",
            "Epoch 449 Batch 0 Loss 0.1823 Accuracy 0.3362\n",
            "Epoch 449 Batch 50 Loss 0.1955 Accuracy 0.3350\n",
            "Epoch 449 Loss 0.1984 Accuracy 0.3343\n",
            "Epoch 450 Batch 0 Loss 0.1730 Accuracy 0.3385\n",
            "Epoch 450 Batch 50 Loss 0.1952 Accuracy 0.3350\n",
            "Epoch 450 Loss 0.1975 Accuracy 0.3346\n",
            "Epoch 451 Batch 0 Loss 0.1967 Accuracy 0.3328\n",
            "Epoch 451 Batch 50 Loss 0.1953 Accuracy 0.3349\n",
            "Epoch 451 Loss 0.1978 Accuracy 0.3343\n",
            "Epoch 452 Batch 0 Loss 0.1764 Accuracy 0.3428\n",
            "Epoch 452 Batch 50 Loss 0.1952 Accuracy 0.3356\n",
            "Epoch 452 Loss 0.1980 Accuracy 0.3346\n",
            "Epoch 453 Batch 0 Loss 0.1770 Accuracy 0.3409\n",
            "Epoch 453 Batch 50 Loss 0.1956 Accuracy 0.3351\n",
            "Epoch 453 Loss 0.1983 Accuracy 0.3345\n",
            "Epoch 454 Batch 0 Loss 0.1708 Accuracy 0.3425\n",
            "Epoch 454 Batch 50 Loss 0.1956 Accuracy 0.3353\n",
            "Epoch 454 Loss 0.1982 Accuracy 0.3343\n",
            "Epoch 455 Batch 0 Loss 0.1741 Accuracy 0.3465\n",
            "Epoch 455 Batch 50 Loss 0.1941 Accuracy 0.3353\n",
            "Epoch 455 Loss 0.1975 Accuracy 0.3345\n",
            "Epoch 456 Batch 0 Loss 0.1769 Accuracy 0.3418\n",
            "Epoch 456 Batch 50 Loss 0.1945 Accuracy 0.3353\n",
            "Epoch 456 Loss 0.1978 Accuracy 0.3345\n",
            "Epoch 457 Batch 0 Loss 0.1800 Accuracy 0.3397\n",
            "Epoch 457 Batch 50 Loss 0.1947 Accuracy 0.3353\n",
            "Epoch 457 Loss 0.1981 Accuracy 0.3343\n",
            "Epoch 458 Batch 0 Loss 0.1773 Accuracy 0.3403\n",
            "Epoch 458 Batch 50 Loss 0.1959 Accuracy 0.3345\n",
            "Epoch 458 Loss 0.1978 Accuracy 0.3343\n",
            "Epoch 459 Batch 0 Loss 0.1733 Accuracy 0.3415\n",
            "Epoch 459 Batch 50 Loss 0.1953 Accuracy 0.3351\n",
            "Epoch 459 Loss 0.1980 Accuracy 0.3345\n",
            "Epoch 460 Batch 0 Loss 0.1646 Accuracy 0.3436\n",
            "Epoch 460 Batch 50 Loss 0.1956 Accuracy 0.3351\n",
            "Epoch 460 Loss 0.1977 Accuracy 0.3347\n",
            "Epoch 461 Batch 0 Loss 0.1711 Accuracy 0.3448\n",
            "Epoch 461 Batch 50 Loss 0.1947 Accuracy 0.3353\n",
            "Epoch 461 Loss 0.1979 Accuracy 0.3346\n",
            "Epoch 462 Batch 0 Loss 0.1736 Accuracy 0.3415\n",
            "Epoch 462 Batch 50 Loss 0.1945 Accuracy 0.3357\n",
            "Epoch 462 Loss 0.1979 Accuracy 0.3348\n",
            "Epoch 463 Batch 0 Loss 0.1733 Accuracy 0.3401\n",
            "Epoch 463 Batch 50 Loss 0.1959 Accuracy 0.3349\n",
            "Epoch 463 Loss 0.1984 Accuracy 0.3344\n",
            "Epoch 464 Batch 0 Loss 0.1753 Accuracy 0.3444\n",
            "Epoch 464 Batch 50 Loss 0.1951 Accuracy 0.3352\n",
            "Epoch 464 Loss 0.1975 Accuracy 0.3344\n",
            "Epoch 465 Batch 0 Loss 0.1717 Accuracy 0.3377\n",
            "Epoch 465 Batch 50 Loss 0.1947 Accuracy 0.3354\n",
            "Epoch 465 Loss 0.1973 Accuracy 0.3346\n",
            "Epoch 466 Batch 0 Loss 0.1751 Accuracy 0.3376\n",
            "Epoch 466 Batch 50 Loss 0.1947 Accuracy 0.3352\n",
            "Epoch 466 Loss 0.1970 Accuracy 0.3345\n",
            "Epoch 467 Batch 0 Loss 0.1951 Accuracy 0.3356\n",
            "Epoch 467 Batch 50 Loss 0.1947 Accuracy 0.3355\n",
            "Epoch 467 Loss 0.1979 Accuracy 0.3345\n",
            "Epoch 468 Batch 0 Loss 0.1797 Accuracy 0.3353\n",
            "Epoch 468 Batch 50 Loss 0.1944 Accuracy 0.3352\n",
            "Epoch 468 Loss 0.1973 Accuracy 0.3344\n",
            "Epoch 469 Batch 0 Loss 0.1720 Accuracy 0.3376\n",
            "Epoch 469 Batch 50 Loss 0.1955 Accuracy 0.3348\n",
            "Epoch 469 Loss 0.1973 Accuracy 0.3344\n",
            "Epoch 470 Batch 0 Loss 0.1686 Accuracy 0.3471\n",
            "Epoch 470 Batch 50 Loss 0.1954 Accuracy 0.3349\n",
            "Epoch 470 Loss 0.1977 Accuracy 0.3343\n",
            "Epoch 471 Batch 0 Loss 0.1894 Accuracy 0.3379\n",
            "Epoch 471 Batch 50 Loss 0.1952 Accuracy 0.3354\n",
            "Epoch 471 Loss 0.1974 Accuracy 0.3345\n",
            "Epoch 472 Batch 0 Loss 0.1889 Accuracy 0.3410\n",
            "Epoch 472 Batch 50 Loss 0.1946 Accuracy 0.3353\n",
            "Epoch 472 Loss 0.1974 Accuracy 0.3346\n",
            "Epoch 473 Batch 0 Loss 0.1794 Accuracy 0.3376\n",
            "Epoch 473 Batch 50 Loss 0.1935 Accuracy 0.3352\n",
            "Epoch 473 Loss 0.1967 Accuracy 0.3345\n",
            "Epoch 474 Batch 0 Loss 0.1670 Accuracy 0.3438\n",
            "Epoch 474 Batch 50 Loss 0.1938 Accuracy 0.3351\n",
            "Epoch 474 Loss 0.1966 Accuracy 0.3345\n",
            "Epoch 475 Batch 0 Loss 0.1731 Accuracy 0.3403\n",
            "Epoch 475 Batch 50 Loss 0.1944 Accuracy 0.3351\n",
            "Epoch 475 Loss 0.1968 Accuracy 0.3345\n",
            "Epoch 476 Batch 0 Loss 0.1979 Accuracy 0.3358\n",
            "Epoch 476 Batch 50 Loss 0.1946 Accuracy 0.3351\n",
            "Epoch 476 Loss 0.1967 Accuracy 0.3345\n",
            "Epoch 477 Batch 0 Loss 0.1795 Accuracy 0.3442\n",
            "Epoch 477 Batch 50 Loss 0.1942 Accuracy 0.3348\n",
            "Epoch 477 Loss 0.1967 Accuracy 0.3344\n",
            "Epoch 478 Batch 0 Loss 0.1748 Accuracy 0.3346\n",
            "Epoch 478 Batch 50 Loss 0.1938 Accuracy 0.3353\n",
            "Epoch 478 Loss 0.1972 Accuracy 0.3344\n",
            "Epoch 479 Batch 0 Loss 0.1683 Accuracy 0.3421\n",
            "Epoch 479 Batch 50 Loss 0.1934 Accuracy 0.3347\n",
            "Epoch 479 Loss 0.1969 Accuracy 0.3342\n",
            "Epoch 480 Batch 0 Loss 0.1695 Accuracy 0.3398\n",
            "Epoch 480 Batch 50 Loss 0.1940 Accuracy 0.3351\n",
            "Epoch 480 Loss 0.1966 Accuracy 0.3344\n",
            "Epoch 481 Batch 0 Loss 0.1802 Accuracy 0.3358\n",
            "Epoch 481 Batch 50 Loss 0.1938 Accuracy 0.3356\n",
            "Epoch 481 Loss 0.1962 Accuracy 0.3348\n",
            "Epoch 482 Batch 0 Loss 0.1823 Accuracy 0.3346\n",
            "Epoch 482 Batch 50 Loss 0.1935 Accuracy 0.3351\n",
            "Epoch 482 Loss 0.1967 Accuracy 0.3342\n",
            "Epoch 483 Batch 0 Loss 0.1683 Accuracy 0.3424\n",
            "Epoch 483 Batch 50 Loss 0.1929 Accuracy 0.3355\n",
            "Epoch 483 Loss 0.1963 Accuracy 0.3344\n",
            "Epoch 484 Batch 0 Loss 0.1723 Accuracy 0.3418\n",
            "Epoch 484 Batch 50 Loss 0.1943 Accuracy 0.3352\n",
            "Epoch 484 Loss 0.1964 Accuracy 0.3345\n",
            "Epoch 485 Batch 0 Loss 0.1691 Accuracy 0.3398\n",
            "Epoch 485 Batch 50 Loss 0.1936 Accuracy 0.3349\n",
            "Epoch 485 Loss 0.1962 Accuracy 0.3345\n",
            "Epoch 486 Batch 0 Loss 0.1834 Accuracy 0.3359\n",
            "Epoch 486 Batch 50 Loss 0.1940 Accuracy 0.3352\n",
            "Epoch 486 Loss 0.1965 Accuracy 0.3344\n",
            "Epoch 487 Batch 0 Loss 0.1866 Accuracy 0.3433\n",
            "Epoch 487 Batch 50 Loss 0.1932 Accuracy 0.3355\n",
            "Epoch 487 Loss 0.1964 Accuracy 0.3347\n",
            "Epoch 488 Batch 0 Loss 0.1828 Accuracy 0.3325\n",
            "Epoch 488 Batch 50 Loss 0.1943 Accuracy 0.3353\n",
            "Epoch 488 Loss 0.1965 Accuracy 0.3345\n",
            "Epoch 489 Batch 0 Loss 0.1744 Accuracy 0.3431\n",
            "Epoch 489 Batch 50 Loss 0.1935 Accuracy 0.3356\n",
            "Epoch 489 Loss 0.1962 Accuracy 0.3347\n",
            "Epoch 490 Batch 0 Loss 0.1692 Accuracy 0.3371\n",
            "Epoch 490 Batch 50 Loss 0.1927 Accuracy 0.3353\n",
            "Epoch 490 Loss 0.1959 Accuracy 0.3344\n",
            "Epoch 491 Batch 0 Loss 0.1824 Accuracy 0.3388\n",
            "Epoch 491 Batch 50 Loss 0.1929 Accuracy 0.3358\n",
            "Epoch 491 Loss 0.1959 Accuracy 0.3349\n",
            "Epoch 492 Batch 0 Loss 0.1800 Accuracy 0.3388\n",
            "Epoch 492 Batch 50 Loss 0.1935 Accuracy 0.3353\n",
            "Epoch 492 Loss 0.1963 Accuracy 0.3346\n",
            "Epoch 493 Batch 0 Loss 0.1931 Accuracy 0.3340\n",
            "Epoch 493 Batch 50 Loss 0.1941 Accuracy 0.3349\n",
            "Epoch 493 Loss 0.1963 Accuracy 0.3343\n",
            "Epoch 494 Batch 0 Loss 0.1738 Accuracy 0.3329\n",
            "Epoch 494 Batch 50 Loss 0.1933 Accuracy 0.3355\n",
            "Epoch 494 Loss 0.1959 Accuracy 0.3347\n",
            "Epoch 495 Batch 0 Loss 0.1686 Accuracy 0.3380\n",
            "Epoch 495 Batch 50 Loss 0.1941 Accuracy 0.3352\n",
            "Epoch 495 Loss 0.1966 Accuracy 0.3345\n",
            "Epoch 496 Batch 0 Loss 0.1701 Accuracy 0.3450\n",
            "Epoch 496 Batch 50 Loss 0.1928 Accuracy 0.3351\n",
            "Epoch 496 Loss 0.1956 Accuracy 0.3344\n",
            "Epoch 497 Batch 0 Loss 0.1758 Accuracy 0.3410\n",
            "Epoch 497 Batch 50 Loss 0.1934 Accuracy 0.3353\n",
            "Epoch 497 Loss 0.1965 Accuracy 0.3345\n",
            "Epoch 498 Batch 0 Loss 0.1748 Accuracy 0.3349\n",
            "Epoch 498 Batch 50 Loss 0.1917 Accuracy 0.3354\n",
            "Epoch 498 Loss 0.1946 Accuracy 0.3349\n",
            "Epoch 499 Batch 0 Loss 0.1754 Accuracy 0.3386\n",
            "Epoch 499 Batch 50 Loss 0.1934 Accuracy 0.3355\n",
            "Epoch 499 Loss 0.1955 Accuracy 0.3348\n",
            "Epoch 500 Batch 0 Loss 0.1689 Accuracy 0.3409\n",
            "Epoch 500 Batch 50 Loss 0.1940 Accuracy 0.3350\n",
            "Epoch 500 Loss 0.1962 Accuracy 0.3347\n",
            "Epoch 501 Batch 0 Loss 0.1805 Accuracy 0.3403\n",
            "Epoch 501 Batch 50 Loss 0.1927 Accuracy 0.3354\n",
            "Epoch 501 Loss 0.1956 Accuracy 0.3346\n",
            "Epoch 502 Batch 0 Loss 0.1709 Accuracy 0.3508\n",
            "Epoch 502 Batch 50 Loss 0.1930 Accuracy 0.3353\n",
            "Epoch 502 Loss 0.1955 Accuracy 0.3346\n",
            "Epoch 503 Batch 0 Loss 0.1883 Accuracy 0.3329\n",
            "Epoch 503 Batch 50 Loss 0.1929 Accuracy 0.3352\n",
            "Epoch 503 Loss 0.1959 Accuracy 0.3344\n",
            "Epoch 504 Batch 0 Loss 0.1822 Accuracy 0.3385\n",
            "Epoch 504 Batch 50 Loss 0.1930 Accuracy 0.3353\n",
            "Epoch 504 Loss 0.1959 Accuracy 0.3345\n",
            "Epoch 505 Batch 0 Loss 0.1799 Accuracy 0.3413\n",
            "Epoch 505 Batch 50 Loss 0.1930 Accuracy 0.3354\n",
            "Epoch 505 Loss 0.1957 Accuracy 0.3346\n",
            "Epoch 506 Batch 0 Loss 0.1822 Accuracy 0.3410\n",
            "Epoch 506 Batch 50 Loss 0.1929 Accuracy 0.3354\n",
            "Epoch 506 Loss 0.1957 Accuracy 0.3345\n",
            "Epoch 507 Batch 0 Loss 0.1734 Accuracy 0.3391\n",
            "Epoch 507 Batch 50 Loss 0.1944 Accuracy 0.3349\n",
            "Epoch 507 Loss 0.1966 Accuracy 0.3344\n",
            "Epoch 508 Batch 0 Loss 0.1789 Accuracy 0.3441\n",
            "Epoch 508 Batch 50 Loss 0.1927 Accuracy 0.3353\n",
            "Epoch 508 Loss 0.1956 Accuracy 0.3343\n",
            "Epoch 509 Batch 0 Loss 0.1723 Accuracy 0.3382\n",
            "Epoch 509 Batch 50 Loss 0.1918 Accuracy 0.3355\n",
            "Epoch 509 Loss 0.1951 Accuracy 0.3346\n",
            "Epoch 510 Batch 0 Loss 0.1861 Accuracy 0.3392\n",
            "Epoch 510 Batch 50 Loss 0.1932 Accuracy 0.3353\n",
            "Epoch 510 Loss 0.1957 Accuracy 0.3347\n",
            "Epoch 511 Batch 0 Loss 0.1680 Accuracy 0.3444\n",
            "Epoch 511 Batch 50 Loss 0.1926 Accuracy 0.3352\n",
            "Epoch 511 Loss 0.1951 Accuracy 0.3346\n",
            "Epoch 512 Batch 0 Loss 0.1772 Accuracy 0.3356\n",
            "Epoch 512 Batch 50 Loss 0.1931 Accuracy 0.3353\n",
            "Epoch 512 Loss 0.1959 Accuracy 0.3346\n",
            "Epoch 513 Batch 0 Loss 0.1797 Accuracy 0.3445\n",
            "Epoch 513 Batch 50 Loss 0.1935 Accuracy 0.3351\n",
            "Epoch 513 Loss 0.1956 Accuracy 0.3345\n",
            "Epoch 514 Batch 0 Loss 0.1724 Accuracy 0.3415\n",
            "Epoch 514 Batch 50 Loss 0.1921 Accuracy 0.3353\n",
            "Epoch 514 Loss 0.1947 Accuracy 0.3346\n",
            "Epoch 515 Batch 0 Loss 0.1817 Accuracy 0.3392\n",
            "Epoch 515 Batch 50 Loss 0.1927 Accuracy 0.3353\n",
            "Epoch 515 Loss 0.1953 Accuracy 0.3347\n",
            "Epoch 516 Batch 0 Loss 0.1648 Accuracy 0.3471\n",
            "Epoch 516 Batch 50 Loss 0.1930 Accuracy 0.3353\n",
            "Epoch 516 Loss 0.1950 Accuracy 0.3348\n",
            "Epoch 517 Batch 0 Loss 0.1703 Accuracy 0.3454\n",
            "Epoch 517 Batch 50 Loss 0.1918 Accuracy 0.3356\n",
            "Epoch 517 Loss 0.1950 Accuracy 0.3347\n",
            "Epoch 518 Batch 0 Loss 0.1780 Accuracy 0.3416\n",
            "Epoch 518 Batch 50 Loss 0.1933 Accuracy 0.3348\n",
            "Epoch 518 Loss 0.1958 Accuracy 0.3341\n",
            "Epoch 519 Batch 0 Loss 0.1778 Accuracy 0.3410\n",
            "Epoch 519 Batch 50 Loss 0.1930 Accuracy 0.3353\n",
            "Epoch 519 Loss 0.1950 Accuracy 0.3346\n",
            "Epoch 520 Batch 0 Loss 0.1668 Accuracy 0.3422\n",
            "Epoch 520 Batch 50 Loss 0.1915 Accuracy 0.3355\n",
            "Epoch 520 Loss 0.1949 Accuracy 0.3347\n",
            "Epoch 521 Batch 0 Loss 0.1697 Accuracy 0.3418\n",
            "Epoch 521 Batch 50 Loss 0.1925 Accuracy 0.3354\n",
            "Epoch 521 Loss 0.1950 Accuracy 0.3348\n",
            "Epoch 522 Batch 0 Loss 0.1765 Accuracy 0.3386\n",
            "Epoch 522 Batch 50 Loss 0.1912 Accuracy 0.3356\n",
            "Epoch 522 Loss 0.1950 Accuracy 0.3346\n",
            "Epoch 523 Batch 0 Loss 0.1660 Accuracy 0.3410\n",
            "Epoch 523 Batch 50 Loss 0.1918 Accuracy 0.3354\n",
            "Epoch 523 Loss 0.1955 Accuracy 0.3344\n",
            "Epoch 524 Batch 0 Loss 0.1749 Accuracy 0.3380\n",
            "Epoch 524 Batch 50 Loss 0.1918 Accuracy 0.3356\n",
            "Epoch 524 Loss 0.1946 Accuracy 0.3348\n",
            "Epoch 525 Batch 0 Loss 0.1736 Accuracy 0.3424\n",
            "Epoch 525 Batch 50 Loss 0.1929 Accuracy 0.3352\n",
            "Epoch 525 Loss 0.1956 Accuracy 0.3346\n",
            "Epoch 526 Batch 0 Loss 0.1710 Accuracy 0.3349\n",
            "Epoch 526 Batch 50 Loss 0.1924 Accuracy 0.3355\n",
            "Epoch 526 Loss 0.1950 Accuracy 0.3348\n",
            "Epoch 527 Batch 0 Loss 0.1772 Accuracy 0.3433\n",
            "Epoch 527 Batch 50 Loss 0.1918 Accuracy 0.3357\n",
            "Epoch 527 Loss 0.1944 Accuracy 0.3351\n",
            "Epoch 528 Batch 0 Loss 0.1784 Accuracy 0.3439\n",
            "Epoch 528 Batch 50 Loss 0.1925 Accuracy 0.3355\n",
            "Epoch 528 Loss 0.1959 Accuracy 0.3344\n",
            "Epoch 529 Batch 0 Loss 0.1708 Accuracy 0.3388\n",
            "Epoch 529 Batch 50 Loss 0.1923 Accuracy 0.3351\n",
            "Epoch 529 Loss 0.1945 Accuracy 0.3346\n",
            "Epoch 530 Batch 0 Loss 0.1883 Accuracy 0.3353\n",
            "Epoch 530 Batch 50 Loss 0.1924 Accuracy 0.3351\n",
            "Epoch 530 Loss 0.1943 Accuracy 0.3347\n",
            "Epoch 531 Batch 0 Loss 0.1682 Accuracy 0.3407\n",
            "Epoch 531 Batch 50 Loss 0.1911 Accuracy 0.3357\n",
            "Epoch 531 Loss 0.1948 Accuracy 0.3345\n",
            "Epoch 532 Batch 0 Loss 0.1775 Accuracy 0.3370\n",
            "Epoch 532 Batch 50 Loss 0.1928 Accuracy 0.3352\n",
            "Epoch 532 Loss 0.1956 Accuracy 0.3345\n",
            "Epoch 533 Batch 0 Loss 0.1725 Accuracy 0.3409\n",
            "Epoch 533 Batch 50 Loss 0.1909 Accuracy 0.3355\n",
            "Epoch 533 Loss 0.1937 Accuracy 0.3348\n",
            "Epoch 534 Batch 0 Loss 0.1653 Accuracy 0.3468\n",
            "Epoch 534 Batch 50 Loss 0.1924 Accuracy 0.3351\n",
            "Epoch 534 Loss 0.1948 Accuracy 0.3345\n",
            "Epoch 535 Batch 0 Loss 0.1876 Accuracy 0.3331\n",
            "Epoch 535 Batch 50 Loss 0.1915 Accuracy 0.3352\n",
            "Epoch 535 Loss 0.1942 Accuracy 0.3346\n",
            "Epoch 536 Batch 0 Loss 0.1729 Accuracy 0.3418\n",
            "Epoch 536 Batch 50 Loss 0.1922 Accuracy 0.3355\n",
            "Epoch 536 Loss 0.1943 Accuracy 0.3347\n",
            "Epoch 537 Batch 0 Loss 0.1858 Accuracy 0.3400\n",
            "Epoch 537 Batch 50 Loss 0.1915 Accuracy 0.3353\n",
            "Epoch 537 Loss 0.1942 Accuracy 0.3348\n",
            "Epoch 538 Batch 0 Loss 0.1645 Accuracy 0.3459\n",
            "Epoch 538 Batch 50 Loss 0.1911 Accuracy 0.3351\n",
            "Epoch 538 Loss 0.1939 Accuracy 0.3344\n",
            "Epoch 539 Batch 0 Loss 0.1698 Accuracy 0.3439\n",
            "Epoch 539 Batch 50 Loss 0.1915 Accuracy 0.3356\n",
            "Epoch 539 Loss 0.1943 Accuracy 0.3347\n",
            "Epoch 540 Batch 0 Loss 0.1709 Accuracy 0.3380\n",
            "Epoch 540 Batch 50 Loss 0.1916 Accuracy 0.3356\n",
            "Epoch 540 Loss 0.1947 Accuracy 0.3348\n",
            "Epoch 541 Batch 0 Loss 0.1719 Accuracy 0.3419\n",
            "Epoch 541 Batch 50 Loss 0.1920 Accuracy 0.3355\n",
            "Epoch 541 Loss 0.1944 Accuracy 0.3346\n",
            "Epoch 542 Batch 0 Loss 0.1714 Accuracy 0.3471\n",
            "Epoch 542 Batch 50 Loss 0.1917 Accuracy 0.3349\n",
            "Epoch 542 Loss 0.1943 Accuracy 0.3344\n",
            "Epoch 543 Batch 0 Loss 0.1789 Accuracy 0.3362\n",
            "Epoch 543 Batch 50 Loss 0.1909 Accuracy 0.3358\n",
            "Epoch 543 Loss 0.1936 Accuracy 0.3348\n",
            "Epoch 544 Batch 0 Loss 0.1768 Accuracy 0.3371\n",
            "Epoch 544 Batch 50 Loss 0.1922 Accuracy 0.3349\n",
            "Epoch 544 Loss 0.1945 Accuracy 0.3344\n",
            "Epoch 545 Batch 0 Loss 0.1857 Accuracy 0.3373\n",
            "Epoch 545 Batch 50 Loss 0.1917 Accuracy 0.3350\n",
            "Epoch 545 Loss 0.1945 Accuracy 0.3343\n",
            "Epoch 546 Batch 0 Loss 0.1711 Accuracy 0.3370\n",
            "Epoch 546 Batch 50 Loss 0.1907 Accuracy 0.3360\n",
            "Epoch 546 Loss 0.1933 Accuracy 0.3349\n",
            "Epoch 547 Batch 0 Loss 0.1688 Accuracy 0.3487\n",
            "Epoch 547 Batch 50 Loss 0.1917 Accuracy 0.3351\n",
            "Epoch 547 Loss 0.1935 Accuracy 0.3347\n",
            "Epoch 548 Batch 0 Loss 0.1861 Accuracy 0.3368\n",
            "Epoch 548 Batch 50 Loss 0.1927 Accuracy 0.3351\n",
            "Epoch 548 Loss 0.1948 Accuracy 0.3345\n",
            "Epoch 549 Batch 0 Loss 0.1794 Accuracy 0.3350\n",
            "Epoch 549 Batch 50 Loss 0.1903 Accuracy 0.3356\n",
            "Epoch 549 Loss 0.1935 Accuracy 0.3348\n",
            "Epoch 550 Batch 0 Loss 0.1879 Accuracy 0.3392\n",
            "Epoch 550 Batch 50 Loss 0.1909 Accuracy 0.3359\n",
            "Epoch 550 Loss 0.1941 Accuracy 0.3350\n",
            "Epoch 551 Batch 0 Loss 0.1755 Accuracy 0.3391\n",
            "Epoch 551 Batch 50 Loss 0.1908 Accuracy 0.3354\n",
            "Epoch 551 Loss 0.1933 Accuracy 0.3348\n",
            "Epoch 552 Batch 0 Loss 0.1749 Accuracy 0.3400\n",
            "Epoch 552 Batch 50 Loss 0.1915 Accuracy 0.3351\n",
            "Epoch 552 Loss 0.1946 Accuracy 0.3346\n",
            "Epoch 553 Batch 0 Loss 0.1681 Accuracy 0.3445\n",
            "Epoch 553 Batch 50 Loss 0.1913 Accuracy 0.3353\n",
            "Epoch 553 Loss 0.1939 Accuracy 0.3347\n",
            "Epoch 554 Batch 0 Loss 0.1675 Accuracy 0.3410\n",
            "Epoch 554 Batch 50 Loss 0.1916 Accuracy 0.3359\n",
            "Epoch 554 Loss 0.1944 Accuracy 0.3349\n",
            "Epoch 555 Batch 0 Loss 0.1740 Accuracy 0.3397\n",
            "Epoch 555 Batch 50 Loss 0.1912 Accuracy 0.3354\n",
            "Epoch 555 Loss 0.1938 Accuracy 0.3347\n",
            "Epoch 556 Batch 0 Loss 0.1712 Accuracy 0.3415\n",
            "Epoch 556 Batch 50 Loss 0.1906 Accuracy 0.3357\n",
            "Epoch 556 Loss 0.1943 Accuracy 0.3346\n",
            "Epoch 557 Batch 0 Loss 0.1784 Accuracy 0.3403\n",
            "Epoch 557 Batch 50 Loss 0.1906 Accuracy 0.3353\n",
            "Epoch 557 Loss 0.1934 Accuracy 0.3345\n",
            "Epoch 558 Batch 0 Loss 0.1773 Accuracy 0.3335\n",
            "Epoch 558 Batch 50 Loss 0.1914 Accuracy 0.3351\n",
            "Epoch 558 Loss 0.1940 Accuracy 0.3348\n",
            "Epoch 559 Batch 0 Loss 0.1726 Accuracy 0.3383\n",
            "Epoch 559 Batch 50 Loss 0.1909 Accuracy 0.3356\n",
            "Epoch 559 Loss 0.1934 Accuracy 0.3349\n",
            "Epoch 560 Batch 0 Loss 0.1748 Accuracy 0.3356\n",
            "Epoch 560 Batch 50 Loss 0.1917 Accuracy 0.3354\n",
            "Epoch 560 Loss 0.1939 Accuracy 0.3346\n",
            "Epoch 561 Batch 0 Loss 0.1596 Accuracy 0.3480\n",
            "Epoch 561 Batch 50 Loss 0.1916 Accuracy 0.3354\n",
            "Epoch 561 Loss 0.1936 Accuracy 0.3347\n",
            "Epoch 562 Batch 0 Loss 0.1701 Accuracy 0.3431\n",
            "Epoch 562 Batch 50 Loss 0.1909 Accuracy 0.3354\n",
            "Epoch 562 Loss 0.1934 Accuracy 0.3348\n",
            "Epoch 563 Batch 0 Loss 0.1776 Accuracy 0.3395\n",
            "Epoch 563 Batch 50 Loss 0.1909 Accuracy 0.3354\n",
            "Epoch 563 Loss 0.1936 Accuracy 0.3346\n",
            "Epoch 564 Batch 0 Loss 0.1780 Accuracy 0.3431\n",
            "Epoch 564 Batch 50 Loss 0.1908 Accuracy 0.3354\n",
            "Epoch 564 Loss 0.1938 Accuracy 0.3346\n",
            "Epoch 565 Batch 0 Loss 0.1732 Accuracy 0.3404\n",
            "Epoch 565 Batch 50 Loss 0.1902 Accuracy 0.3354\n",
            "Epoch 565 Loss 0.1930 Accuracy 0.3346\n",
            "Epoch 566 Batch 0 Loss 0.1647 Accuracy 0.3496\n",
            "Epoch 566 Batch 50 Loss 0.1901 Accuracy 0.3355\n",
            "Epoch 566 Loss 0.1928 Accuracy 0.3347\n",
            "Epoch 567 Batch 0 Loss 0.1674 Accuracy 0.3474\n",
            "Epoch 567 Batch 50 Loss 0.1900 Accuracy 0.3356\n",
            "Epoch 567 Loss 0.1929 Accuracy 0.3350\n",
            "Epoch 568 Batch 0 Loss 0.1814 Accuracy 0.3373\n",
            "Epoch 568 Batch 50 Loss 0.1902 Accuracy 0.3354\n",
            "Epoch 568 Loss 0.1932 Accuracy 0.3346\n",
            "Epoch 569 Batch 0 Loss 0.1706 Accuracy 0.3434\n",
            "Epoch 569 Batch 50 Loss 0.1905 Accuracy 0.3359\n",
            "Epoch 569 Loss 0.1935 Accuracy 0.3350\n",
            "Epoch 570 Batch 0 Loss 0.1705 Accuracy 0.3428\n",
            "Epoch 570 Batch 50 Loss 0.1905 Accuracy 0.3353\n",
            "Epoch 570 Loss 0.1935 Accuracy 0.3346\n",
            "Epoch 571 Batch 0 Loss 0.1833 Accuracy 0.3416\n",
            "Epoch 571 Batch 50 Loss 0.1912 Accuracy 0.3350\n",
            "Epoch 571 Loss 0.1935 Accuracy 0.3345\n",
            "Epoch 572 Batch 0 Loss 0.1704 Accuracy 0.3453\n",
            "Epoch 572 Batch 50 Loss 0.1908 Accuracy 0.3355\n",
            "Epoch 572 Loss 0.1926 Accuracy 0.3351\n",
            "Epoch 573 Batch 0 Loss 0.1752 Accuracy 0.3373\n",
            "Epoch 573 Batch 50 Loss 0.1907 Accuracy 0.3351\n",
            "Epoch 573 Loss 0.1930 Accuracy 0.3347\n",
            "Epoch 574 Batch 0 Loss 0.1794 Accuracy 0.3361\n",
            "Epoch 574 Batch 50 Loss 0.1913 Accuracy 0.3353\n",
            "Epoch 574 Loss 0.1939 Accuracy 0.3345\n",
            "Epoch 575 Batch 0 Loss 0.1814 Accuracy 0.3419\n",
            "Epoch 575 Batch 50 Loss 0.1906 Accuracy 0.3357\n",
            "Epoch 575 Loss 0.1933 Accuracy 0.3349\n",
            "Epoch 576 Batch 0 Loss 0.1618 Accuracy 0.3416\n",
            "Epoch 576 Batch 50 Loss 0.1895 Accuracy 0.3359\n",
            "Epoch 576 Loss 0.1925 Accuracy 0.3350\n",
            "Epoch 577 Batch 0 Loss 0.1784 Accuracy 0.3332\n",
            "Epoch 577 Batch 50 Loss 0.1890 Accuracy 0.3356\n",
            "Epoch 577 Loss 0.1916 Accuracy 0.3349\n",
            "Epoch 578 Batch 0 Loss 0.1823 Accuracy 0.3441\n",
            "Epoch 578 Batch 50 Loss 0.1905 Accuracy 0.3353\n",
            "Epoch 578 Loss 0.1924 Accuracy 0.3348\n",
            "Epoch 579 Batch 0 Loss 0.1764 Accuracy 0.3374\n",
            "Epoch 579 Batch 50 Loss 0.1905 Accuracy 0.3354\n",
            "Epoch 579 Loss 0.1932 Accuracy 0.3347\n",
            "Epoch 580 Batch 0 Loss 0.1879 Accuracy 0.3322\n",
            "Epoch 580 Batch 50 Loss 0.1915 Accuracy 0.3352\n",
            "Epoch 580 Loss 0.1934 Accuracy 0.3347\n",
            "Epoch 581 Batch 0 Loss 0.1799 Accuracy 0.3341\n",
            "Epoch 581 Batch 50 Loss 0.1908 Accuracy 0.3355\n",
            "Epoch 581 Loss 0.1932 Accuracy 0.3348\n",
            "Epoch 582 Batch 0 Loss 0.1832 Accuracy 0.3322\n",
            "Epoch 582 Batch 50 Loss 0.1913 Accuracy 0.3349\n",
            "Epoch 582 Loss 0.1933 Accuracy 0.3345\n",
            "Epoch 583 Batch 0 Loss 0.1683 Accuracy 0.3379\n",
            "Epoch 583 Batch 50 Loss 0.1908 Accuracy 0.3351\n",
            "Epoch 583 Loss 0.1929 Accuracy 0.3348\n",
            "Epoch 584 Batch 0 Loss 0.1700 Accuracy 0.3409\n",
            "Epoch 584 Batch 50 Loss 0.1911 Accuracy 0.3357\n",
            "Epoch 584 Loss 0.1933 Accuracy 0.3350\n",
            "Epoch 585 Batch 0 Loss 0.1827 Accuracy 0.3367\n",
            "Epoch 585 Batch 50 Loss 0.1905 Accuracy 0.3356\n",
            "Epoch 585 Loss 0.1931 Accuracy 0.3346\n",
            "Epoch 586 Batch 0 Loss 0.1716 Accuracy 0.3323\n",
            "Epoch 586 Batch 50 Loss 0.1903 Accuracy 0.3356\n",
            "Epoch 586 Loss 0.1928 Accuracy 0.3348\n",
            "Epoch 587 Batch 0 Loss 0.1871 Accuracy 0.3340\n",
            "Epoch 587 Batch 50 Loss 0.1907 Accuracy 0.3355\n",
            "Epoch 587 Loss 0.1930 Accuracy 0.3348\n",
            "Epoch 588 Batch 0 Loss 0.1676 Accuracy 0.3424\n",
            "Epoch 588 Batch 50 Loss 0.1907 Accuracy 0.3356\n",
            "Epoch 588 Loss 0.1931 Accuracy 0.3350\n",
            "Epoch 589 Batch 0 Loss 0.1835 Accuracy 0.3406\n",
            "Epoch 589 Batch 50 Loss 0.1904 Accuracy 0.3353\n",
            "Epoch 589 Loss 0.1928 Accuracy 0.3347\n",
            "Epoch 590 Batch 0 Loss 0.1716 Accuracy 0.3340\n",
            "Epoch 590 Batch 50 Loss 0.1895 Accuracy 0.3356\n",
            "Epoch 590 Loss 0.1921 Accuracy 0.3348\n",
            "Epoch 591 Batch 0 Loss 0.1816 Accuracy 0.3379\n",
            "Epoch 591 Batch 50 Loss 0.1897 Accuracy 0.3355\n",
            "Epoch 591 Loss 0.1926 Accuracy 0.3347\n",
            "Epoch 592 Batch 0 Loss 0.1750 Accuracy 0.3401\n",
            "Epoch 592 Batch 50 Loss 0.1891 Accuracy 0.3363\n",
            "Epoch 592 Loss 0.1923 Accuracy 0.3352\n",
            "Epoch 593 Batch 0 Loss 0.1827 Accuracy 0.3433\n",
            "Epoch 593 Batch 50 Loss 0.1905 Accuracy 0.3354\n",
            "Epoch 593 Loss 0.1923 Accuracy 0.3349\n",
            "Epoch 594 Batch 0 Loss 0.1706 Accuracy 0.3434\n",
            "Epoch 594 Batch 50 Loss 0.1897 Accuracy 0.3355\n",
            "Epoch 594 Loss 0.1924 Accuracy 0.3349\n",
            "Epoch 595 Batch 0 Loss 0.1793 Accuracy 0.3368\n",
            "Epoch 595 Batch 50 Loss 0.1895 Accuracy 0.3358\n",
            "Epoch 595 Loss 0.1913 Accuracy 0.3350\n",
            "Epoch 596 Batch 0 Loss 0.1715 Accuracy 0.3379\n",
            "Epoch 596 Batch 50 Loss 0.1894 Accuracy 0.3354\n",
            "Epoch 596 Loss 0.1927 Accuracy 0.3346\n",
            "Epoch 597 Batch 0 Loss 0.1717 Accuracy 0.3401\n",
            "Epoch 597 Batch 50 Loss 0.1894 Accuracy 0.3360\n",
            "Epoch 597 Loss 0.1922 Accuracy 0.3350\n",
            "Epoch 598 Batch 0 Loss 0.1725 Accuracy 0.3424\n",
            "Epoch 598 Batch 50 Loss 0.1897 Accuracy 0.3352\n",
            "Epoch 598 Loss 0.1924 Accuracy 0.3346\n",
            "Epoch 599 Batch 0 Loss 0.1726 Accuracy 0.3377\n",
            "Epoch 599 Batch 50 Loss 0.1899 Accuracy 0.3357\n",
            "Epoch 599 Loss 0.1925 Accuracy 0.3347\n",
            "Epoch 600 Batch 0 Loss 0.1760 Accuracy 0.3368\n",
            "Epoch 600 Batch 50 Loss 0.1898 Accuracy 0.3357\n",
            "Epoch 600 Loss 0.1925 Accuracy 0.3350\n",
            "Epoch 601 Batch 0 Loss 0.1655 Accuracy 0.3510\n",
            "Epoch 601 Batch 50 Loss 0.1899 Accuracy 0.3356\n",
            "Epoch 601 Loss 0.1919 Accuracy 0.3351\n",
            "Epoch 602 Batch 0 Loss 0.1758 Accuracy 0.3391\n",
            "Epoch 602 Batch 50 Loss 0.1888 Accuracy 0.3360\n",
            "Epoch 602 Loss 0.1917 Accuracy 0.3351\n",
            "Epoch 603 Batch 0 Loss 0.1769 Accuracy 0.3418\n",
            "Epoch 603 Batch 50 Loss 0.1889 Accuracy 0.3355\n",
            "Epoch 603 Loss 0.1921 Accuracy 0.3348\n",
            "Epoch 604 Batch 0 Loss 0.1806 Accuracy 0.3413\n",
            "Epoch 604 Batch 50 Loss 0.1889 Accuracy 0.3357\n",
            "Epoch 604 Loss 0.1913 Accuracy 0.3352\n",
            "Epoch 605 Batch 0 Loss 0.1658 Accuracy 0.3507\n",
            "Epoch 605 Batch 50 Loss 0.1882 Accuracy 0.3359\n",
            "Epoch 605 Loss 0.1914 Accuracy 0.3350\n",
            "Epoch 606 Batch 0 Loss 0.1694 Accuracy 0.3376\n",
            "Epoch 606 Batch 50 Loss 0.1898 Accuracy 0.3356\n",
            "Epoch 606 Loss 0.1926 Accuracy 0.3346\n",
            "Epoch 607 Batch 0 Loss 0.1702 Accuracy 0.3370\n",
            "Epoch 607 Batch 50 Loss 0.1896 Accuracy 0.3357\n",
            "Epoch 607 Loss 0.1922 Accuracy 0.3348\n",
            "Epoch 608 Batch 0 Loss 0.1726 Accuracy 0.3434\n",
            "Epoch 608 Batch 50 Loss 0.1893 Accuracy 0.3357\n",
            "Epoch 608 Loss 0.1917 Accuracy 0.3349\n",
            "Epoch 609 Batch 0 Loss 0.1658 Accuracy 0.3431\n",
            "Epoch 609 Batch 50 Loss 0.1893 Accuracy 0.3355\n",
            "Epoch 609 Loss 0.1916 Accuracy 0.3351\n",
            "Epoch 610 Batch 0 Loss 0.1685 Accuracy 0.3385\n",
            "Epoch 610 Batch 50 Loss 0.1898 Accuracy 0.3354\n",
            "Epoch 610 Loss 0.1920 Accuracy 0.3350\n",
            "Epoch 611 Batch 0 Loss 0.1675 Accuracy 0.3397\n",
            "Epoch 611 Batch 50 Loss 0.1902 Accuracy 0.3353\n",
            "Epoch 611 Loss 0.1924 Accuracy 0.3349\n",
            "Epoch 612 Batch 0 Loss 0.1756 Accuracy 0.3453\n",
            "Epoch 612 Batch 50 Loss 0.1899 Accuracy 0.3351\n",
            "Epoch 612 Loss 0.1922 Accuracy 0.3346\n",
            "Epoch 613 Batch 0 Loss 0.1683 Accuracy 0.3469\n",
            "Epoch 613 Batch 50 Loss 0.1888 Accuracy 0.3359\n",
            "Epoch 613 Loss 0.1915 Accuracy 0.3350\n",
            "Epoch 614 Batch 0 Loss 0.1655 Accuracy 0.3478\n",
            "Epoch 614 Batch 50 Loss 0.1891 Accuracy 0.3356\n",
            "Epoch 614 Loss 0.1916 Accuracy 0.3349\n",
            "Epoch 615 Batch 0 Loss 0.1748 Accuracy 0.3341\n",
            "Epoch 615 Batch 50 Loss 0.1892 Accuracy 0.3353\n",
            "Epoch 615 Loss 0.1916 Accuracy 0.3348\n",
            "Epoch 616 Batch 0 Loss 0.1845 Accuracy 0.3392\n",
            "Epoch 616 Batch 50 Loss 0.1901 Accuracy 0.3352\n",
            "Epoch 616 Loss 0.1925 Accuracy 0.3345\n",
            "Epoch 617 Batch 0 Loss 0.1755 Accuracy 0.3403\n",
            "Epoch 617 Batch 50 Loss 0.1891 Accuracy 0.3353\n",
            "Epoch 617 Loss 0.1915 Accuracy 0.3348\n",
            "Epoch 618 Batch 0 Loss 0.1686 Accuracy 0.3401\n",
            "Epoch 618 Batch 50 Loss 0.1898 Accuracy 0.3354\n",
            "Epoch 618 Loss 0.1928 Accuracy 0.3346\n",
            "Epoch 619 Batch 0 Loss 0.1793 Accuracy 0.3400\n",
            "Epoch 619 Batch 50 Loss 0.1895 Accuracy 0.3356\n",
            "Epoch 619 Loss 0.1918 Accuracy 0.3350\n",
            "Epoch 620 Batch 0 Loss 0.1739 Accuracy 0.3415\n",
            "Epoch 620 Batch 50 Loss 0.1893 Accuracy 0.3352\n",
            "Epoch 620 Loss 0.1913 Accuracy 0.3349\n",
            "Epoch 621 Batch 0 Loss 0.1820 Accuracy 0.3439\n",
            "Epoch 621 Batch 50 Loss 0.1885 Accuracy 0.3359\n",
            "Epoch 621 Loss 0.1919 Accuracy 0.3351\n",
            "Epoch 622 Batch 0 Loss 0.1812 Accuracy 0.3427\n",
            "Epoch 622 Batch 50 Loss 0.1902 Accuracy 0.3355\n",
            "Epoch 622 Loss 0.1921 Accuracy 0.3347\n",
            "Epoch 623 Batch 0 Loss 0.1828 Accuracy 0.3382\n",
            "Epoch 623 Batch 50 Loss 0.1894 Accuracy 0.3353\n",
            "Epoch 623 Loss 0.1914 Accuracy 0.3347\n",
            "Epoch 624 Batch 0 Loss 0.1651 Accuracy 0.3409\n",
            "Epoch 624 Batch 50 Loss 0.1895 Accuracy 0.3355\n",
            "Epoch 624 Loss 0.1917 Accuracy 0.3349\n",
            "Epoch 625 Batch 0 Loss 0.1865 Accuracy 0.3350\n",
            "Epoch 625 Batch 50 Loss 0.1895 Accuracy 0.3357\n",
            "Epoch 625 Loss 0.1921 Accuracy 0.3350\n",
            "Epoch 626 Batch 0 Loss 0.1763 Accuracy 0.3317\n",
            "Epoch 626 Batch 50 Loss 0.1899 Accuracy 0.3353\n",
            "Epoch 626 Loss 0.1924 Accuracy 0.3347\n",
            "Epoch 627 Batch 0 Loss 0.1722 Accuracy 0.3415\n",
            "Epoch 627 Batch 50 Loss 0.1886 Accuracy 0.3356\n",
            "Epoch 627 Loss 0.1911 Accuracy 0.3349\n",
            "Epoch 628 Batch 0 Loss 0.1802 Accuracy 0.3364\n",
            "Epoch 628 Batch 50 Loss 0.1889 Accuracy 0.3357\n",
            "Epoch 628 Loss 0.1916 Accuracy 0.3350\n",
            "Epoch 629 Batch 0 Loss 0.1770 Accuracy 0.3388\n",
            "Epoch 629 Batch 50 Loss 0.1879 Accuracy 0.3362\n",
            "Epoch 629 Loss 0.1907 Accuracy 0.3352\n",
            "Epoch 630 Batch 0 Loss 0.1711 Accuracy 0.3347\n",
            "Epoch 630 Batch 50 Loss 0.1898 Accuracy 0.3355\n",
            "Epoch 630 Loss 0.1916 Accuracy 0.3348\n",
            "Epoch 631 Batch 0 Loss 0.1833 Accuracy 0.3392\n",
            "Epoch 631 Batch 50 Loss 0.1896 Accuracy 0.3355\n",
            "Epoch 631 Loss 0.1916 Accuracy 0.3350\n",
            "Epoch 632 Batch 0 Loss 0.1755 Accuracy 0.3398\n",
            "Epoch 632 Batch 50 Loss 0.1904 Accuracy 0.3355\n",
            "Epoch 632 Loss 0.1923 Accuracy 0.3346\n",
            "Epoch 633 Batch 0 Loss 0.1761 Accuracy 0.3415\n",
            "Epoch 633 Batch 50 Loss 0.1883 Accuracy 0.3357\n",
            "Epoch 633 Loss 0.1907 Accuracy 0.3349\n",
            "Epoch 634 Batch 0 Loss 0.1764 Accuracy 0.3442\n",
            "Epoch 634 Batch 50 Loss 0.1882 Accuracy 0.3356\n",
            "Epoch 634 Loss 0.1910 Accuracy 0.3350\n",
            "Epoch 635 Batch 0 Loss 0.1638 Accuracy 0.3469\n",
            "Epoch 635 Batch 50 Loss 0.1884 Accuracy 0.3354\n",
            "Epoch 635 Loss 0.1913 Accuracy 0.3347\n",
            "Epoch 636 Batch 0 Loss 0.1871 Accuracy 0.3319\n",
            "Epoch 636 Batch 50 Loss 0.1883 Accuracy 0.3357\n",
            "Epoch 636 Loss 0.1913 Accuracy 0.3347\n",
            "Epoch 637 Batch 0 Loss 0.1738 Accuracy 0.3382\n",
            "Epoch 637 Batch 50 Loss 0.1887 Accuracy 0.3353\n",
            "Epoch 637 Loss 0.1917 Accuracy 0.3346\n",
            "Epoch 638 Batch 0 Loss 0.1737 Accuracy 0.3391\n",
            "Epoch 638 Batch 50 Loss 0.1893 Accuracy 0.3356\n",
            "Epoch 638 Loss 0.1914 Accuracy 0.3350\n",
            "Epoch 639 Batch 0 Loss 0.1759 Accuracy 0.3356\n",
            "Epoch 639 Batch 50 Loss 0.1891 Accuracy 0.3353\n",
            "Epoch 639 Loss 0.1918 Accuracy 0.3346\n",
            "Epoch 640 Batch 0 Loss 0.1682 Accuracy 0.3434\n",
            "Epoch 640 Batch 50 Loss 0.1894 Accuracy 0.3352\n",
            "Epoch 640 Loss 0.1909 Accuracy 0.3348\n",
            "Epoch 641 Batch 0 Loss 0.1657 Accuracy 0.3445\n",
            "Epoch 641 Batch 50 Loss 0.1883 Accuracy 0.3361\n",
            "Epoch 641 Loss 0.1908 Accuracy 0.3351\n",
            "Epoch 642 Batch 0 Loss 0.1837 Accuracy 0.3334\n",
            "Epoch 642 Batch 50 Loss 0.1890 Accuracy 0.3354\n",
            "Epoch 642 Loss 0.1915 Accuracy 0.3349\n",
            "Epoch 643 Batch 0 Loss 0.1797 Accuracy 0.3349\n",
            "Epoch 643 Batch 50 Loss 0.1880 Accuracy 0.3359\n",
            "Epoch 643 Loss 0.1903 Accuracy 0.3351\n",
            "Epoch 644 Batch 0 Loss 0.1621 Accuracy 0.3471\n",
            "Epoch 644 Batch 50 Loss 0.1893 Accuracy 0.3352\n",
            "Epoch 644 Loss 0.1913 Accuracy 0.3346\n",
            "Epoch 645 Batch 0 Loss 0.1745 Accuracy 0.3362\n",
            "Epoch 645 Batch 50 Loss 0.1885 Accuracy 0.3361\n",
            "Epoch 645 Loss 0.1910 Accuracy 0.3353\n",
            "Epoch 646 Batch 0 Loss 0.1802 Accuracy 0.3332\n",
            "Epoch 646 Batch 50 Loss 0.1883 Accuracy 0.3358\n",
            "Epoch 646 Loss 0.1908 Accuracy 0.3351\n",
            "Epoch 647 Batch 0 Loss 0.1651 Accuracy 0.3450\n",
            "Epoch 647 Batch 50 Loss 0.1882 Accuracy 0.3359\n",
            "Epoch 647 Loss 0.1905 Accuracy 0.3352\n",
            "Epoch 648 Batch 0 Loss 0.1643 Accuracy 0.3436\n",
            "Epoch 648 Batch 50 Loss 0.1885 Accuracy 0.3357\n",
            "Epoch 648 Loss 0.1911 Accuracy 0.3349\n",
            "Epoch 649 Batch 0 Loss 0.1647 Accuracy 0.3447\n",
            "Epoch 649 Batch 50 Loss 0.1885 Accuracy 0.3361\n",
            "Epoch 649 Loss 0.1908 Accuracy 0.3353\n",
            "Epoch 650 Batch 0 Loss 0.1597 Accuracy 0.3466\n",
            "Epoch 650 Batch 50 Loss 0.1887 Accuracy 0.3357\n",
            "Epoch 650 Loss 0.1919 Accuracy 0.3347\n",
            "Epoch 651 Batch 0 Loss 0.1659 Accuracy 0.3406\n",
            "Epoch 651 Batch 50 Loss 0.1878 Accuracy 0.3358\n",
            "Epoch 651 Loss 0.1904 Accuracy 0.3350\n",
            "Epoch 652 Batch 0 Loss 0.1611 Accuracy 0.3445\n",
            "Epoch 652 Batch 50 Loss 0.1879 Accuracy 0.3359\n",
            "Epoch 652 Loss 0.1901 Accuracy 0.3350\n",
            "Epoch 653 Batch 0 Loss 0.1813 Accuracy 0.3319\n",
            "Epoch 653 Batch 50 Loss 0.1889 Accuracy 0.3359\n",
            "Epoch 653 Loss 0.1913 Accuracy 0.3350\n",
            "Epoch 654 Batch 0 Loss 0.1774 Accuracy 0.3422\n",
            "Epoch 654 Batch 50 Loss 0.1891 Accuracy 0.3349\n",
            "Epoch 654 Loss 0.1914 Accuracy 0.3346\n",
            "Epoch 655 Batch 0 Loss 0.1749 Accuracy 0.3388\n",
            "Epoch 655 Batch 50 Loss 0.1885 Accuracy 0.3352\n",
            "Epoch 655 Loss 0.1905 Accuracy 0.3349\n",
            "Epoch 656 Batch 0 Loss 0.1744 Accuracy 0.3367\n",
            "Epoch 656 Batch 50 Loss 0.1886 Accuracy 0.3359\n",
            "Epoch 656 Loss 0.1907 Accuracy 0.3350\n",
            "Epoch 657 Batch 0 Loss 0.1859 Accuracy 0.3346\n",
            "Epoch 657 Batch 50 Loss 0.1887 Accuracy 0.3356\n",
            "Epoch 657 Loss 0.1915 Accuracy 0.3348\n",
            "Epoch 658 Batch 0 Loss 0.1713 Accuracy 0.3409\n",
            "Epoch 658 Batch 50 Loss 0.1891 Accuracy 0.3353\n",
            "Epoch 658 Loss 0.1913 Accuracy 0.3348\n",
            "Epoch 659 Batch 0 Loss 0.1665 Accuracy 0.3427\n",
            "Epoch 659 Batch 50 Loss 0.1883 Accuracy 0.3354\n",
            "Epoch 659 Loss 0.1904 Accuracy 0.3349\n",
            "Epoch 660 Batch 0 Loss 0.1646 Accuracy 0.3474\n",
            "Epoch 660 Batch 50 Loss 0.1884 Accuracy 0.3355\n",
            "Epoch 660 Loss 0.1903 Accuracy 0.3349\n",
            "Epoch 661 Batch 0 Loss 0.1786 Accuracy 0.3356\n",
            "Epoch 661 Batch 50 Loss 0.1872 Accuracy 0.3357\n",
            "Epoch 661 Loss 0.1902 Accuracy 0.3350\n",
            "Epoch 662 Batch 0 Loss 0.1668 Accuracy 0.3416\n",
            "Epoch 662 Batch 50 Loss 0.1887 Accuracy 0.3355\n",
            "Epoch 662 Loss 0.1905 Accuracy 0.3349\n",
            "Epoch 663 Batch 0 Loss 0.1761 Accuracy 0.3373\n",
            "Epoch 663 Batch 50 Loss 0.1882 Accuracy 0.3355\n",
            "Epoch 663 Loss 0.1906 Accuracy 0.3348\n",
            "Epoch 664 Batch 0 Loss 0.1696 Accuracy 0.3398\n",
            "Epoch 664 Batch 50 Loss 0.1888 Accuracy 0.3356\n",
            "Epoch 664 Loss 0.1908 Accuracy 0.3349\n",
            "Epoch 665 Batch 0 Loss 0.1785 Accuracy 0.3394\n",
            "Epoch 665 Batch 50 Loss 0.1876 Accuracy 0.3358\n",
            "Epoch 665 Loss 0.1899 Accuracy 0.3351\n",
            "Epoch 666 Batch 0 Loss 0.1680 Accuracy 0.3415\n",
            "Epoch 666 Batch 50 Loss 0.1876 Accuracy 0.3360\n",
            "Epoch 666 Loss 0.1902 Accuracy 0.3350\n",
            "Epoch 667 Batch 0 Loss 0.1846 Accuracy 0.3350\n",
            "Epoch 667 Batch 50 Loss 0.1883 Accuracy 0.3354\n",
            "Epoch 667 Loss 0.1902 Accuracy 0.3349\n",
            "Epoch 668 Batch 0 Loss 0.1743 Accuracy 0.3404\n",
            "Epoch 668 Batch 50 Loss 0.1884 Accuracy 0.3355\n",
            "Epoch 668 Loss 0.1907 Accuracy 0.3347\n",
            "Epoch 669 Batch 0 Loss 0.1636 Accuracy 0.3406\n",
            "Epoch 669 Batch 50 Loss 0.1884 Accuracy 0.3356\n",
            "Epoch 669 Loss 0.1905 Accuracy 0.3349\n",
            "Epoch 670 Batch 0 Loss 0.1809 Accuracy 0.3377\n",
            "Epoch 670 Batch 50 Loss 0.1883 Accuracy 0.3360\n",
            "Epoch 670 Loss 0.1905 Accuracy 0.3353\n",
            "Epoch 671 Batch 0 Loss 0.1737 Accuracy 0.3439\n",
            "Epoch 671 Batch 50 Loss 0.1885 Accuracy 0.3355\n",
            "Epoch 671 Loss 0.1903 Accuracy 0.3352\n",
            "Epoch 672 Batch 0 Loss 0.1867 Accuracy 0.3362\n",
            "Epoch 672 Batch 50 Loss 0.1873 Accuracy 0.3358\n",
            "Epoch 672 Loss 0.1897 Accuracy 0.3352\n",
            "Epoch 673 Batch 0 Loss 0.1729 Accuracy 0.3444\n",
            "Epoch 673 Batch 50 Loss 0.1875 Accuracy 0.3358\n",
            "Epoch 673 Loss 0.1909 Accuracy 0.3348\n",
            "Epoch 674 Batch 0 Loss 0.1697 Accuracy 0.3442\n",
            "Epoch 674 Batch 50 Loss 0.1873 Accuracy 0.3357\n",
            "Epoch 674 Loss 0.1903 Accuracy 0.3348\n",
            "Epoch 675 Batch 0 Loss 0.1903 Accuracy 0.3368\n",
            "Epoch 675 Batch 50 Loss 0.1885 Accuracy 0.3358\n",
            "Epoch 675 Loss 0.1903 Accuracy 0.3352\n",
            "Epoch 676 Batch 0 Loss 0.1692 Accuracy 0.3422\n",
            "Epoch 676 Batch 50 Loss 0.1877 Accuracy 0.3359\n",
            "Epoch 676 Loss 0.1900 Accuracy 0.3350\n",
            "Epoch 677 Batch 0 Loss 0.1775 Accuracy 0.3397\n",
            "Epoch 677 Batch 50 Loss 0.1890 Accuracy 0.3352\n",
            "Epoch 677 Loss 0.1906 Accuracy 0.3349\n",
            "Epoch 678 Batch 0 Loss 0.1759 Accuracy 0.3416\n",
            "Epoch 678 Batch 50 Loss 0.1870 Accuracy 0.3356\n",
            "Epoch 678 Loss 0.1900 Accuracy 0.3347\n",
            "Epoch 679 Batch 0 Loss 0.1603 Accuracy 0.3483\n",
            "Epoch 679 Batch 50 Loss 0.1868 Accuracy 0.3359\n",
            "Epoch 679 Loss 0.1894 Accuracy 0.3353\n",
            "Epoch 680 Batch 0 Loss 0.1767 Accuracy 0.3421\n",
            "Epoch 680 Batch 50 Loss 0.1871 Accuracy 0.3359\n",
            "Epoch 680 Loss 0.1901 Accuracy 0.3351\n",
            "Epoch 681 Batch 0 Loss 0.1801 Accuracy 0.3316\n",
            "Epoch 681 Batch 50 Loss 0.1880 Accuracy 0.3352\n",
            "Epoch 681 Loss 0.1898 Accuracy 0.3348\n",
            "Epoch 682 Batch 0 Loss 0.1725 Accuracy 0.3377\n",
            "Epoch 682 Batch 50 Loss 0.1879 Accuracy 0.3357\n",
            "Epoch 682 Loss 0.1900 Accuracy 0.3351\n",
            "Epoch 683 Batch 0 Loss 0.1800 Accuracy 0.3356\n",
            "Epoch 683 Batch 50 Loss 0.1874 Accuracy 0.3356\n",
            "Epoch 683 Loss 0.1897 Accuracy 0.3350\n",
            "Epoch 684 Batch 0 Loss 0.1684 Accuracy 0.3447\n",
            "Epoch 684 Batch 50 Loss 0.1880 Accuracy 0.3360\n",
            "Epoch 684 Loss 0.1903 Accuracy 0.3351\n",
            "Epoch 685 Batch 0 Loss 0.1720 Accuracy 0.3404\n",
            "Epoch 685 Batch 50 Loss 0.1876 Accuracy 0.3355\n",
            "Epoch 685 Loss 0.1904 Accuracy 0.3349\n",
            "Epoch 686 Batch 0 Loss 0.1725 Accuracy 0.3404\n",
            "Epoch 686 Batch 50 Loss 0.1877 Accuracy 0.3355\n",
            "Epoch 686 Loss 0.1897 Accuracy 0.3350\n",
            "Epoch 687 Batch 0 Loss 0.1702 Accuracy 0.3433\n",
            "Epoch 687 Batch 50 Loss 0.1879 Accuracy 0.3357\n",
            "Epoch 687 Loss 0.1904 Accuracy 0.3351\n",
            "Epoch 688 Batch 0 Loss 0.1733 Accuracy 0.3377\n",
            "Epoch 688 Batch 50 Loss 0.1881 Accuracy 0.3354\n",
            "Epoch 688 Loss 0.1903 Accuracy 0.3347\n",
            "Epoch 689 Batch 0 Loss 0.1811 Accuracy 0.3379\n",
            "Epoch 689 Batch 50 Loss 0.1871 Accuracy 0.3354\n",
            "Epoch 689 Loss 0.1894 Accuracy 0.3349\n",
            "Epoch 690 Batch 0 Loss 0.1737 Accuracy 0.3428\n",
            "Epoch 690 Batch 50 Loss 0.1873 Accuracy 0.3359\n",
            "Epoch 690 Loss 0.1901 Accuracy 0.3351\n",
            "Epoch 691 Batch 0 Loss 0.1775 Accuracy 0.3415\n",
            "Epoch 691 Batch 50 Loss 0.1873 Accuracy 0.3354\n",
            "Epoch 691 Loss 0.1898 Accuracy 0.3348\n",
            "Epoch 692 Batch 0 Loss 0.1677 Accuracy 0.3409\n",
            "Epoch 692 Batch 50 Loss 0.1869 Accuracy 0.3357\n",
            "Epoch 692 Loss 0.1895 Accuracy 0.3352\n",
            "Epoch 693 Batch 0 Loss 0.1758 Accuracy 0.3395\n",
            "Epoch 693 Batch 50 Loss 0.1874 Accuracy 0.3357\n",
            "Epoch 693 Loss 0.1896 Accuracy 0.3352\n",
            "Epoch 694 Batch 0 Loss 0.1805 Accuracy 0.3392\n",
            "Epoch 694 Batch 50 Loss 0.1881 Accuracy 0.3355\n",
            "Epoch 694 Loss 0.1900 Accuracy 0.3349\n",
            "Epoch 695 Batch 0 Loss 0.1756 Accuracy 0.3352\n",
            "Epoch 695 Batch 50 Loss 0.1873 Accuracy 0.3354\n",
            "Epoch 695 Loss 0.1898 Accuracy 0.3348\n",
            "Epoch 696 Batch 0 Loss 0.1706 Accuracy 0.3428\n",
            "Epoch 696 Batch 50 Loss 0.1880 Accuracy 0.3353\n",
            "Epoch 696 Loss 0.1898 Accuracy 0.3350\n",
            "Epoch 697 Batch 0 Loss 0.1631 Accuracy 0.3436\n",
            "Epoch 697 Batch 50 Loss 0.1881 Accuracy 0.3354\n",
            "Epoch 697 Loss 0.1905 Accuracy 0.3350\n",
            "Epoch 698 Batch 0 Loss 0.1777 Accuracy 0.3386\n",
            "Epoch 698 Batch 50 Loss 0.1874 Accuracy 0.3356\n",
            "Epoch 698 Loss 0.1897 Accuracy 0.3350\n",
            "Epoch 699 Batch 0 Loss 0.1569 Accuracy 0.3453\n",
            "Epoch 699 Batch 50 Loss 0.1865 Accuracy 0.3361\n",
            "Epoch 699 Loss 0.1894 Accuracy 0.3351\n",
            "Epoch 700 Batch 0 Loss 0.1751 Accuracy 0.3377\n",
            "Epoch 700 Batch 50 Loss 0.1875 Accuracy 0.3358\n",
            "Epoch 700 Loss 0.1898 Accuracy 0.3351\n",
            "Epoch 701 Batch 0 Loss 0.1690 Accuracy 0.3380\n",
            "Epoch 701 Batch 50 Loss 0.1870 Accuracy 0.3360\n",
            "Epoch 701 Loss 0.1893 Accuracy 0.3351\n",
            "Epoch 702 Batch 0 Loss 0.1620 Accuracy 0.3448\n",
            "Epoch 702 Batch 50 Loss 0.1870 Accuracy 0.3359\n",
            "Epoch 702 Loss 0.1893 Accuracy 0.3352\n",
            "Epoch 703 Batch 0 Loss 0.1694 Accuracy 0.3434\n",
            "Epoch 703 Batch 50 Loss 0.1872 Accuracy 0.3356\n",
            "Epoch 703 Loss 0.1893 Accuracy 0.3351\n",
            "Epoch 704 Batch 0 Loss 0.1836 Accuracy 0.3310\n",
            "Epoch 704 Batch 50 Loss 0.1866 Accuracy 0.3361\n",
            "Epoch 704 Loss 0.1896 Accuracy 0.3351\n",
            "Epoch 705 Batch 0 Loss 0.1629 Accuracy 0.3431\n",
            "Epoch 705 Batch 50 Loss 0.1865 Accuracy 0.3360\n",
            "Epoch 705 Loss 0.1891 Accuracy 0.3353\n",
            "Epoch 706 Batch 0 Loss 0.1708 Accuracy 0.3409\n",
            "Epoch 706 Batch 50 Loss 0.1871 Accuracy 0.3357\n",
            "Epoch 706 Loss 0.1890 Accuracy 0.3353\n",
            "Epoch 707 Batch 0 Loss 0.1765 Accuracy 0.3353\n",
            "Epoch 707 Batch 50 Loss 0.1860 Accuracy 0.3358\n",
            "Epoch 707 Loss 0.1886 Accuracy 0.3352\n",
            "Epoch 708 Batch 0 Loss 0.1788 Accuracy 0.3371\n",
            "Epoch 708 Batch 50 Loss 0.1872 Accuracy 0.3355\n",
            "Epoch 708 Loss 0.1900 Accuracy 0.3348\n",
            "Epoch 709 Batch 0 Loss 0.1679 Accuracy 0.3404\n",
            "Epoch 709 Batch 50 Loss 0.1865 Accuracy 0.3354\n",
            "Epoch 709 Loss 0.1885 Accuracy 0.3352\n",
            "Epoch 710 Batch 0 Loss 0.1701 Accuracy 0.3412\n",
            "Epoch 710 Batch 50 Loss 0.1862 Accuracy 0.3360\n",
            "Epoch 710 Loss 0.1890 Accuracy 0.3351\n",
            "Epoch 711 Batch 0 Loss 0.1643 Accuracy 0.3474\n",
            "Epoch 711 Batch 50 Loss 0.1877 Accuracy 0.3355\n",
            "Epoch 711 Loss 0.1895 Accuracy 0.3351\n",
            "Epoch 712 Batch 0 Loss 0.1669 Accuracy 0.3442\n",
            "Epoch 712 Batch 50 Loss 0.1862 Accuracy 0.3361\n",
            "Epoch 712 Loss 0.1884 Accuracy 0.3354\n",
            "Epoch 713 Batch 0 Loss 0.1710 Accuracy 0.3356\n",
            "Epoch 713 Batch 50 Loss 0.1873 Accuracy 0.3357\n",
            "Epoch 713 Loss 0.1901 Accuracy 0.3349\n",
            "Epoch 714 Batch 0 Loss 0.1686 Accuracy 0.3442\n",
            "Epoch 714 Batch 50 Loss 0.1870 Accuracy 0.3359\n",
            "Epoch 714 Loss 0.1892 Accuracy 0.3350\n",
            "Epoch 715 Batch 0 Loss 0.1610 Accuracy 0.3430\n",
            "Epoch 715 Batch 50 Loss 0.1871 Accuracy 0.3361\n",
            "Epoch 715 Loss 0.1890 Accuracy 0.3355\n",
            "Epoch 716 Batch 0 Loss 0.1674 Accuracy 0.3394\n",
            "Epoch 716 Batch 50 Loss 0.1866 Accuracy 0.3356\n",
            "Epoch 716 Loss 0.1888 Accuracy 0.3350\n",
            "Epoch 717 Batch 0 Loss 0.1620 Accuracy 0.3450\n",
            "Epoch 717 Batch 50 Loss 0.1870 Accuracy 0.3360\n",
            "Epoch 717 Loss 0.1891 Accuracy 0.3351\n",
            "Epoch 718 Batch 0 Loss 0.1614 Accuracy 0.3453\n",
            "Epoch 718 Batch 50 Loss 0.1877 Accuracy 0.3355\n",
            "Epoch 718 Loss 0.1892 Accuracy 0.3350\n",
            "Epoch 719 Batch 0 Loss 0.1641 Accuracy 0.3439\n",
            "Epoch 719 Batch 50 Loss 0.1866 Accuracy 0.3358\n",
            "Epoch 719 Loss 0.1890 Accuracy 0.3351\n",
            "Epoch 720 Batch 0 Loss 0.1643 Accuracy 0.3434\n",
            "Epoch 720 Batch 50 Loss 0.1867 Accuracy 0.3360\n",
            "Epoch 720 Loss 0.1888 Accuracy 0.3354\n",
            "Epoch 721 Batch 0 Loss 0.1707 Accuracy 0.3447\n",
            "Epoch 721 Batch 50 Loss 0.1869 Accuracy 0.3357\n",
            "Epoch 721 Loss 0.1895 Accuracy 0.3350\n",
            "Epoch 722 Batch 0 Loss 0.1721 Accuracy 0.3391\n",
            "Epoch 722 Batch 50 Loss 0.1869 Accuracy 0.3360\n",
            "Epoch 722 Loss 0.1888 Accuracy 0.3353\n",
            "Epoch 723 Batch 0 Loss 0.1658 Accuracy 0.3409\n",
            "Epoch 723 Batch 50 Loss 0.1864 Accuracy 0.3357\n",
            "Epoch 723 Loss 0.1887 Accuracy 0.3352\n",
            "Epoch 724 Batch 0 Loss 0.1739 Accuracy 0.3377\n",
            "Epoch 724 Batch 50 Loss 0.1871 Accuracy 0.3357\n",
            "Epoch 724 Loss 0.1894 Accuracy 0.3352\n",
            "Epoch 725 Batch 0 Loss 0.1721 Accuracy 0.3353\n",
            "Epoch 725 Batch 50 Loss 0.1859 Accuracy 0.3359\n",
            "Epoch 725 Loss 0.1886 Accuracy 0.3352\n",
            "Epoch 726 Batch 0 Loss 0.1822 Accuracy 0.3383\n",
            "Epoch 726 Batch 50 Loss 0.1865 Accuracy 0.3356\n",
            "Epoch 726 Loss 0.1892 Accuracy 0.3349\n",
            "Epoch 727 Batch 0 Loss 0.1833 Accuracy 0.3404\n",
            "Epoch 727 Batch 50 Loss 0.1859 Accuracy 0.3362\n",
            "Epoch 727 Loss 0.1880 Accuracy 0.3353\n",
            "Epoch 728 Batch 0 Loss 0.1822 Accuracy 0.3337\n",
            "Epoch 728 Batch 50 Loss 0.1864 Accuracy 0.3357\n",
            "Epoch 728 Loss 0.1887 Accuracy 0.3350\n",
            "Epoch 729 Batch 0 Loss 0.1691 Accuracy 0.3358\n",
            "Epoch 729 Batch 50 Loss 0.1873 Accuracy 0.3356\n",
            "Epoch 729 Loss 0.1894 Accuracy 0.3349\n",
            "Epoch 730 Batch 0 Loss 0.1727 Accuracy 0.3439\n",
            "Epoch 730 Batch 50 Loss 0.1862 Accuracy 0.3359\n",
            "Epoch 730 Loss 0.1880 Accuracy 0.3354\n",
            "Epoch 731 Batch 0 Loss 0.1664 Accuracy 0.3425\n",
            "Epoch 731 Batch 50 Loss 0.1871 Accuracy 0.3356\n",
            "Epoch 731 Loss 0.1888 Accuracy 0.3351\n",
            "Epoch 732 Batch 0 Loss 0.1716 Accuracy 0.3475\n",
            "Epoch 732 Batch 50 Loss 0.1865 Accuracy 0.3359\n",
            "Epoch 732 Loss 0.1891 Accuracy 0.3350\n",
            "Epoch 733 Batch 0 Loss 0.1778 Accuracy 0.3367\n",
            "Epoch 733 Batch 50 Loss 0.1869 Accuracy 0.3357\n",
            "Epoch 733 Loss 0.1895 Accuracy 0.3351\n",
            "Epoch 734 Batch 0 Loss 0.1722 Accuracy 0.3350\n",
            "Epoch 734 Batch 50 Loss 0.1869 Accuracy 0.3358\n",
            "Epoch 734 Loss 0.1892 Accuracy 0.3351\n",
            "Epoch 735 Batch 0 Loss 0.1799 Accuracy 0.3368\n",
            "Epoch 735 Batch 50 Loss 0.1853 Accuracy 0.3361\n",
            "Epoch 735 Loss 0.1880 Accuracy 0.3351\n",
            "Epoch 736 Batch 0 Loss 0.1730 Accuracy 0.3412\n",
            "Epoch 736 Batch 50 Loss 0.1862 Accuracy 0.3358\n",
            "Epoch 736 Loss 0.1890 Accuracy 0.3349\n",
            "Epoch 737 Batch 0 Loss 0.1691 Accuracy 0.3421\n",
            "Epoch 737 Batch 50 Loss 0.1852 Accuracy 0.3357\n",
            "Epoch 737 Loss 0.1879 Accuracy 0.3351\n",
            "Epoch 738 Batch 0 Loss 0.1677 Accuracy 0.3409\n",
            "Epoch 738 Batch 50 Loss 0.1858 Accuracy 0.3360\n",
            "Epoch 738 Loss 0.1885 Accuracy 0.3352\n",
            "Epoch 739 Batch 0 Loss 0.1680 Accuracy 0.3451\n",
            "Epoch 739 Batch 50 Loss 0.1866 Accuracy 0.3360\n",
            "Epoch 739 Loss 0.1891 Accuracy 0.3353\n",
            "Epoch 740 Batch 0 Loss 0.1663 Accuracy 0.3400\n",
            "Epoch 740 Batch 50 Loss 0.1864 Accuracy 0.3356\n",
            "Epoch 740 Loss 0.1886 Accuracy 0.3353\n",
            "Epoch 741 Batch 0 Loss 0.1796 Accuracy 0.3419\n",
            "Epoch 741 Batch 50 Loss 0.1865 Accuracy 0.3358\n",
            "Epoch 741 Loss 0.1885 Accuracy 0.3351\n",
            "Epoch 742 Batch 0 Loss 0.1698 Accuracy 0.3427\n",
            "Epoch 742 Batch 50 Loss 0.1862 Accuracy 0.3358\n",
            "Epoch 742 Loss 0.1881 Accuracy 0.3352\n",
            "Epoch 743 Batch 0 Loss 0.1738 Accuracy 0.3401\n",
            "Epoch 743 Batch 50 Loss 0.1866 Accuracy 0.3361\n",
            "Epoch 743 Loss 0.1889 Accuracy 0.3351\n",
            "Epoch 744 Batch 0 Loss 0.1674 Accuracy 0.3431\n",
            "Epoch 744 Batch 50 Loss 0.1855 Accuracy 0.3362\n",
            "Epoch 744 Loss 0.1885 Accuracy 0.3352\n",
            "Epoch 745 Batch 0 Loss 0.1798 Accuracy 0.3371\n",
            "Epoch 745 Batch 50 Loss 0.1862 Accuracy 0.3358\n",
            "Epoch 745 Loss 0.1886 Accuracy 0.3349\n",
            "Epoch 746 Batch 0 Loss 0.1678 Accuracy 0.3404\n",
            "Epoch 746 Batch 50 Loss 0.1865 Accuracy 0.3359\n",
            "Epoch 746 Loss 0.1883 Accuracy 0.3354\n",
            "Epoch 747 Batch 0 Loss 0.1632 Accuracy 0.3398\n",
            "Epoch 747 Batch 50 Loss 0.1871 Accuracy 0.3353\n",
            "Epoch 747 Loss 0.1890 Accuracy 0.3349\n",
            "Epoch 748 Batch 0 Loss 0.1785 Accuracy 0.3422\n",
            "Epoch 748 Batch 50 Loss 0.1863 Accuracy 0.3360\n",
            "Epoch 748 Loss 0.1893 Accuracy 0.3350\n",
            "Epoch 749 Batch 0 Loss 0.1723 Accuracy 0.3457\n",
            "Epoch 749 Batch 50 Loss 0.1862 Accuracy 0.3359\n",
            "Epoch 749 Loss 0.1888 Accuracy 0.3351\n",
            "Epoch 750 Batch 0 Loss 0.1640 Accuracy 0.3436\n",
            "Epoch 750 Batch 50 Loss 0.1849 Accuracy 0.3359\n",
            "Epoch 750 Loss 0.1877 Accuracy 0.3352\n",
            "Epoch 751 Batch 0 Loss 0.1808 Accuracy 0.3347\n",
            "Epoch 751 Batch 50 Loss 0.1860 Accuracy 0.3358\n",
            "Epoch 751 Loss 0.1884 Accuracy 0.3350\n",
            "Epoch 752 Batch 0 Loss 0.1705 Accuracy 0.3379\n",
            "Epoch 752 Batch 50 Loss 0.1863 Accuracy 0.3357\n",
            "Epoch 752 Loss 0.1888 Accuracy 0.3349\n",
            "Epoch 753 Batch 0 Loss 0.1654 Accuracy 0.3364\n",
            "Epoch 753 Batch 50 Loss 0.1868 Accuracy 0.3356\n",
            "Epoch 753 Loss 0.1884 Accuracy 0.3349\n",
            "Epoch 754 Batch 0 Loss 0.1763 Accuracy 0.3418\n",
            "Epoch 754 Batch 50 Loss 0.1864 Accuracy 0.3360\n",
            "Epoch 754 Loss 0.1886 Accuracy 0.3352\n",
            "Epoch 755 Batch 0 Loss 0.1799 Accuracy 0.3388\n",
            "Epoch 755 Batch 50 Loss 0.1871 Accuracy 0.3354\n",
            "Epoch 755 Loss 0.1894 Accuracy 0.3350\n",
            "Epoch 756 Batch 0 Loss 0.1682 Accuracy 0.3416\n",
            "Epoch 756 Batch 50 Loss 0.1868 Accuracy 0.3358\n",
            "Epoch 756 Loss 0.1885 Accuracy 0.3350\n",
            "Epoch 757 Batch 0 Loss 0.1650 Accuracy 0.3391\n",
            "Epoch 757 Batch 50 Loss 0.1859 Accuracy 0.3354\n",
            "Epoch 757 Loss 0.1879 Accuracy 0.3350\n",
            "Epoch 758 Batch 0 Loss 0.1875 Accuracy 0.3352\n",
            "Epoch 758 Batch 50 Loss 0.1867 Accuracy 0.3355\n",
            "Epoch 758 Loss 0.1886 Accuracy 0.3350\n",
            "Epoch 759 Batch 0 Loss 0.1736 Accuracy 0.3418\n",
            "Epoch 759 Batch 50 Loss 0.1865 Accuracy 0.3359\n",
            "Epoch 759 Loss 0.1883 Accuracy 0.3353\n",
            "Epoch 760 Batch 0 Loss 0.1619 Accuracy 0.3409\n",
            "Epoch 760 Batch 50 Loss 0.1859 Accuracy 0.3357\n",
            "Epoch 760 Loss 0.1882 Accuracy 0.3352\n",
            "Epoch 761 Batch 0 Loss 0.1662 Accuracy 0.3471\n",
            "Epoch 761 Batch 50 Loss 0.1864 Accuracy 0.3361\n",
            "Epoch 761 Loss 0.1885 Accuracy 0.3352\n",
            "Epoch 762 Batch 0 Loss 0.1701 Accuracy 0.3388\n",
            "Epoch 762 Batch 50 Loss 0.1860 Accuracy 0.3358\n",
            "Epoch 762 Loss 0.1880 Accuracy 0.3352\n",
            "Epoch 763 Batch 0 Loss 0.1668 Accuracy 0.3388\n",
            "Epoch 763 Batch 50 Loss 0.1853 Accuracy 0.3359\n",
            "Epoch 763 Loss 0.1879 Accuracy 0.3353\n",
            "Epoch 764 Batch 0 Loss 0.1751 Accuracy 0.3370\n",
            "Epoch 764 Batch 50 Loss 0.1861 Accuracy 0.3358\n",
            "Epoch 764 Loss 0.1883 Accuracy 0.3352\n",
            "Epoch 765 Batch 0 Loss 0.1803 Accuracy 0.3364\n",
            "Epoch 765 Batch 50 Loss 0.1859 Accuracy 0.3359\n",
            "Epoch 765 Loss 0.1890 Accuracy 0.3350\n",
            "Epoch 766 Batch 0 Loss 0.1734 Accuracy 0.3376\n",
            "Epoch 766 Batch 50 Loss 0.1873 Accuracy 0.3351\n",
            "Epoch 766 Loss 0.1884 Accuracy 0.3350\n",
            "Epoch 767 Batch 0 Loss 0.1695 Accuracy 0.3416\n",
            "Epoch 767 Batch 50 Loss 0.1858 Accuracy 0.3360\n",
            "Epoch 767 Loss 0.1876 Accuracy 0.3356\n",
            "Epoch 768 Batch 0 Loss 0.1800 Accuracy 0.3386\n",
            "Epoch 768 Batch 50 Loss 0.1856 Accuracy 0.3357\n",
            "Epoch 768 Loss 0.1878 Accuracy 0.3351\n",
            "Epoch 769 Batch 0 Loss 0.1687 Accuracy 0.3448\n",
            "Epoch 769 Batch 50 Loss 0.1856 Accuracy 0.3358\n",
            "Epoch 769 Loss 0.1877 Accuracy 0.3351\n",
            "Epoch 770 Batch 0 Loss 0.1829 Accuracy 0.3394\n",
            "Epoch 770 Batch 50 Loss 0.1860 Accuracy 0.3357\n",
            "Epoch 770 Loss 0.1880 Accuracy 0.3352\n",
            "Epoch 771 Batch 0 Loss 0.1642 Accuracy 0.3469\n",
            "Epoch 771 Batch 50 Loss 0.1847 Accuracy 0.3361\n",
            "Epoch 771 Loss 0.1875 Accuracy 0.3354\n",
            "Epoch 772 Batch 0 Loss 0.1718 Accuracy 0.3415\n",
            "Epoch 772 Batch 50 Loss 0.1858 Accuracy 0.3360\n",
            "Epoch 772 Loss 0.1884 Accuracy 0.3352\n",
            "Epoch 773 Batch 0 Loss 0.1732 Accuracy 0.3389\n",
            "Epoch 773 Batch 50 Loss 0.1865 Accuracy 0.3358\n",
            "Epoch 773 Loss 0.1880 Accuracy 0.3353\n",
            "Epoch 774 Batch 0 Loss 0.1688 Accuracy 0.3386\n",
            "Epoch 774 Batch 50 Loss 0.1861 Accuracy 0.3355\n",
            "Epoch 774 Loss 0.1880 Accuracy 0.3350\n",
            "Epoch 775 Batch 0 Loss 0.1628 Accuracy 0.3427\n",
            "Epoch 775 Batch 50 Loss 0.1862 Accuracy 0.3357\n",
            "Epoch 775 Loss 0.1885 Accuracy 0.3349\n",
            "Epoch 776 Batch 0 Loss 0.1716 Accuracy 0.3415\n",
            "Epoch 776 Batch 50 Loss 0.1848 Accuracy 0.3364\n",
            "Epoch 776 Loss 0.1874 Accuracy 0.3354\n",
            "Epoch 777 Batch 0 Loss 0.1686 Accuracy 0.3422\n",
            "Epoch 777 Batch 50 Loss 0.1858 Accuracy 0.3362\n",
            "Epoch 777 Loss 0.1882 Accuracy 0.3352\n",
            "Epoch 778 Batch 0 Loss 0.1675 Accuracy 0.3434\n",
            "Epoch 778 Batch 50 Loss 0.1857 Accuracy 0.3357\n",
            "Epoch 778 Loss 0.1878 Accuracy 0.3350\n",
            "Epoch 779 Batch 0 Loss 0.1669 Accuracy 0.3438\n",
            "Epoch 779 Batch 50 Loss 0.1859 Accuracy 0.3359\n",
            "Epoch 779 Loss 0.1881 Accuracy 0.3349\n",
            "Epoch 780 Batch 0 Loss 0.1734 Accuracy 0.3406\n",
            "Epoch 780 Batch 50 Loss 0.1848 Accuracy 0.3361\n",
            "Epoch 780 Loss 0.1877 Accuracy 0.3352\n",
            "Epoch 781 Batch 0 Loss 0.1753 Accuracy 0.3361\n",
            "Epoch 781 Batch 50 Loss 0.1858 Accuracy 0.3359\n",
            "Epoch 781 Loss 0.1879 Accuracy 0.3351\n",
            "Epoch 782 Batch 0 Loss 0.1857 Accuracy 0.3340\n",
            "Epoch 782 Batch 50 Loss 0.1856 Accuracy 0.3358\n",
            "Epoch 782 Loss 0.1882 Accuracy 0.3351\n",
            "Epoch 783 Batch 0 Loss 0.1789 Accuracy 0.3395\n",
            "Epoch 783 Batch 50 Loss 0.1863 Accuracy 0.3357\n",
            "Epoch 783 Loss 0.1880 Accuracy 0.3353\n",
            "Epoch 784 Batch 0 Loss 0.1673 Accuracy 0.3406\n",
            "Epoch 784 Batch 50 Loss 0.1863 Accuracy 0.3357\n",
            "Epoch 784 Loss 0.1879 Accuracy 0.3352\n",
            "Epoch 785 Batch 0 Loss 0.1703 Accuracy 0.3401\n",
            "Epoch 785 Batch 50 Loss 0.1862 Accuracy 0.3359\n",
            "Epoch 785 Loss 0.1883 Accuracy 0.3352\n",
            "Epoch 786 Batch 0 Loss 0.1736 Accuracy 0.3431\n",
            "Epoch 786 Batch 50 Loss 0.1856 Accuracy 0.3358\n",
            "Epoch 786 Loss 0.1875 Accuracy 0.3351\n",
            "Epoch 787 Batch 0 Loss 0.1741 Accuracy 0.3397\n",
            "Epoch 787 Batch 50 Loss 0.1850 Accuracy 0.3361\n",
            "Epoch 787 Loss 0.1874 Accuracy 0.3353\n",
            "Epoch 788 Batch 0 Loss 0.1643 Accuracy 0.3445\n",
            "Epoch 788 Batch 50 Loss 0.1858 Accuracy 0.3357\n",
            "Epoch 788 Loss 0.1883 Accuracy 0.3350\n",
            "Epoch 789 Batch 0 Loss 0.1698 Accuracy 0.3395\n",
            "Epoch 789 Batch 50 Loss 0.1851 Accuracy 0.3359\n",
            "Epoch 789 Loss 0.1874 Accuracy 0.3353\n",
            "Epoch 790 Batch 0 Loss 0.1797 Accuracy 0.3368\n",
            "Epoch 790 Batch 50 Loss 0.1860 Accuracy 0.3357\n",
            "Epoch 790 Loss 0.1882 Accuracy 0.3351\n",
            "Epoch 791 Batch 0 Loss 0.1666 Accuracy 0.3400\n",
            "Epoch 791 Batch 50 Loss 0.1849 Accuracy 0.3362\n",
            "Epoch 791 Loss 0.1876 Accuracy 0.3354\n",
            "Epoch 792 Batch 0 Loss 0.1749 Accuracy 0.3438\n",
            "Epoch 792 Batch 50 Loss 0.1852 Accuracy 0.3362\n",
            "Epoch 792 Loss 0.1882 Accuracy 0.3353\n",
            "Epoch 793 Batch 0 Loss 0.1673 Accuracy 0.3410\n",
            "Epoch 793 Batch 50 Loss 0.1856 Accuracy 0.3357\n",
            "Epoch 793 Loss 0.1876 Accuracy 0.3351\n",
            "Epoch 794 Batch 0 Loss 0.1676 Accuracy 0.3445\n",
            "Epoch 794 Batch 50 Loss 0.1854 Accuracy 0.3358\n",
            "Epoch 794 Loss 0.1874 Accuracy 0.3354\n",
            "Epoch 795 Batch 0 Loss 0.1797 Accuracy 0.3359\n",
            "Epoch 795 Batch 50 Loss 0.1857 Accuracy 0.3357\n",
            "Epoch 795 Loss 0.1874 Accuracy 0.3351\n",
            "Epoch 796 Batch 0 Loss 0.1697 Accuracy 0.3406\n",
            "Epoch 796 Batch 50 Loss 0.1858 Accuracy 0.3358\n",
            "Epoch 796 Loss 0.1880 Accuracy 0.3350\n",
            "Epoch 797 Batch 0 Loss 0.1714 Accuracy 0.3377\n",
            "Epoch 797 Batch 50 Loss 0.1846 Accuracy 0.3360\n",
            "Epoch 797 Loss 0.1872 Accuracy 0.3352\n",
            "Epoch 798 Batch 0 Loss 0.1730 Accuracy 0.3413\n",
            "Epoch 798 Batch 50 Loss 0.1850 Accuracy 0.3358\n",
            "Epoch 798 Loss 0.1873 Accuracy 0.3352\n",
            "Epoch 799 Batch 0 Loss 0.1606 Accuracy 0.3404\n",
            "Epoch 799 Batch 50 Loss 0.1860 Accuracy 0.3357\n",
            "Epoch 799 Loss 0.1877 Accuracy 0.3353\n",
            "Epoch 800 Batch 0 Loss 0.1729 Accuracy 0.3385\n",
            "Epoch 800 Batch 50 Loss 0.1854 Accuracy 0.3355\n",
            "Epoch 800 Loss 0.1874 Accuracy 0.3350\n",
            "Epoch 801 Batch 0 Loss 0.1743 Accuracy 0.3439\n",
            "Epoch 801 Batch 50 Loss 0.1858 Accuracy 0.3361\n",
            "Epoch 801 Loss 0.1881 Accuracy 0.3354\n",
            "Epoch 802 Batch 0 Loss 0.1739 Accuracy 0.3376\n",
            "Epoch 802 Batch 50 Loss 0.1857 Accuracy 0.3360\n",
            "Epoch 802 Loss 0.1875 Accuracy 0.3353\n",
            "Epoch 803 Batch 0 Loss 0.1652 Accuracy 0.3463\n",
            "Epoch 803 Batch 50 Loss 0.1864 Accuracy 0.3356\n",
            "Epoch 803 Loss 0.1882 Accuracy 0.3350\n",
            "Epoch 804 Batch 0 Loss 0.1583 Accuracy 0.3471\n",
            "Epoch 804 Batch 50 Loss 0.1854 Accuracy 0.3357\n",
            "Epoch 804 Loss 0.1877 Accuracy 0.3350\n",
            "Epoch 805 Batch 0 Loss 0.1749 Accuracy 0.3377\n",
            "Epoch 805 Batch 50 Loss 0.1856 Accuracy 0.3359\n",
            "Epoch 805 Loss 0.1874 Accuracy 0.3352\n",
            "Epoch 806 Batch 0 Loss 0.1767 Accuracy 0.3392\n",
            "Epoch 806 Batch 50 Loss 0.1851 Accuracy 0.3359\n",
            "Epoch 806 Loss 0.1876 Accuracy 0.3352\n",
            "Epoch 807 Batch 0 Loss 0.1655 Accuracy 0.3409\n",
            "Epoch 807 Batch 50 Loss 0.1844 Accuracy 0.3361\n",
            "Epoch 807 Loss 0.1866 Accuracy 0.3356\n",
            "Epoch 808 Batch 0 Loss 0.1707 Accuracy 0.3373\n",
            "Epoch 808 Batch 50 Loss 0.1854 Accuracy 0.3360\n",
            "Epoch 808 Loss 0.1875 Accuracy 0.3352\n",
            "Epoch 809 Batch 0 Loss 0.1579 Accuracy 0.3454\n",
            "Epoch 809 Batch 50 Loss 0.1843 Accuracy 0.3360\n",
            "Epoch 809 Loss 0.1875 Accuracy 0.3352\n",
            "Epoch 810 Batch 0 Loss 0.1781 Accuracy 0.3421\n",
            "Epoch 810 Batch 50 Loss 0.1851 Accuracy 0.3360\n",
            "Epoch 810 Loss 0.1871 Accuracy 0.3352\n",
            "Epoch 811 Batch 0 Loss 0.1869 Accuracy 0.3346\n",
            "Epoch 811 Batch 50 Loss 0.1852 Accuracy 0.3358\n",
            "Epoch 811 Loss 0.1874 Accuracy 0.3352\n",
            "Epoch 812 Batch 0 Loss 0.1772 Accuracy 0.3373\n",
            "Epoch 812 Batch 50 Loss 0.1846 Accuracy 0.3359\n",
            "Epoch 812 Loss 0.1872 Accuracy 0.3353\n",
            "Epoch 813 Batch 0 Loss 0.1707 Accuracy 0.3418\n",
            "Epoch 813 Batch 50 Loss 0.1852 Accuracy 0.3357\n",
            "Epoch 813 Loss 0.1874 Accuracy 0.3351\n",
            "Epoch 814 Batch 0 Loss 0.1722 Accuracy 0.3410\n",
            "Epoch 814 Batch 50 Loss 0.1850 Accuracy 0.3359\n",
            "Epoch 814 Loss 0.1870 Accuracy 0.3350\n",
            "Epoch 815 Batch 0 Loss 0.1593 Accuracy 0.3421\n",
            "Epoch 815 Batch 50 Loss 0.1847 Accuracy 0.3362\n",
            "Epoch 815 Loss 0.1868 Accuracy 0.3355\n",
            "Epoch 816 Batch 0 Loss 0.1641 Accuracy 0.3400\n",
            "Epoch 816 Batch 50 Loss 0.1857 Accuracy 0.3356\n",
            "Epoch 816 Loss 0.1876 Accuracy 0.3350\n",
            "Epoch 817 Batch 0 Loss 0.1699 Accuracy 0.3359\n",
            "Epoch 817 Batch 50 Loss 0.1844 Accuracy 0.3364\n",
            "Epoch 817 Loss 0.1868 Accuracy 0.3356\n",
            "Epoch 818 Batch 0 Loss 0.1595 Accuracy 0.3415\n",
            "Epoch 818 Batch 50 Loss 0.1847 Accuracy 0.3364\n",
            "Epoch 818 Loss 0.1873 Accuracy 0.3355\n",
            "Epoch 819 Batch 0 Loss 0.1620 Accuracy 0.3434\n",
            "Epoch 819 Batch 50 Loss 0.1866 Accuracy 0.3351\n",
            "Epoch 819 Loss 0.1874 Accuracy 0.3349\n",
            "Epoch 820 Batch 0 Loss 0.1764 Accuracy 0.3407\n",
            "Epoch 820 Batch 50 Loss 0.1856 Accuracy 0.3357\n",
            "Epoch 820 Loss 0.1874 Accuracy 0.3352\n",
            "Epoch 821 Batch 0 Loss 0.1763 Accuracy 0.3371\n",
            "Epoch 821 Batch 50 Loss 0.1858 Accuracy 0.3359\n",
            "Epoch 821 Loss 0.1877 Accuracy 0.3352\n",
            "Epoch 822 Batch 0 Loss 0.1671 Accuracy 0.3431\n",
            "Epoch 822 Batch 50 Loss 0.1841 Accuracy 0.3364\n",
            "Epoch 822 Loss 0.1868 Accuracy 0.3355\n",
            "Epoch 823 Batch 0 Loss 0.1767 Accuracy 0.3376\n",
            "Epoch 823 Batch 50 Loss 0.1853 Accuracy 0.3354\n",
            "Epoch 823 Loss 0.1874 Accuracy 0.3351\n",
            "Epoch 824 Batch 0 Loss 0.1814 Accuracy 0.3326\n",
            "Epoch 824 Batch 50 Loss 0.1851 Accuracy 0.3358\n",
            "Epoch 824 Loss 0.1873 Accuracy 0.3350\n",
            "Epoch 825 Batch 0 Loss 0.1754 Accuracy 0.3424\n",
            "Epoch 825 Batch 50 Loss 0.1856 Accuracy 0.3360\n",
            "Epoch 825 Loss 0.1873 Accuracy 0.3350\n",
            "Epoch 826 Batch 0 Loss 0.1652 Accuracy 0.3428\n",
            "Epoch 826 Batch 50 Loss 0.1855 Accuracy 0.3356\n",
            "Epoch 826 Loss 0.1876 Accuracy 0.3351\n",
            "Epoch 827 Batch 0 Loss 0.1681 Accuracy 0.3406\n",
            "Epoch 827 Batch 50 Loss 0.1847 Accuracy 0.3361\n",
            "Epoch 827 Loss 0.1871 Accuracy 0.3354\n",
            "Epoch 828 Batch 0 Loss 0.1687 Accuracy 0.3410\n",
            "Epoch 828 Batch 50 Loss 0.1847 Accuracy 0.3359\n",
            "Epoch 828 Loss 0.1874 Accuracy 0.3350\n",
            "Epoch 829 Batch 0 Loss 0.1695 Accuracy 0.3382\n",
            "Epoch 829 Batch 50 Loss 0.1846 Accuracy 0.3361\n",
            "Epoch 829 Loss 0.1867 Accuracy 0.3353\n",
            "Epoch 830 Batch 0 Loss 0.1809 Accuracy 0.3364\n",
            "Epoch 830 Batch 50 Loss 0.1859 Accuracy 0.3356\n",
            "Epoch 830 Loss 0.1873 Accuracy 0.3351\n",
            "Epoch 831 Batch 0 Loss 0.1737 Accuracy 0.3394\n",
            "Epoch 831 Batch 50 Loss 0.1855 Accuracy 0.3355\n",
            "Epoch 831 Loss 0.1876 Accuracy 0.3350\n",
            "Epoch 832 Batch 0 Loss 0.1800 Accuracy 0.3337\n",
            "Epoch 832 Batch 50 Loss 0.1848 Accuracy 0.3358\n",
            "Epoch 832 Loss 0.1871 Accuracy 0.3351\n",
            "Epoch 833 Batch 0 Loss 0.1695 Accuracy 0.3457\n",
            "Epoch 833 Batch 50 Loss 0.1851 Accuracy 0.3362\n",
            "Epoch 833 Loss 0.1872 Accuracy 0.3354\n",
            "Epoch 834 Batch 0 Loss 0.1713 Accuracy 0.3442\n",
            "Epoch 834 Batch 50 Loss 0.1842 Accuracy 0.3362\n",
            "Epoch 834 Loss 0.1863 Accuracy 0.3356\n",
            "Epoch 835 Batch 0 Loss 0.1640 Accuracy 0.3431\n",
            "Epoch 835 Batch 50 Loss 0.1836 Accuracy 0.3363\n",
            "Epoch 835 Loss 0.1862 Accuracy 0.3354\n",
            "Epoch 836 Batch 0 Loss 0.1701 Accuracy 0.3379\n",
            "Epoch 836 Batch 50 Loss 0.1842 Accuracy 0.3360\n",
            "Epoch 836 Loss 0.1869 Accuracy 0.3351\n",
            "Epoch 837 Batch 0 Loss 0.1870 Accuracy 0.3331\n",
            "Epoch 837 Batch 50 Loss 0.1843 Accuracy 0.3360\n",
            "Epoch 837 Loss 0.1865 Accuracy 0.3354\n",
            "Epoch 838 Batch 0 Loss 0.1770 Accuracy 0.3383\n",
            "Epoch 838 Batch 50 Loss 0.1849 Accuracy 0.3356\n",
            "Epoch 838 Loss 0.1868 Accuracy 0.3351\n",
            "Epoch 839 Batch 0 Loss 0.1649 Accuracy 0.3431\n",
            "Epoch 839 Batch 50 Loss 0.1854 Accuracy 0.3360\n",
            "Epoch 839 Loss 0.1868 Accuracy 0.3354\n",
            "Epoch 840 Batch 0 Loss 0.1552 Accuracy 0.3508\n",
            "Epoch 840 Batch 50 Loss 0.1841 Accuracy 0.3360\n",
            "Epoch 840 Loss 0.1868 Accuracy 0.3355\n",
            "Epoch 841 Batch 0 Loss 0.1840 Accuracy 0.3344\n",
            "Epoch 841 Batch 50 Loss 0.1843 Accuracy 0.3361\n",
            "Epoch 841 Loss 0.1865 Accuracy 0.3354\n",
            "Epoch 842 Batch 0 Loss 0.1615 Accuracy 0.3398\n",
            "Epoch 842 Batch 50 Loss 0.1848 Accuracy 0.3362\n",
            "Epoch 842 Loss 0.1870 Accuracy 0.3354\n",
            "Epoch 843 Batch 0 Loss 0.1762 Accuracy 0.3394\n",
            "Epoch 843 Batch 50 Loss 0.1840 Accuracy 0.3361\n",
            "Epoch 843 Loss 0.1862 Accuracy 0.3356\n",
            "Epoch 844 Batch 0 Loss 0.1696 Accuracy 0.3486\n",
            "Epoch 844 Batch 50 Loss 0.1840 Accuracy 0.3362\n",
            "Epoch 844 Loss 0.1867 Accuracy 0.3354\n",
            "Epoch 845 Batch 0 Loss 0.1836 Accuracy 0.3326\n",
            "Epoch 845 Batch 50 Loss 0.1850 Accuracy 0.3360\n",
            "Epoch 845 Loss 0.1866 Accuracy 0.3355\n",
            "Epoch 846 Batch 0 Loss 0.1583 Accuracy 0.3392\n",
            "Epoch 846 Batch 50 Loss 0.1846 Accuracy 0.3362\n",
            "Epoch 846 Loss 0.1866 Accuracy 0.3353\n",
            "Epoch 847 Batch 0 Loss 0.1771 Accuracy 0.3367\n",
            "Epoch 847 Batch 50 Loss 0.1841 Accuracy 0.3360\n",
            "Epoch 847 Loss 0.1868 Accuracy 0.3354\n",
            "Epoch 848 Batch 0 Loss 0.1698 Accuracy 0.3394\n",
            "Epoch 848 Batch 50 Loss 0.1851 Accuracy 0.3360\n",
            "Epoch 848 Loss 0.1868 Accuracy 0.3354\n",
            "Epoch 849 Batch 0 Loss 0.1556 Accuracy 0.3428\n",
            "Epoch 849 Batch 50 Loss 0.1850 Accuracy 0.3354\n",
            "Epoch 849 Loss 0.1868 Accuracy 0.3348\n",
            "Epoch 850 Batch 0 Loss 0.1691 Accuracy 0.3457\n",
            "Epoch 850 Batch 50 Loss 0.1849 Accuracy 0.3360\n",
            "Epoch 850 Loss 0.1870 Accuracy 0.3353\n",
            "Epoch 851 Batch 0 Loss 0.1766 Accuracy 0.3374\n",
            "Epoch 851 Batch 50 Loss 0.1840 Accuracy 0.3360\n",
            "Epoch 851 Loss 0.1862 Accuracy 0.3353\n",
            "Epoch 852 Batch 0 Loss 0.1646 Accuracy 0.3425\n",
            "Epoch 852 Batch 50 Loss 0.1849 Accuracy 0.3357\n",
            "Epoch 852 Loss 0.1868 Accuracy 0.3353\n",
            "Epoch 853 Batch 0 Loss 0.1758 Accuracy 0.3389\n",
            "Epoch 853 Batch 50 Loss 0.1845 Accuracy 0.3359\n",
            "Epoch 853 Loss 0.1861 Accuracy 0.3355\n",
            "Epoch 854 Batch 0 Loss 0.1815 Accuracy 0.3368\n",
            "Epoch 854 Batch 50 Loss 0.1854 Accuracy 0.3358\n",
            "Epoch 854 Loss 0.1875 Accuracy 0.3351\n",
            "Epoch 855 Batch 0 Loss 0.1580 Accuracy 0.3472\n",
            "Epoch 855 Batch 50 Loss 0.1852 Accuracy 0.3359\n",
            "Epoch 855 Loss 0.1869 Accuracy 0.3351\n",
            "Epoch 856 Batch 0 Loss 0.1583 Accuracy 0.3388\n",
            "Epoch 856 Batch 50 Loss 0.1846 Accuracy 0.3358\n",
            "Epoch 856 Loss 0.1870 Accuracy 0.3352\n",
            "Epoch 857 Batch 0 Loss 0.1714 Accuracy 0.3428\n",
            "Epoch 857 Batch 50 Loss 0.1838 Accuracy 0.3367\n",
            "Epoch 857 Loss 0.1860 Accuracy 0.3358\n",
            "Epoch 858 Batch 0 Loss 0.1797 Accuracy 0.3332\n",
            "Epoch 858 Batch 50 Loss 0.1848 Accuracy 0.3359\n",
            "Epoch 858 Loss 0.1866 Accuracy 0.3353\n",
            "Epoch 859 Batch 0 Loss 0.1714 Accuracy 0.3358\n",
            "Epoch 859 Batch 50 Loss 0.1841 Accuracy 0.3359\n",
            "Epoch 859 Loss 0.1862 Accuracy 0.3354\n",
            "Epoch 860 Batch 0 Loss 0.1638 Accuracy 0.3430\n",
            "Epoch 860 Batch 50 Loss 0.1850 Accuracy 0.3360\n",
            "Epoch 860 Loss 0.1869 Accuracy 0.3353\n",
            "Epoch 861 Batch 0 Loss 0.1738 Accuracy 0.3332\n",
            "Epoch 861 Batch 50 Loss 0.1841 Accuracy 0.3358\n",
            "Epoch 861 Loss 0.1864 Accuracy 0.3355\n",
            "Epoch 862 Batch 0 Loss 0.1704 Accuracy 0.3377\n",
            "Epoch 862 Batch 50 Loss 0.1840 Accuracy 0.3359\n",
            "Epoch 862 Loss 0.1858 Accuracy 0.3353\n",
            "Epoch 863 Batch 0 Loss 0.1749 Accuracy 0.3389\n",
            "Epoch 863 Batch 50 Loss 0.1835 Accuracy 0.3358\n",
            "Epoch 863 Loss 0.1860 Accuracy 0.3353\n",
            "Epoch 864 Batch 0 Loss 0.1725 Accuracy 0.3392\n",
            "Epoch 864 Batch 50 Loss 0.1840 Accuracy 0.3359\n",
            "Epoch 864 Loss 0.1867 Accuracy 0.3352\n",
            "Epoch 865 Batch 0 Loss 0.1781 Accuracy 0.3406\n",
            "Epoch 865 Batch 50 Loss 0.1847 Accuracy 0.3358\n",
            "Epoch 865 Loss 0.1863 Accuracy 0.3352\n",
            "Epoch 866 Batch 0 Loss 0.1795 Accuracy 0.3401\n",
            "Epoch 866 Batch 50 Loss 0.1837 Accuracy 0.3362\n",
            "Epoch 866 Loss 0.1862 Accuracy 0.3355\n",
            "Epoch 867 Batch 0 Loss 0.1723 Accuracy 0.3368\n",
            "Epoch 867 Batch 50 Loss 0.1845 Accuracy 0.3358\n",
            "Epoch 867 Loss 0.1866 Accuracy 0.3354\n",
            "Epoch 868 Batch 0 Loss 0.1832 Accuracy 0.3382\n",
            "Epoch 868 Batch 50 Loss 0.1847 Accuracy 0.3359\n",
            "Epoch 868 Loss 0.1866 Accuracy 0.3352\n",
            "Epoch 869 Batch 0 Loss 0.1683 Accuracy 0.3403\n",
            "Epoch 869 Batch 50 Loss 0.1844 Accuracy 0.3361\n",
            "Epoch 869 Loss 0.1865 Accuracy 0.3354\n",
            "Epoch 870 Batch 0 Loss 0.1719 Accuracy 0.3456\n",
            "Epoch 870 Batch 50 Loss 0.1846 Accuracy 0.3359\n",
            "Epoch 870 Loss 0.1863 Accuracy 0.3353\n",
            "Epoch 871 Batch 0 Loss 0.1541 Accuracy 0.3468\n",
            "Epoch 871 Batch 50 Loss 0.1839 Accuracy 0.3360\n",
            "Epoch 871 Loss 0.1864 Accuracy 0.3354\n",
            "Epoch 872 Batch 0 Loss 0.1647 Accuracy 0.3431\n",
            "Epoch 872 Batch 50 Loss 0.1840 Accuracy 0.3357\n",
            "Epoch 872 Loss 0.1866 Accuracy 0.3350\n",
            "Epoch 873 Batch 0 Loss 0.1726 Accuracy 0.3415\n",
            "Epoch 873 Batch 50 Loss 0.1840 Accuracy 0.3360\n",
            "Epoch 873 Loss 0.1861 Accuracy 0.3354\n",
            "Epoch 874 Batch 0 Loss 0.1727 Accuracy 0.3421\n",
            "Epoch 874 Batch 50 Loss 0.1849 Accuracy 0.3356\n",
            "Epoch 874 Loss 0.1864 Accuracy 0.3353\n",
            "Epoch 875 Batch 0 Loss 0.1755 Accuracy 0.3367\n",
            "Epoch 875 Batch 50 Loss 0.1840 Accuracy 0.3363\n",
            "Epoch 875 Loss 0.1862 Accuracy 0.3355\n",
            "Epoch 876 Batch 0 Loss 0.1682 Accuracy 0.3371\n",
            "Epoch 876 Batch 50 Loss 0.1852 Accuracy 0.3358\n",
            "Epoch 876 Loss 0.1870 Accuracy 0.3353\n",
            "Epoch 877 Batch 0 Loss 0.1775 Accuracy 0.3388\n",
            "Epoch 877 Batch 50 Loss 0.1848 Accuracy 0.3357\n",
            "Epoch 877 Loss 0.1866 Accuracy 0.3353\n",
            "Epoch 878 Batch 0 Loss 0.1754 Accuracy 0.3438\n",
            "Epoch 878 Batch 50 Loss 0.1841 Accuracy 0.3359\n",
            "Epoch 878 Loss 0.1862 Accuracy 0.3352\n",
            "Epoch 879 Batch 0 Loss 0.1653 Accuracy 0.3412\n",
            "Epoch 879 Batch 50 Loss 0.1839 Accuracy 0.3362\n",
            "Epoch 879 Loss 0.1860 Accuracy 0.3356\n",
            "Epoch 880 Batch 0 Loss 0.1718 Accuracy 0.3394\n",
            "Epoch 880 Batch 50 Loss 0.1846 Accuracy 0.3359\n",
            "Epoch 880 Loss 0.1865 Accuracy 0.3353\n",
            "Epoch 881 Batch 0 Loss 0.1818 Accuracy 0.3385\n",
            "Epoch 881 Batch 50 Loss 0.1847 Accuracy 0.3360\n",
            "Epoch 881 Loss 0.1860 Accuracy 0.3355\n",
            "Epoch 882 Batch 0 Loss 0.1531 Accuracy 0.3415\n",
            "Epoch 882 Batch 50 Loss 0.1834 Accuracy 0.3363\n",
            "Epoch 882 Loss 0.1861 Accuracy 0.3355\n",
            "Epoch 883 Batch 0 Loss 0.1769 Accuracy 0.3362\n",
            "Epoch 883 Batch 50 Loss 0.1834 Accuracy 0.3360\n",
            "Epoch 883 Loss 0.1857 Accuracy 0.3354\n",
            "Epoch 884 Batch 0 Loss 0.1788 Accuracy 0.3392\n",
            "Epoch 884 Batch 50 Loss 0.1838 Accuracy 0.3363\n",
            "Epoch 884 Loss 0.1858 Accuracy 0.3353\n",
            "Epoch 885 Batch 0 Loss 0.1743 Accuracy 0.3427\n",
            "Epoch 885 Batch 50 Loss 0.1843 Accuracy 0.3362\n",
            "Epoch 885 Loss 0.1863 Accuracy 0.3355\n",
            "Epoch 886 Batch 0 Loss 0.1754 Accuracy 0.3397\n",
            "Epoch 886 Batch 50 Loss 0.1841 Accuracy 0.3356\n",
            "Epoch 886 Loss 0.1861 Accuracy 0.3349\n",
            "Epoch 887 Batch 0 Loss 0.1744 Accuracy 0.3415\n",
            "Epoch 887 Batch 50 Loss 0.1836 Accuracy 0.3358\n",
            "Epoch 887 Loss 0.1855 Accuracy 0.3354\n",
            "Epoch 888 Batch 0 Loss 0.1810 Accuracy 0.3371\n",
            "Epoch 888 Batch 50 Loss 0.1841 Accuracy 0.3357\n",
            "Epoch 888 Loss 0.1865 Accuracy 0.3348\n",
            "Epoch 889 Batch 0 Loss 0.1741 Accuracy 0.3376\n",
            "Epoch 889 Batch 50 Loss 0.1842 Accuracy 0.3359\n",
            "Epoch 889 Loss 0.1858 Accuracy 0.3354\n",
            "Epoch 890 Batch 0 Loss 0.1772 Accuracy 0.3398\n",
            "Epoch 890 Batch 50 Loss 0.1839 Accuracy 0.3362\n",
            "Epoch 890 Loss 0.1859 Accuracy 0.3357\n",
            "Epoch 891 Batch 0 Loss 0.1612 Accuracy 0.3385\n",
            "Epoch 891 Batch 50 Loss 0.1841 Accuracy 0.3360\n",
            "Epoch 891 Loss 0.1856 Accuracy 0.3354\n",
            "Epoch 892 Batch 0 Loss 0.1637 Accuracy 0.3425\n",
            "Epoch 892 Batch 50 Loss 0.1839 Accuracy 0.3362\n",
            "Epoch 892 Loss 0.1858 Accuracy 0.3355\n",
            "Epoch 893 Batch 0 Loss 0.1732 Accuracy 0.3367\n",
            "Epoch 893 Batch 50 Loss 0.1850 Accuracy 0.3358\n",
            "Epoch 893 Loss 0.1865 Accuracy 0.3352\n",
            "Epoch 894 Batch 0 Loss 0.1581 Accuracy 0.3444\n",
            "Epoch 894 Batch 50 Loss 0.1839 Accuracy 0.3359\n",
            "Epoch 894 Loss 0.1861 Accuracy 0.3352\n",
            "Epoch 895 Batch 0 Loss 0.1732 Accuracy 0.3430\n",
            "Epoch 895 Batch 50 Loss 0.1842 Accuracy 0.3357\n",
            "Epoch 895 Loss 0.1863 Accuracy 0.3352\n",
            "Epoch 896 Batch 0 Loss 0.1643 Accuracy 0.3403\n",
            "Epoch 896 Batch 50 Loss 0.1840 Accuracy 0.3357\n",
            "Epoch 896 Loss 0.1859 Accuracy 0.3350\n",
            "Epoch 897 Batch 0 Loss 0.1626 Accuracy 0.3447\n",
            "Epoch 897 Batch 50 Loss 0.1835 Accuracy 0.3366\n",
            "Epoch 897 Loss 0.1862 Accuracy 0.3356\n",
            "Epoch 898 Batch 0 Loss 0.1792 Accuracy 0.3394\n",
            "Epoch 898 Batch 50 Loss 0.1835 Accuracy 0.3358\n",
            "Epoch 898 Loss 0.1860 Accuracy 0.3352\n",
            "Epoch 899 Batch 0 Loss 0.1678 Accuracy 0.3419\n",
            "Epoch 899 Batch 50 Loss 0.1838 Accuracy 0.3359\n",
            "Epoch 899 Loss 0.1863 Accuracy 0.3351\n",
            "Epoch 900 Batch 0 Loss 0.1714 Accuracy 0.3389\n",
            "Epoch 900 Batch 50 Loss 0.1831 Accuracy 0.3360\n",
            "Epoch 900 Loss 0.1859 Accuracy 0.3351\n",
            "Epoch 901 Batch 0 Loss 0.1613 Accuracy 0.3431\n",
            "Epoch 901 Batch 50 Loss 0.1840 Accuracy 0.3360\n",
            "Epoch 901 Loss 0.1865 Accuracy 0.3353\n",
            "Epoch 902 Batch 0 Loss 0.1584 Accuracy 0.3501\n",
            "Epoch 902 Batch 50 Loss 0.1835 Accuracy 0.3361\n",
            "Epoch 902 Loss 0.1853 Accuracy 0.3356\n",
            "Epoch 903 Batch 0 Loss 0.1702 Accuracy 0.3353\n",
            "Epoch 903 Batch 50 Loss 0.1835 Accuracy 0.3362\n",
            "Epoch 903 Loss 0.1863 Accuracy 0.3353\n",
            "Epoch 904 Batch 0 Loss 0.1630 Accuracy 0.3392\n",
            "Epoch 904 Batch 50 Loss 0.1837 Accuracy 0.3362\n",
            "Epoch 904 Loss 0.1860 Accuracy 0.3355\n",
            "Epoch 905 Batch 0 Loss 0.1733 Accuracy 0.3434\n",
            "Epoch 905 Batch 50 Loss 0.1838 Accuracy 0.3357\n",
            "Epoch 905 Loss 0.1852 Accuracy 0.3355\n",
            "Epoch 906 Batch 0 Loss 0.1736 Accuracy 0.3391\n",
            "Epoch 906 Batch 50 Loss 0.1844 Accuracy 0.3359\n",
            "Epoch 906 Loss 0.1862 Accuracy 0.3352\n",
            "Epoch 907 Batch 0 Loss 0.1704 Accuracy 0.3383\n",
            "Epoch 907 Batch 50 Loss 0.1840 Accuracy 0.3359\n",
            "Epoch 907 Loss 0.1854 Accuracy 0.3354\n",
            "Epoch 908 Batch 0 Loss 0.1681 Accuracy 0.3421\n",
            "Epoch 908 Batch 50 Loss 0.1845 Accuracy 0.3357\n",
            "Epoch 908 Loss 0.1865 Accuracy 0.3352\n",
            "Epoch 909 Batch 0 Loss 0.1778 Accuracy 0.3346\n",
            "Epoch 909 Batch 50 Loss 0.1830 Accuracy 0.3359\n",
            "Epoch 909 Loss 0.1854 Accuracy 0.3352\n",
            "Epoch 910 Batch 0 Loss 0.1697 Accuracy 0.3382\n",
            "Epoch 910 Batch 50 Loss 0.1839 Accuracy 0.3355\n",
            "Epoch 910 Loss 0.1860 Accuracy 0.3351\n",
            "Epoch 911 Batch 0 Loss 0.1817 Accuracy 0.3322\n",
            "Epoch 911 Batch 50 Loss 0.1829 Accuracy 0.3362\n",
            "Epoch 911 Loss 0.1853 Accuracy 0.3356\n",
            "Epoch 912 Batch 0 Loss 0.1586 Accuracy 0.3507\n",
            "Epoch 912 Batch 50 Loss 0.1842 Accuracy 0.3360\n",
            "Epoch 912 Loss 0.1856 Accuracy 0.3355\n",
            "Epoch 913 Batch 0 Loss 0.1697 Accuracy 0.3386\n",
            "Epoch 913 Batch 50 Loss 0.1836 Accuracy 0.3364\n",
            "Epoch 913 Loss 0.1857 Accuracy 0.3358\n",
            "Epoch 914 Batch 0 Loss 0.1675 Accuracy 0.3434\n",
            "Epoch 914 Batch 50 Loss 0.1836 Accuracy 0.3363\n",
            "Epoch 914 Loss 0.1860 Accuracy 0.3353\n",
            "Epoch 915 Batch 0 Loss 0.1736 Accuracy 0.3386\n",
            "Epoch 915 Batch 50 Loss 0.1838 Accuracy 0.3359\n",
            "Epoch 915 Loss 0.1853 Accuracy 0.3353\n",
            "Epoch 916 Batch 0 Loss 0.1706 Accuracy 0.3468\n",
            "Epoch 916 Batch 50 Loss 0.1831 Accuracy 0.3361\n",
            "Epoch 916 Loss 0.1852 Accuracy 0.3357\n",
            "Epoch 917 Batch 0 Loss 0.1757 Accuracy 0.3395\n",
            "Epoch 917 Batch 50 Loss 0.1840 Accuracy 0.3361\n",
            "Epoch 917 Loss 0.1857 Accuracy 0.3353\n",
            "Epoch 918 Batch 0 Loss 0.1596 Accuracy 0.3425\n",
            "Epoch 918 Batch 50 Loss 0.1834 Accuracy 0.3359\n",
            "Epoch 918 Loss 0.1861 Accuracy 0.3352\n",
            "Epoch 919 Batch 0 Loss 0.1742 Accuracy 0.3444\n",
            "Epoch 919 Batch 50 Loss 0.1839 Accuracy 0.3360\n",
            "Epoch 919 Loss 0.1860 Accuracy 0.3353\n",
            "Epoch 920 Batch 0 Loss 0.1670 Accuracy 0.3453\n",
            "Epoch 920 Batch 50 Loss 0.1828 Accuracy 0.3364\n",
            "Epoch 920 Loss 0.1852 Accuracy 0.3357\n",
            "Epoch 921 Batch 0 Loss 0.1760 Accuracy 0.3416\n",
            "Epoch 921 Batch 50 Loss 0.1849 Accuracy 0.3358\n",
            "Epoch 921 Loss 0.1867 Accuracy 0.3353\n",
            "Epoch 922 Batch 0 Loss 0.1575 Accuracy 0.3451\n",
            "Epoch 922 Batch 50 Loss 0.1844 Accuracy 0.3359\n",
            "Epoch 922 Loss 0.1858 Accuracy 0.3353\n",
            "Epoch 923 Batch 0 Loss 0.1594 Accuracy 0.3447\n",
            "Epoch 923 Batch 50 Loss 0.1832 Accuracy 0.3362\n",
            "Epoch 923 Loss 0.1854 Accuracy 0.3357\n",
            "Epoch 924 Batch 0 Loss 0.1726 Accuracy 0.3364\n",
            "Epoch 924 Batch 50 Loss 0.1833 Accuracy 0.3357\n",
            "Epoch 924 Loss 0.1852 Accuracy 0.3353\n",
            "Epoch 925 Batch 0 Loss 0.1663 Accuracy 0.3419\n",
            "Epoch 925 Batch 50 Loss 0.1829 Accuracy 0.3366\n",
            "Epoch 925 Loss 0.1857 Accuracy 0.3353\n",
            "Epoch 926 Batch 0 Loss 0.1705 Accuracy 0.3331\n",
            "Epoch 926 Batch 50 Loss 0.1836 Accuracy 0.3358\n",
            "Epoch 926 Loss 0.1856 Accuracy 0.3351\n",
            "Epoch 927 Batch 0 Loss 0.1733 Accuracy 0.3349\n",
            "Epoch 927 Batch 50 Loss 0.1835 Accuracy 0.3357\n",
            "Epoch 927 Loss 0.1856 Accuracy 0.3353\n",
            "Epoch 928 Batch 0 Loss 0.1656 Accuracy 0.3385\n",
            "Epoch 928 Batch 50 Loss 0.1831 Accuracy 0.3364\n",
            "Epoch 928 Loss 0.1853 Accuracy 0.3355\n",
            "Epoch 929 Batch 0 Loss 0.1750 Accuracy 0.3428\n",
            "Epoch 929 Batch 50 Loss 0.1836 Accuracy 0.3361\n",
            "Epoch 929 Loss 0.1858 Accuracy 0.3353\n",
            "Epoch 930 Batch 0 Loss 0.1724 Accuracy 0.3374\n",
            "Epoch 930 Batch 50 Loss 0.1829 Accuracy 0.3363\n",
            "Epoch 930 Loss 0.1855 Accuracy 0.3354\n",
            "Epoch 931 Batch 0 Loss 0.1705 Accuracy 0.3353\n",
            "Epoch 931 Batch 50 Loss 0.1836 Accuracy 0.3360\n",
            "Epoch 931 Loss 0.1855 Accuracy 0.3355\n",
            "Epoch 932 Batch 0 Loss 0.1640 Accuracy 0.3409\n",
            "Epoch 932 Batch 50 Loss 0.1828 Accuracy 0.3361\n",
            "Epoch 932 Loss 0.1849 Accuracy 0.3355\n",
            "Epoch 933 Batch 0 Loss 0.1756 Accuracy 0.3368\n",
            "Epoch 933 Batch 50 Loss 0.1838 Accuracy 0.3361\n",
            "Epoch 933 Loss 0.1854 Accuracy 0.3354\n",
            "Epoch 934 Batch 0 Loss 0.1719 Accuracy 0.3456\n",
            "Epoch 934 Batch 50 Loss 0.1829 Accuracy 0.3360\n",
            "Epoch 934 Loss 0.1853 Accuracy 0.3354\n",
            "Epoch 935 Batch 0 Loss 0.1729 Accuracy 0.3403\n",
            "Epoch 935 Batch 50 Loss 0.1842 Accuracy 0.3357\n",
            "Epoch 935 Loss 0.1860 Accuracy 0.3351\n",
            "Epoch 936 Batch 0 Loss 0.1666 Accuracy 0.3415\n",
            "Epoch 936 Batch 50 Loss 0.1836 Accuracy 0.3359\n",
            "Epoch 936 Loss 0.1852 Accuracy 0.3354\n",
            "Epoch 937 Batch 0 Loss 0.1712 Accuracy 0.3371\n",
            "Epoch 937 Batch 50 Loss 0.1825 Accuracy 0.3362\n",
            "Epoch 937 Loss 0.1856 Accuracy 0.3353\n",
            "Epoch 938 Batch 0 Loss 0.1751 Accuracy 0.3362\n",
            "Epoch 938 Batch 50 Loss 0.1826 Accuracy 0.3360\n",
            "Epoch 938 Loss 0.1853 Accuracy 0.3353\n",
            "Epoch 939 Batch 0 Loss 0.1605 Accuracy 0.3422\n",
            "Epoch 939 Batch 50 Loss 0.1818 Accuracy 0.3366\n",
            "Epoch 939 Loss 0.1845 Accuracy 0.3357\n",
            "Epoch 940 Batch 0 Loss 0.1645 Accuracy 0.3445\n",
            "Epoch 940 Batch 50 Loss 0.1828 Accuracy 0.3361\n",
            "Epoch 940 Loss 0.1849 Accuracy 0.3354\n",
            "Epoch 941 Batch 0 Loss 0.1670 Accuracy 0.3400\n",
            "Epoch 941 Batch 50 Loss 0.1827 Accuracy 0.3365\n",
            "Epoch 941 Loss 0.1852 Accuracy 0.3356\n",
            "Epoch 942 Batch 0 Loss 0.1734 Accuracy 0.3406\n",
            "Epoch 942 Batch 50 Loss 0.1834 Accuracy 0.3356\n",
            "Epoch 942 Loss 0.1854 Accuracy 0.3350\n",
            "Epoch 943 Batch 0 Loss 0.1689 Accuracy 0.3386\n",
            "Epoch 943 Batch 50 Loss 0.1832 Accuracy 0.3361\n",
            "Epoch 943 Loss 0.1851 Accuracy 0.3358\n",
            "Epoch 944 Batch 0 Loss 0.1827 Accuracy 0.3320\n",
            "Epoch 944 Batch 50 Loss 0.1831 Accuracy 0.3362\n",
            "Epoch 944 Loss 0.1855 Accuracy 0.3356\n",
            "Epoch 945 Batch 0 Loss 0.1647 Accuracy 0.3434\n",
            "Epoch 945 Batch 50 Loss 0.1822 Accuracy 0.3364\n",
            "Epoch 945 Loss 0.1845 Accuracy 0.3354\n",
            "Epoch 946 Batch 0 Loss 0.1652 Accuracy 0.3395\n",
            "Epoch 946 Batch 50 Loss 0.1831 Accuracy 0.3359\n",
            "Epoch 946 Loss 0.1850 Accuracy 0.3354\n",
            "Epoch 947 Batch 0 Loss 0.1816 Accuracy 0.3353\n",
            "Epoch 947 Batch 50 Loss 0.1823 Accuracy 0.3363\n",
            "Epoch 947 Loss 0.1848 Accuracy 0.3356\n",
            "Epoch 948 Batch 0 Loss 0.1601 Accuracy 0.3448\n",
            "Epoch 948 Batch 50 Loss 0.1827 Accuracy 0.3362\n",
            "Epoch 948 Loss 0.1847 Accuracy 0.3356\n",
            "Epoch 949 Batch 0 Loss 0.1615 Accuracy 0.3415\n",
            "Epoch 949 Batch 50 Loss 0.1830 Accuracy 0.3363\n",
            "Epoch 949 Loss 0.1851 Accuracy 0.3354\n",
            "Epoch 950 Batch 0 Loss 0.1586 Accuracy 0.3419\n",
            "Epoch 950 Batch 50 Loss 0.1838 Accuracy 0.3357\n",
            "Epoch 950 Loss 0.1848 Accuracy 0.3353\n",
            "Epoch 951 Batch 0 Loss 0.1520 Accuracy 0.3481\n",
            "Epoch 951 Batch 50 Loss 0.1829 Accuracy 0.3364\n",
            "Epoch 951 Loss 0.1852 Accuracy 0.3356\n",
            "Epoch 952 Batch 0 Loss 0.1714 Accuracy 0.3344\n",
            "Epoch 952 Batch 50 Loss 0.1828 Accuracy 0.3359\n",
            "Epoch 952 Loss 0.1854 Accuracy 0.3354\n",
            "Epoch 953 Batch 0 Loss 0.1645 Accuracy 0.3456\n",
            "Epoch 953 Batch 50 Loss 0.1825 Accuracy 0.3364\n",
            "Epoch 953 Loss 0.1849 Accuracy 0.3354\n",
            "Epoch 954 Batch 0 Loss 0.1779 Accuracy 0.3359\n",
            "Epoch 954 Batch 50 Loss 0.1832 Accuracy 0.3360\n",
            "Epoch 954 Loss 0.1852 Accuracy 0.3355\n",
            "Epoch 955 Batch 0 Loss 0.1702 Accuracy 0.3457\n",
            "Epoch 955 Batch 50 Loss 0.1821 Accuracy 0.3365\n",
            "Epoch 955 Loss 0.1850 Accuracy 0.3356\n",
            "Epoch 956 Batch 0 Loss 0.1635 Accuracy 0.3406\n",
            "Epoch 956 Batch 50 Loss 0.1830 Accuracy 0.3360\n",
            "Epoch 956 Loss 0.1855 Accuracy 0.3353\n",
            "Epoch 957 Batch 0 Loss 0.1612 Accuracy 0.3415\n",
            "Epoch 957 Batch 50 Loss 0.1831 Accuracy 0.3360\n",
            "Epoch 957 Loss 0.1849 Accuracy 0.3355\n",
            "Epoch 958 Batch 0 Loss 0.1595 Accuracy 0.3468\n",
            "Epoch 958 Batch 50 Loss 0.1827 Accuracy 0.3364\n",
            "Epoch 958 Loss 0.1852 Accuracy 0.3356\n",
            "Epoch 959 Batch 0 Loss 0.1632 Accuracy 0.3421\n",
            "Epoch 959 Batch 50 Loss 0.1828 Accuracy 0.3361\n",
            "Epoch 959 Loss 0.1849 Accuracy 0.3355\n",
            "Epoch 960 Batch 0 Loss 0.1594 Accuracy 0.3424\n",
            "Epoch 960 Batch 50 Loss 0.1838 Accuracy 0.3362\n",
            "Epoch 960 Loss 0.1853 Accuracy 0.3355\n",
            "Epoch 961 Batch 0 Loss 0.1620 Accuracy 0.3477\n",
            "Epoch 961 Batch 50 Loss 0.1834 Accuracy 0.3359\n",
            "Epoch 961 Loss 0.1853 Accuracy 0.3354\n",
            "Epoch 962 Batch 0 Loss 0.1666 Accuracy 0.3400\n",
            "Epoch 962 Batch 50 Loss 0.1829 Accuracy 0.3363\n",
            "Epoch 962 Loss 0.1850 Accuracy 0.3355\n",
            "Epoch 963 Batch 0 Loss 0.1579 Accuracy 0.3394\n",
            "Epoch 963 Batch 50 Loss 0.1824 Accuracy 0.3362\n",
            "Epoch 963 Loss 0.1838 Accuracy 0.3357\n",
            "Epoch 964 Batch 0 Loss 0.1721 Accuracy 0.3385\n",
            "Epoch 964 Batch 50 Loss 0.1841 Accuracy 0.3357\n",
            "Epoch 964 Loss 0.1859 Accuracy 0.3352\n",
            "Epoch 965 Batch 0 Loss 0.1739 Accuracy 0.3465\n",
            "Epoch 965 Batch 50 Loss 0.1834 Accuracy 0.3358\n",
            "Epoch 965 Loss 0.1857 Accuracy 0.3352\n",
            "Epoch 966 Batch 0 Loss 0.1809 Accuracy 0.3376\n",
            "Epoch 966 Batch 50 Loss 0.1825 Accuracy 0.3361\n",
            "Epoch 966 Loss 0.1852 Accuracy 0.3354\n",
            "Epoch 967 Batch 0 Loss 0.1737 Accuracy 0.3365\n",
            "Epoch 967 Batch 50 Loss 0.1821 Accuracy 0.3358\n",
            "Epoch 967 Loss 0.1849 Accuracy 0.3350\n",
            "Epoch 968 Batch 0 Loss 0.1812 Accuracy 0.3373\n",
            "Epoch 968 Batch 50 Loss 0.1834 Accuracy 0.3357\n",
            "Epoch 968 Loss 0.1852 Accuracy 0.3351\n",
            "Epoch 969 Batch 0 Loss 0.1657 Accuracy 0.3459\n",
            "Epoch 969 Batch 50 Loss 0.1828 Accuracy 0.3359\n",
            "Epoch 969 Loss 0.1848 Accuracy 0.3353\n",
            "Epoch 970 Batch 0 Loss 0.1692 Accuracy 0.3433\n",
            "Epoch 970 Batch 50 Loss 0.1826 Accuracy 0.3361\n",
            "Epoch 970 Loss 0.1845 Accuracy 0.3355\n",
            "Epoch 971 Batch 0 Loss 0.1523 Accuracy 0.3453\n",
            "Epoch 971 Batch 50 Loss 0.1832 Accuracy 0.3356\n",
            "Epoch 971 Loss 0.1849 Accuracy 0.3353\n",
            "Epoch 972 Batch 0 Loss 0.1703 Accuracy 0.3406\n",
            "Epoch 972 Batch 50 Loss 0.1825 Accuracy 0.3362\n",
            "Epoch 972 Loss 0.1852 Accuracy 0.3355\n",
            "Epoch 973 Batch 0 Loss 0.1793 Accuracy 0.3283\n",
            "Epoch 973 Batch 50 Loss 0.1827 Accuracy 0.3358\n",
            "Epoch 973 Loss 0.1847 Accuracy 0.3354\n",
            "Epoch 974 Batch 0 Loss 0.1582 Accuracy 0.3505\n",
            "Epoch 974 Batch 50 Loss 0.1831 Accuracy 0.3360\n",
            "Epoch 974 Loss 0.1853 Accuracy 0.3354\n",
            "Epoch 975 Batch 0 Loss 0.1687 Accuracy 0.3394\n",
            "Epoch 975 Batch 50 Loss 0.1822 Accuracy 0.3365\n",
            "Epoch 975 Loss 0.1843 Accuracy 0.3358\n",
            "Epoch 976 Batch 0 Loss 0.1735 Accuracy 0.3431\n",
            "Epoch 976 Batch 50 Loss 0.1830 Accuracy 0.3361\n",
            "Epoch 976 Loss 0.1853 Accuracy 0.3351\n",
            "Epoch 977 Batch 0 Loss 0.1685 Accuracy 0.3368\n",
            "Epoch 977 Batch 50 Loss 0.1820 Accuracy 0.3360\n",
            "Epoch 977 Loss 0.1848 Accuracy 0.3351\n",
            "Epoch 978 Batch 0 Loss 0.1756 Accuracy 0.3365\n",
            "Epoch 978 Batch 50 Loss 0.1826 Accuracy 0.3362\n",
            "Epoch 978 Loss 0.1848 Accuracy 0.3355\n",
            "Epoch 979 Batch 0 Loss 0.1569 Accuracy 0.3483\n",
            "Epoch 979 Batch 50 Loss 0.1830 Accuracy 0.3363\n",
            "Epoch 979 Loss 0.1847 Accuracy 0.3354\n",
            "Epoch 980 Batch 0 Loss 0.1824 Accuracy 0.3401\n",
            "Epoch 980 Batch 50 Loss 0.1831 Accuracy 0.3361\n",
            "Epoch 980 Loss 0.1851 Accuracy 0.3354\n",
            "Epoch 981 Batch 0 Loss 0.1712 Accuracy 0.3397\n",
            "Epoch 981 Batch 50 Loss 0.1824 Accuracy 0.3363\n",
            "Epoch 981 Loss 0.1845 Accuracy 0.3355\n",
            "Epoch 982 Batch 0 Loss 0.1725 Accuracy 0.3406\n",
            "Epoch 982 Batch 50 Loss 0.1826 Accuracy 0.3362\n",
            "Epoch 982 Loss 0.1851 Accuracy 0.3353\n",
            "Epoch 983 Batch 0 Loss 0.1619 Accuracy 0.3492\n",
            "Epoch 983 Batch 50 Loss 0.1836 Accuracy 0.3358\n",
            "Epoch 983 Loss 0.1855 Accuracy 0.3352\n",
            "Epoch 984 Batch 0 Loss 0.1665 Accuracy 0.3481\n",
            "Epoch 984 Batch 50 Loss 0.1823 Accuracy 0.3366\n",
            "Epoch 984 Loss 0.1846 Accuracy 0.3355\n",
            "Epoch 985 Batch 0 Loss 0.1680 Accuracy 0.3361\n",
            "Epoch 985 Batch 50 Loss 0.1825 Accuracy 0.3361\n",
            "Epoch 985 Loss 0.1843 Accuracy 0.3356\n",
            "Epoch 986 Batch 0 Loss 0.1679 Accuracy 0.3413\n",
            "Epoch 986 Batch 50 Loss 0.1833 Accuracy 0.3359\n",
            "Epoch 986 Loss 0.1855 Accuracy 0.3353\n",
            "Epoch 987 Batch 0 Loss 0.1849 Accuracy 0.3431\n",
            "Epoch 987 Batch 50 Loss 0.1828 Accuracy 0.3359\n",
            "Epoch 987 Loss 0.1844 Accuracy 0.3353\n",
            "Epoch 988 Batch 0 Loss 0.1653 Accuracy 0.3475\n",
            "Epoch 988 Batch 50 Loss 0.1828 Accuracy 0.3361\n",
            "Epoch 988 Loss 0.1842 Accuracy 0.3357\n",
            "Epoch 989 Batch 0 Loss 0.1732 Accuracy 0.3385\n",
            "Epoch 989 Batch 50 Loss 0.1817 Accuracy 0.3362\n",
            "Epoch 989 Loss 0.1845 Accuracy 0.3355\n",
            "Epoch 990 Batch 0 Loss 0.1795 Accuracy 0.3350\n",
            "Epoch 990 Batch 50 Loss 0.1827 Accuracy 0.3361\n",
            "Epoch 990 Loss 0.1848 Accuracy 0.3354\n",
            "Epoch 991 Batch 0 Loss 0.1803 Accuracy 0.3392\n",
            "Epoch 991 Batch 50 Loss 0.1827 Accuracy 0.3361\n",
            "Epoch 991 Loss 0.1850 Accuracy 0.3354\n",
            "Epoch 992 Batch 0 Loss 0.1755 Accuracy 0.3347\n",
            "Epoch 992 Batch 50 Loss 0.1833 Accuracy 0.3357\n",
            "Epoch 992 Loss 0.1849 Accuracy 0.3353\n",
            "Epoch 993 Batch 0 Loss 0.1638 Accuracy 0.3451\n",
            "Epoch 993 Batch 50 Loss 0.1825 Accuracy 0.3360\n",
            "Epoch 993 Loss 0.1844 Accuracy 0.3355\n",
            "Epoch 994 Batch 0 Loss 0.1733 Accuracy 0.3365\n",
            "Epoch 994 Batch 50 Loss 0.1834 Accuracy 0.3356\n",
            "Epoch 994 Loss 0.1850 Accuracy 0.3354\n",
            "Epoch 995 Batch 0 Loss 0.1571 Accuracy 0.3444\n",
            "Epoch 995 Batch 50 Loss 0.1820 Accuracy 0.3364\n",
            "Epoch 995 Loss 0.1846 Accuracy 0.3355\n",
            "Epoch 996 Batch 0 Loss 0.1832 Accuracy 0.3316\n",
            "Epoch 996 Batch 50 Loss 0.1832 Accuracy 0.3357\n",
            "Epoch 996 Loss 0.1852 Accuracy 0.3351\n",
            "Epoch 997 Batch 0 Loss 0.1690 Accuracy 0.3457\n",
            "Epoch 997 Batch 50 Loss 0.1823 Accuracy 0.3361\n",
            "Epoch 997 Loss 0.1845 Accuracy 0.3353\n",
            "Epoch 998 Batch 0 Loss 0.1686 Accuracy 0.3487\n",
            "Epoch 998 Batch 50 Loss 0.1831 Accuracy 0.3362\n",
            "Epoch 998 Loss 0.1844 Accuracy 0.3355\n",
            "Epoch 999 Batch 0 Loss 0.1729 Accuracy 0.3347\n",
            "Epoch 999 Batch 50 Loss 0.1827 Accuracy 0.3359\n",
            "Epoch 999 Loss 0.1847 Accuracy 0.3355\n",
            "Epoch 1000 Batch 0 Loss 0.1701 Accuracy 0.3428\n",
            "Epoch 1000 Batch 50 Loss 0.1821 Accuracy 0.3365\n",
            "Epoch 1000 Loss 0.1844 Accuracy 0.3356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EShPJcIS0pEe"
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZDenScKnYUQ",
        "outputId": "57b39649-60f4-43c9-ff7e-cd9c5f2e442c"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    start_token = [1]\n",
        "    end_token = [2]\n",
        "  \n",
        "    sentence = preprocess_sentence(inp_sentence)\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    \n",
        "    encoder_input = tf.expand_dims(inputs, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "    decoder_input = [1]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_length_targ):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "    sentence = inp_lang_tokenizer.encode(sentence)\n",
        "  \n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "\n",
        "        # plot the attention weights\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "        fontdict = {'fontsize': 10}\n",
        "\n",
        "        ax.set_xticks(range(len(sentence)+2))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "\n",
        "        ax.set_ylim(len(result)-1.5, -0.5)\n",
        "\n",
        "        ax.set_xticklabels(\n",
        "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
        "            fontdict=fontdict, rotation=90)\n",
        "\n",
        "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
        "                            if i < tokenizer_en.vocab_size], \n",
        "                           fontdict=fontdict)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def translate(sentence, plot=''):\n",
        "    result, attention_weights = evaluate(sentence)\n",
        "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
        "        \n",
        "translate(\"good morning.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: good morning.\n",
            "Predicted translation: ['<start>', 'с', 'добрым', 'утром', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGbH0YwH0qHU",
        "outputId": "4ac31f41-e585-47e9-b5da-60a42f47fd35"
      },
      "source": [
        "translate('how are you?')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: how are you?\n",
            "Predicted translation: ['<start>', 'как', 'живёшь', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuUuuco30wVO"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}