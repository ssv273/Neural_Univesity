{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_26",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1-lpxSyXqQL7KdK3ldhvXMNjYalsY90C8",
      "authorship_tag": "ABX9TyM9GvpNcpFidffdH9/aA7td",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssv273/Neural_Univesity/blob/main/hw_26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4BIn6hKt0i4"
      },
      "source": [
        "import numpy as np # библиотека для работы с массивами данных\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model # из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input # из кераса загружаем необходимые слои для нейросети\n",
        "from tensorflow.keras.optimizers import RMSprop, Adadelta # из кераса загружаем выбранный оптимизатор\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # загружаем метод ограничения последовательности заданной длиной\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # загружаем токенизатор кераса для обработки текста\n",
        "from tensorflow.keras import utils # загружаем утилиты кераса для one hot кодировки\n",
        "from tensorflow.keras.utils import plot_model # удобный график для визуализации архитектуры модели\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import yaml # импортируем модуль для удобной работы с файлами"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdvN_SMTj7jP",
        "outputId": "64ce8025-de57-4252-90d0-0b9792a7dceb"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/Neural_Univesity/hw_17/rus.zip\" -d /content/rus"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Neural_Univesity/hw_17/rus.zip\n",
            "  inflating: /content/rus/rus.txt    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1w9cHqyu-pE"
      },
      "source": [
        "data_path = '/content/rus/rus.txt'\n",
        "num_samples = 30000\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "#     w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(preprocess_sentence(input_text))\n",
        "    target_texts.append(preprocess_sentence(target_text))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJGJA7JfmF0k",
        "outputId": "b322f866-a4b1-4be3-8f61-dac9f50449ab"
      },
      "source": [
        "len(input_texts), len(target_texts)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 30000)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONU-EgbL5TvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a829899-43f9-4856-bd06-0a459b901412"
      },
      "source": [
        "input_texts[10], target_texts[10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<start> Run . <end>', '<start> Беги ! <end>')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jESVipX-5TlN"
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = pad_sequences(tensor, padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoyxCcT1ldaV"
      },
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
        "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3bMtv2lliFX",
        "outputId": "22a0a558-18e4-4fdc-9c08-c2221acf73f0"
      },
      "source": [
        "input_tensor.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9vefEOY5UTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b344b3-3f62-473c-a93e-682405eec354"
      },
      "source": [
        "len(inp_lang_tokenizer.word_index)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3645"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x8BKFUJpQ4V"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmXrFCA6mgHo"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ing9GeOVnSd0"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "    \n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ-vvVOhnehP"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "    \n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "    \n",
        "    \n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEUxOa51nkIC"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                 look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFls-myVno2H"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W95aOtWonsVT"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
        "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y7-rryinyLB"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynk5YFRLn3pj"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "# learning_rate = 0.0001\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "  \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIJsH58qn7uI"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2ix3sXGn_4Z"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpFuXEC6oFJ2"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx9GUD0RoM1f"
      },
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "  \n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8uWNNSaokNM"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor)\n",
        "BATCH_SIZE = 512\n",
        "steps_per_epoch = len(input_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hvumKBxoSN9",
        "outputId": "be6daf94-89fb-46bb-a5c7-2c234bcc102b"
      },
      "source": [
        "%%time\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        train_step(inp, tar)\n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.9570 Accuracy 0.2933\n",
            "Epoch 1 Batch 50 Loss 0.9278 Accuracy 0.2970\n",
            "Epoch 1 Loss 0.9287 Accuracy 0.2967\n",
            "Epoch 2 Batch 0 Loss 0.8661 Accuracy 0.2990\n",
            "Epoch 2 Batch 50 Loss 0.8614 Accuracy 0.3002\n",
            "Epoch 2 Loss 0.8651 Accuracy 0.2996\n",
            "Epoch 3 Batch 0 Loss 0.7754 Accuracy 0.3074\n",
            "Epoch 3 Batch 50 Loss 0.8027 Accuracy 0.3033\n",
            "Epoch 3 Loss 0.8046 Accuracy 0.3028\n",
            "Epoch 4 Batch 0 Loss 0.7016 Accuracy 0.3096\n",
            "Epoch 4 Batch 50 Loss 0.7408 Accuracy 0.3060\n",
            "Epoch 4 Loss 0.7441 Accuracy 0.3060\n",
            "Epoch 5 Batch 0 Loss 0.6734 Accuracy 0.3041\n",
            "Epoch 5 Batch 50 Loss 0.6938 Accuracy 0.3081\n",
            "Epoch 5 Loss 0.6948 Accuracy 0.3078\n",
            "Epoch 6 Batch 0 Loss 0.6032 Accuracy 0.3178\n",
            "Epoch 6 Batch 50 Loss 0.6471 Accuracy 0.3103\n",
            "Epoch 6 Loss 0.6506 Accuracy 0.3097\n",
            "Epoch 7 Batch 0 Loss 0.5938 Accuracy 0.3173\n",
            "Epoch 7 Batch 50 Loss 0.6064 Accuracy 0.3135\n",
            "Epoch 7 Loss 0.6106 Accuracy 0.3129\n",
            "Epoch 8 Batch 0 Loss 0.5533 Accuracy 0.3226\n",
            "Epoch 8 Batch 50 Loss 0.5654 Accuracy 0.3155\n",
            "Epoch 8 Loss 0.5689 Accuracy 0.3148\n",
            "Epoch 9 Batch 0 Loss 0.4903 Accuracy 0.3275\n",
            "Epoch 9 Batch 50 Loss 0.5318 Accuracy 0.3171\n",
            "Epoch 9 Loss 0.5372 Accuracy 0.3164\n",
            "Epoch 10 Batch 0 Loss 0.4935 Accuracy 0.3203\n",
            "Epoch 10 Batch 50 Loss 0.5038 Accuracy 0.3186\n",
            "Epoch 10 Loss 0.5086 Accuracy 0.3180\n",
            "Epoch 11 Batch 0 Loss 0.4420 Accuracy 0.3241\n",
            "Epoch 11 Batch 50 Loss 0.4822 Accuracy 0.3194\n",
            "Epoch 11 Loss 0.4872 Accuracy 0.3188\n",
            "Epoch 12 Batch 0 Loss 0.4374 Accuracy 0.3241\n",
            "Epoch 12 Batch 50 Loss 0.4605 Accuracy 0.3201\n",
            "Epoch 12 Loss 0.4659 Accuracy 0.3197\n",
            "Epoch 13 Batch 0 Loss 0.4051 Accuracy 0.3275\n",
            "Epoch 13 Batch 50 Loss 0.4393 Accuracy 0.3214\n",
            "Epoch 13 Loss 0.4442 Accuracy 0.3210\n",
            "Epoch 14 Batch 0 Loss 0.3826 Accuracy 0.3310\n",
            "Epoch 14 Batch 50 Loss 0.4224 Accuracy 0.3223\n",
            "Epoch 14 Loss 0.4278 Accuracy 0.3216\n",
            "Epoch 15 Batch 0 Loss 0.3585 Accuracy 0.3325\n",
            "Epoch 15 Batch 50 Loss 0.4133 Accuracy 0.3229\n",
            "Epoch 15 Loss 0.4174 Accuracy 0.3224\n",
            "Epoch 16 Batch 0 Loss 0.3581 Accuracy 0.3250\n",
            "Epoch 16 Batch 50 Loss 0.4031 Accuracy 0.3234\n",
            "Epoch 16 Loss 0.4085 Accuracy 0.3228\n",
            "Epoch 17 Batch 0 Loss 0.3505 Accuracy 0.3295\n",
            "Epoch 17 Batch 50 Loss 0.3979 Accuracy 0.3236\n",
            "Epoch 17 Loss 0.4032 Accuracy 0.3229\n",
            "Epoch 18 Batch 0 Loss 0.3474 Accuracy 0.3274\n",
            "Epoch 18 Batch 50 Loss 0.3867 Accuracy 0.3238\n",
            "Epoch 18 Loss 0.3907 Accuracy 0.3235\n",
            "Epoch 19 Batch 0 Loss 0.3510 Accuracy 0.3319\n",
            "Epoch 19 Batch 50 Loss 0.3792 Accuracy 0.3248\n",
            "Epoch 19 Loss 0.3847 Accuracy 0.3240\n",
            "Epoch 20 Batch 0 Loss 0.3563 Accuracy 0.3280\n",
            "Epoch 20 Batch 50 Loss 0.3800 Accuracy 0.3242\n",
            "Epoch 20 Loss 0.3845 Accuracy 0.3236\n",
            "Epoch 21 Batch 0 Loss 0.3240 Accuracy 0.3314\n",
            "Epoch 21 Batch 50 Loss 0.3728 Accuracy 0.3247\n",
            "Epoch 21 Loss 0.3786 Accuracy 0.3239\n",
            "Epoch 22 Batch 0 Loss 0.3076 Accuracy 0.3370\n",
            "Epoch 22 Batch 50 Loss 0.3691 Accuracy 0.3249\n",
            "Epoch 22 Loss 0.3744 Accuracy 0.3244\n",
            "Epoch 23 Batch 0 Loss 0.3220 Accuracy 0.3334\n",
            "Epoch 23 Batch 50 Loss 0.3678 Accuracy 0.3253\n",
            "Epoch 23 Loss 0.3733 Accuracy 0.3242\n",
            "Epoch 24 Batch 0 Loss 0.3086 Accuracy 0.3271\n",
            "Epoch 24 Batch 50 Loss 0.3618 Accuracy 0.3254\n",
            "Epoch 24 Loss 0.3691 Accuracy 0.3246\n",
            "Epoch 25 Batch 0 Loss 0.3265 Accuracy 0.3325\n",
            "Epoch 25 Batch 50 Loss 0.3612 Accuracy 0.3254\n",
            "Epoch 25 Loss 0.3662 Accuracy 0.3248\n",
            "Epoch 26 Batch 0 Loss 0.3308 Accuracy 0.3280\n",
            "Epoch 26 Batch 50 Loss 0.3582 Accuracy 0.3258\n",
            "Epoch 26 Loss 0.3651 Accuracy 0.3249\n",
            "Epoch 27 Batch 0 Loss 0.3198 Accuracy 0.3289\n",
            "Epoch 27 Batch 50 Loss 0.3554 Accuracy 0.3263\n",
            "Epoch 27 Loss 0.3611 Accuracy 0.3255\n",
            "Epoch 28 Batch 0 Loss 0.3032 Accuracy 0.3331\n",
            "Epoch 28 Batch 50 Loss 0.3546 Accuracy 0.3258\n",
            "Epoch 28 Loss 0.3611 Accuracy 0.3251\n",
            "Epoch 29 Batch 0 Loss 0.3118 Accuracy 0.3323\n",
            "Epoch 29 Batch 50 Loss 0.3539 Accuracy 0.3259\n",
            "Epoch 29 Loss 0.3605 Accuracy 0.3253\n",
            "Epoch 30 Batch 0 Loss 0.3175 Accuracy 0.3374\n",
            "Epoch 30 Batch 50 Loss 0.3541 Accuracy 0.3260\n",
            "Epoch 30 Loss 0.3592 Accuracy 0.3253\n",
            "Epoch 31 Batch 0 Loss 0.3106 Accuracy 0.3322\n",
            "Epoch 31 Batch 50 Loss 0.3510 Accuracy 0.3262\n",
            "Epoch 31 Loss 0.3559 Accuracy 0.3257\n",
            "Epoch 32 Batch 0 Loss 0.3063 Accuracy 0.3275\n",
            "Epoch 32 Batch 50 Loss 0.3531 Accuracy 0.3264\n",
            "Epoch 32 Loss 0.3590 Accuracy 0.3257\n",
            "Epoch 33 Batch 0 Loss 0.2999 Accuracy 0.3305\n",
            "Epoch 33 Batch 50 Loss 0.3468 Accuracy 0.3269\n",
            "Epoch 33 Loss 0.3529 Accuracy 0.3260\n",
            "Epoch 34 Batch 0 Loss 0.3046 Accuracy 0.3284\n",
            "Epoch 34 Batch 50 Loss 0.3492 Accuracy 0.3267\n",
            "Epoch 34 Loss 0.3551 Accuracy 0.3262\n",
            "Epoch 35 Batch 0 Loss 0.3184 Accuracy 0.3316\n",
            "Epoch 35 Batch 50 Loss 0.3432 Accuracy 0.3271\n",
            "Epoch 35 Loss 0.3511 Accuracy 0.3264\n",
            "Epoch 36 Batch 0 Loss 0.2943 Accuracy 0.3410\n",
            "Epoch 36 Batch 50 Loss 0.3451 Accuracy 0.3270\n",
            "Epoch 36 Loss 0.3526 Accuracy 0.3261\n",
            "Epoch 37 Batch 0 Loss 0.3043 Accuracy 0.3335\n",
            "Epoch 37 Batch 50 Loss 0.3453 Accuracy 0.3272\n",
            "Epoch 37 Loss 0.3511 Accuracy 0.3266\n",
            "Epoch 38 Batch 0 Loss 0.3023 Accuracy 0.3277\n",
            "Epoch 38 Batch 50 Loss 0.3392 Accuracy 0.3274\n",
            "Epoch 38 Loss 0.3449 Accuracy 0.3271\n",
            "Epoch 39 Batch 0 Loss 0.2889 Accuracy 0.3391\n",
            "Epoch 39 Batch 50 Loss 0.3381 Accuracy 0.3280\n",
            "Epoch 39 Loss 0.3433 Accuracy 0.3273\n",
            "Epoch 40 Batch 0 Loss 0.2925 Accuracy 0.3328\n",
            "Epoch 40 Batch 50 Loss 0.3325 Accuracy 0.3282\n",
            "Epoch 40 Loss 0.3387 Accuracy 0.3274\n",
            "Epoch 41 Batch 0 Loss 0.2795 Accuracy 0.3374\n",
            "Epoch 41 Batch 50 Loss 0.3303 Accuracy 0.3282\n",
            "Epoch 41 Loss 0.3373 Accuracy 0.3274\n",
            "Epoch 42 Batch 0 Loss 0.2903 Accuracy 0.3325\n",
            "Epoch 42 Batch 50 Loss 0.3274 Accuracy 0.3281\n",
            "Epoch 42 Loss 0.3323 Accuracy 0.3279\n",
            "Epoch 43 Batch 0 Loss 0.2850 Accuracy 0.3356\n",
            "Epoch 43 Batch 50 Loss 0.3232 Accuracy 0.3289\n",
            "Epoch 43 Loss 0.3296 Accuracy 0.3281\n",
            "Epoch 44 Batch 0 Loss 0.2778 Accuracy 0.3359\n",
            "Epoch 44 Batch 50 Loss 0.3201 Accuracy 0.3294\n",
            "Epoch 44 Loss 0.3268 Accuracy 0.3283\n",
            "Epoch 45 Batch 0 Loss 0.2684 Accuracy 0.3353\n",
            "Epoch 45 Batch 50 Loss 0.3188 Accuracy 0.3290\n",
            "Epoch 45 Loss 0.3240 Accuracy 0.3287\n",
            "Epoch 46 Batch 0 Loss 0.2690 Accuracy 0.3326\n",
            "Epoch 46 Batch 50 Loss 0.3139 Accuracy 0.3292\n",
            "Epoch 46 Loss 0.3202 Accuracy 0.3286\n",
            "Epoch 47 Batch 0 Loss 0.2548 Accuracy 0.3406\n",
            "Epoch 47 Batch 50 Loss 0.3137 Accuracy 0.3294\n",
            "Epoch 47 Loss 0.3198 Accuracy 0.3287\n",
            "Epoch 48 Batch 0 Loss 0.2655 Accuracy 0.3370\n",
            "Epoch 48 Batch 50 Loss 0.3130 Accuracy 0.3295\n",
            "Epoch 48 Loss 0.3179 Accuracy 0.3289\n",
            "Epoch 49 Batch 0 Loss 0.2642 Accuracy 0.3334\n",
            "Epoch 49 Batch 50 Loss 0.3078 Accuracy 0.3302\n",
            "Epoch 49 Loss 0.3138 Accuracy 0.3293\n",
            "Epoch 50 Batch 0 Loss 0.2444 Accuracy 0.3431\n",
            "Epoch 50 Batch 50 Loss 0.3048 Accuracy 0.3301\n",
            "Epoch 50 Loss 0.3108 Accuracy 0.3296\n",
            "Epoch 51 Batch 0 Loss 0.2615 Accuracy 0.3335\n",
            "Epoch 51 Batch 50 Loss 0.3044 Accuracy 0.3303\n",
            "Epoch 51 Loss 0.3095 Accuracy 0.3297\n",
            "Epoch 52 Batch 0 Loss 0.2715 Accuracy 0.3364\n",
            "Epoch 52 Batch 50 Loss 0.3011 Accuracy 0.3303\n",
            "Epoch 52 Loss 0.3079 Accuracy 0.3298\n",
            "Epoch 53 Batch 0 Loss 0.2504 Accuracy 0.3380\n",
            "Epoch 53 Batch 50 Loss 0.2995 Accuracy 0.3306\n",
            "Epoch 53 Loss 0.3050 Accuracy 0.3299\n",
            "Epoch 54 Batch 0 Loss 0.2637 Accuracy 0.3341\n",
            "Epoch 54 Batch 50 Loss 0.2984 Accuracy 0.3304\n",
            "Epoch 54 Loss 0.3037 Accuracy 0.3298\n",
            "Epoch 55 Batch 0 Loss 0.2488 Accuracy 0.3355\n",
            "Epoch 55 Batch 50 Loss 0.2965 Accuracy 0.3306\n",
            "Epoch 55 Loss 0.3026 Accuracy 0.3300\n",
            "Epoch 56 Batch 0 Loss 0.2545 Accuracy 0.3352\n",
            "Epoch 56 Batch 50 Loss 0.2932 Accuracy 0.3308\n",
            "Epoch 56 Loss 0.2984 Accuracy 0.3303\n",
            "Epoch 57 Batch 0 Loss 0.2475 Accuracy 0.3359\n",
            "Epoch 57 Batch 50 Loss 0.2926 Accuracy 0.3310\n",
            "Epoch 57 Loss 0.2989 Accuracy 0.3302\n",
            "Epoch 58 Batch 0 Loss 0.2468 Accuracy 0.3427\n",
            "Epoch 58 Batch 50 Loss 0.2897 Accuracy 0.3313\n",
            "Epoch 58 Loss 0.2957 Accuracy 0.3306\n",
            "Epoch 59 Batch 0 Loss 0.2430 Accuracy 0.3379\n",
            "Epoch 59 Batch 50 Loss 0.2881 Accuracy 0.3310\n",
            "Epoch 59 Loss 0.2947 Accuracy 0.3304\n",
            "Epoch 60 Batch 0 Loss 0.2294 Accuracy 0.3389\n",
            "Epoch 60 Batch 50 Loss 0.2879 Accuracy 0.3309\n",
            "Epoch 60 Loss 0.2924 Accuracy 0.3306\n",
            "Epoch 61 Batch 0 Loss 0.2345 Accuracy 0.3421\n",
            "Epoch 61 Batch 50 Loss 0.2844 Accuracy 0.3316\n",
            "Epoch 61 Loss 0.2905 Accuracy 0.3306\n",
            "Epoch 62 Batch 0 Loss 0.2433 Accuracy 0.3388\n",
            "Epoch 62 Batch 50 Loss 0.2854 Accuracy 0.3317\n",
            "Epoch 62 Loss 0.2912 Accuracy 0.3309\n",
            "Epoch 63 Batch 0 Loss 0.2373 Accuracy 0.3374\n",
            "Epoch 63 Batch 50 Loss 0.2852 Accuracy 0.3317\n",
            "Epoch 63 Loss 0.2899 Accuracy 0.3308\n",
            "Epoch 64 Batch 0 Loss 0.2631 Accuracy 0.3290\n",
            "Epoch 64 Batch 50 Loss 0.2814 Accuracy 0.3316\n",
            "Epoch 64 Loss 0.2864 Accuracy 0.3310\n",
            "Epoch 65 Batch 0 Loss 0.2419 Accuracy 0.3374\n",
            "Epoch 65 Batch 50 Loss 0.2803 Accuracy 0.3317\n",
            "Epoch 65 Loss 0.2860 Accuracy 0.3307\n",
            "Epoch 66 Batch 0 Loss 0.2250 Accuracy 0.3362\n",
            "Epoch 66 Batch 50 Loss 0.2790 Accuracy 0.3317\n",
            "Epoch 66 Loss 0.2853 Accuracy 0.3310\n",
            "Epoch 67 Batch 0 Loss 0.2297 Accuracy 0.3447\n",
            "Epoch 67 Batch 50 Loss 0.2780 Accuracy 0.3320\n",
            "Epoch 67 Loss 0.2829 Accuracy 0.3312\n",
            "Epoch 68 Batch 0 Loss 0.2338 Accuracy 0.3350\n",
            "Epoch 68 Batch 50 Loss 0.2763 Accuracy 0.3317\n",
            "Epoch 68 Loss 0.2817 Accuracy 0.3311\n",
            "Epoch 69 Batch 0 Loss 0.2342 Accuracy 0.3410\n",
            "Epoch 69 Batch 50 Loss 0.2758 Accuracy 0.3318\n",
            "Epoch 69 Loss 0.2810 Accuracy 0.3309\n",
            "Epoch 70 Batch 0 Loss 0.2386 Accuracy 0.3395\n",
            "Epoch 70 Batch 50 Loss 0.2746 Accuracy 0.3322\n",
            "Epoch 70 Loss 0.2801 Accuracy 0.3315\n",
            "Epoch 71 Batch 0 Loss 0.2210 Accuracy 0.3392\n",
            "Epoch 71 Batch 50 Loss 0.2752 Accuracy 0.3318\n",
            "Epoch 71 Loss 0.2805 Accuracy 0.3311\n",
            "Epoch 72 Batch 0 Loss 0.2430 Accuracy 0.3358\n",
            "Epoch 72 Batch 50 Loss 0.2714 Accuracy 0.3321\n",
            "Epoch 72 Loss 0.2770 Accuracy 0.3313\n",
            "Epoch 73 Batch 0 Loss 0.2367 Accuracy 0.3409\n",
            "Epoch 73 Batch 50 Loss 0.2699 Accuracy 0.3321\n",
            "Epoch 73 Loss 0.2749 Accuracy 0.3315\n",
            "Epoch 74 Batch 0 Loss 0.2366 Accuracy 0.3377\n",
            "Epoch 74 Batch 50 Loss 0.2708 Accuracy 0.3320\n",
            "Epoch 74 Loss 0.2760 Accuracy 0.3314\n",
            "Epoch 75 Batch 0 Loss 0.2195 Accuracy 0.3409\n",
            "Epoch 75 Batch 50 Loss 0.2665 Accuracy 0.3327\n",
            "Epoch 75 Loss 0.2724 Accuracy 0.3320\n",
            "Epoch 76 Batch 0 Loss 0.2471 Accuracy 0.3319\n",
            "Epoch 76 Batch 50 Loss 0.2669 Accuracy 0.3325\n",
            "Epoch 76 Loss 0.2714 Accuracy 0.3318\n",
            "Epoch 77 Batch 0 Loss 0.2543 Accuracy 0.3328\n",
            "Epoch 77 Batch 50 Loss 0.2677 Accuracy 0.3324\n",
            "Epoch 77 Loss 0.2727 Accuracy 0.3316\n",
            "Epoch 78 Batch 0 Loss 0.2208 Accuracy 0.3415\n",
            "Epoch 78 Batch 50 Loss 0.2659 Accuracy 0.3324\n",
            "Epoch 78 Loss 0.2713 Accuracy 0.3317\n",
            "Epoch 79 Batch 0 Loss 0.2378 Accuracy 0.3361\n",
            "Epoch 79 Batch 50 Loss 0.2659 Accuracy 0.3322\n",
            "Epoch 79 Loss 0.2705 Accuracy 0.3318\n",
            "Epoch 80 Batch 0 Loss 0.2442 Accuracy 0.3323\n",
            "Epoch 80 Batch 50 Loss 0.2634 Accuracy 0.3330\n",
            "Epoch 80 Loss 0.2687 Accuracy 0.3321\n",
            "Epoch 81 Batch 0 Loss 0.2260 Accuracy 0.3401\n",
            "Epoch 81 Batch 50 Loss 0.2629 Accuracy 0.3329\n",
            "Epoch 81 Loss 0.2678 Accuracy 0.3320\n",
            "Epoch 82 Batch 0 Loss 0.2155 Accuracy 0.3361\n",
            "Epoch 82 Batch 50 Loss 0.2613 Accuracy 0.3325\n",
            "Epoch 82 Loss 0.2655 Accuracy 0.3321\n",
            "Epoch 83 Batch 0 Loss 0.2340 Accuracy 0.3410\n",
            "Epoch 83 Batch 50 Loss 0.2623 Accuracy 0.3326\n",
            "Epoch 83 Loss 0.2680 Accuracy 0.3317\n",
            "Epoch 84 Batch 0 Loss 0.2286 Accuracy 0.3356\n",
            "Epoch 84 Batch 50 Loss 0.2593 Accuracy 0.3329\n",
            "Epoch 84 Loss 0.2642 Accuracy 0.3323\n",
            "Epoch 85 Batch 0 Loss 0.2200 Accuracy 0.3416\n",
            "Epoch 85 Batch 50 Loss 0.2577 Accuracy 0.3328\n",
            "Epoch 85 Loss 0.2624 Accuracy 0.3322\n",
            "Epoch 86 Batch 0 Loss 0.2299 Accuracy 0.3374\n",
            "Epoch 86 Batch 50 Loss 0.2596 Accuracy 0.3330\n",
            "Epoch 86 Loss 0.2625 Accuracy 0.3326\n",
            "Epoch 87 Batch 0 Loss 0.2254 Accuracy 0.3398\n",
            "Epoch 87 Batch 50 Loss 0.2584 Accuracy 0.3330\n",
            "Epoch 87 Loss 0.2632 Accuracy 0.3321\n",
            "Epoch 88 Batch 0 Loss 0.2062 Accuracy 0.3385\n",
            "Epoch 88 Batch 50 Loss 0.2561 Accuracy 0.3334\n",
            "Epoch 88 Loss 0.2614 Accuracy 0.3324\n",
            "Epoch 89 Batch 0 Loss 0.2123 Accuracy 0.3421\n",
            "Epoch 89 Batch 50 Loss 0.2553 Accuracy 0.3330\n",
            "Epoch 89 Loss 0.2610 Accuracy 0.3320\n",
            "Epoch 90 Batch 0 Loss 0.2185 Accuracy 0.3382\n",
            "Epoch 90 Batch 50 Loss 0.2551 Accuracy 0.3329\n",
            "Epoch 90 Loss 0.2594 Accuracy 0.3321\n",
            "Epoch 91 Batch 0 Loss 0.2086 Accuracy 0.3428\n",
            "Epoch 91 Batch 50 Loss 0.2556 Accuracy 0.3331\n",
            "Epoch 91 Loss 0.2600 Accuracy 0.3325\n",
            "Epoch 92 Batch 0 Loss 0.2078 Accuracy 0.3391\n",
            "Epoch 92 Batch 50 Loss 0.2550 Accuracy 0.3329\n",
            "Epoch 92 Loss 0.2594 Accuracy 0.3321\n",
            "Epoch 93 Batch 0 Loss 0.2242 Accuracy 0.3401\n",
            "Epoch 93 Batch 50 Loss 0.2525 Accuracy 0.3332\n",
            "Epoch 93 Loss 0.2578 Accuracy 0.3323\n",
            "Epoch 94 Batch 0 Loss 0.2159 Accuracy 0.3379\n",
            "Epoch 94 Batch 50 Loss 0.2533 Accuracy 0.3328\n",
            "Epoch 94 Loss 0.2585 Accuracy 0.3321\n",
            "Epoch 95 Batch 0 Loss 0.2208 Accuracy 0.3395\n",
            "Epoch 95 Batch 50 Loss 0.2513 Accuracy 0.3334\n",
            "Epoch 95 Loss 0.2571 Accuracy 0.3323\n",
            "Epoch 96 Batch 0 Loss 0.2138 Accuracy 0.3404\n",
            "Epoch 96 Batch 50 Loss 0.2494 Accuracy 0.3333\n",
            "Epoch 96 Loss 0.2560 Accuracy 0.3324\n",
            "Epoch 97 Batch 0 Loss 0.2401 Accuracy 0.3355\n",
            "Epoch 97 Batch 50 Loss 0.2518 Accuracy 0.3328\n",
            "Epoch 97 Loss 0.2563 Accuracy 0.3323\n",
            "Epoch 98 Batch 0 Loss 0.2200 Accuracy 0.3374\n",
            "Epoch 98 Batch 50 Loss 0.2504 Accuracy 0.3332\n",
            "Epoch 98 Loss 0.2551 Accuracy 0.3325\n",
            "Epoch 99 Batch 0 Loss 0.2158 Accuracy 0.3385\n",
            "Epoch 99 Batch 50 Loss 0.2477 Accuracy 0.3338\n",
            "Epoch 99 Loss 0.2529 Accuracy 0.3328\n",
            "Epoch 100 Batch 0 Loss 0.2132 Accuracy 0.3395\n",
            "Epoch 100 Batch 50 Loss 0.2474 Accuracy 0.3332\n",
            "Epoch 100 Loss 0.2527 Accuracy 0.3325\n",
            "Epoch 101 Batch 0 Loss 0.2162 Accuracy 0.3317\n",
            "Epoch 101 Batch 50 Loss 0.2482 Accuracy 0.3332\n",
            "Epoch 101 Loss 0.2522 Accuracy 0.3327\n",
            "Epoch 102 Batch 0 Loss 0.2135 Accuracy 0.3389\n",
            "Epoch 102 Batch 50 Loss 0.2466 Accuracy 0.3335\n",
            "Epoch 102 Loss 0.2516 Accuracy 0.3326\n",
            "Epoch 103 Batch 0 Loss 0.1908 Accuracy 0.3472\n",
            "Epoch 103 Batch 50 Loss 0.2463 Accuracy 0.3338\n",
            "Epoch 103 Loss 0.2519 Accuracy 0.3327\n",
            "Epoch 104 Batch 0 Loss 0.2077 Accuracy 0.3418\n",
            "Epoch 104 Batch 50 Loss 0.2474 Accuracy 0.3335\n",
            "Epoch 104 Loss 0.2510 Accuracy 0.3330\n",
            "Epoch 105 Batch 0 Loss 0.2047 Accuracy 0.3424\n",
            "Epoch 105 Batch 50 Loss 0.2458 Accuracy 0.3333\n",
            "Epoch 105 Loss 0.2506 Accuracy 0.3327\n",
            "Epoch 106 Batch 0 Loss 0.2147 Accuracy 0.3377\n",
            "Epoch 106 Batch 50 Loss 0.2468 Accuracy 0.3331\n",
            "Epoch 106 Loss 0.2511 Accuracy 0.3325\n",
            "Epoch 107 Batch 0 Loss 0.1909 Accuracy 0.3474\n",
            "Epoch 107 Batch 50 Loss 0.2448 Accuracy 0.3333\n",
            "Epoch 107 Loss 0.2496 Accuracy 0.3326\n",
            "Epoch 108 Batch 0 Loss 0.2072 Accuracy 0.3371\n",
            "Epoch 108 Batch 50 Loss 0.2430 Accuracy 0.3334\n",
            "Epoch 108 Loss 0.2481 Accuracy 0.3327\n",
            "Epoch 109 Batch 0 Loss 0.2052 Accuracy 0.3397\n",
            "Epoch 109 Batch 50 Loss 0.2412 Accuracy 0.3341\n",
            "Epoch 109 Loss 0.2466 Accuracy 0.3331\n",
            "Epoch 110 Batch 0 Loss 0.2071 Accuracy 0.3365\n",
            "Epoch 110 Batch 50 Loss 0.2443 Accuracy 0.3335\n",
            "Epoch 110 Loss 0.2489 Accuracy 0.3325\n",
            "Epoch 111 Batch 0 Loss 0.1994 Accuracy 0.3459\n",
            "Epoch 111 Batch 50 Loss 0.2425 Accuracy 0.3338\n",
            "Epoch 111 Loss 0.2472 Accuracy 0.3330\n",
            "Epoch 112 Batch 0 Loss 0.2092 Accuracy 0.3404\n",
            "Epoch 112 Batch 50 Loss 0.2413 Accuracy 0.3336\n",
            "Epoch 112 Loss 0.2456 Accuracy 0.3329\n",
            "Epoch 113 Batch 0 Loss 0.1964 Accuracy 0.3460\n",
            "Epoch 113 Batch 50 Loss 0.2404 Accuracy 0.3340\n",
            "Epoch 113 Loss 0.2449 Accuracy 0.3331\n",
            "Epoch 114 Batch 0 Loss 0.2161 Accuracy 0.3389\n",
            "Epoch 114 Batch 50 Loss 0.2410 Accuracy 0.3337\n",
            "Epoch 114 Loss 0.2456 Accuracy 0.3329\n",
            "Epoch 115 Batch 0 Loss 0.2068 Accuracy 0.3368\n",
            "Epoch 115 Batch 50 Loss 0.2403 Accuracy 0.3336\n",
            "Epoch 115 Loss 0.2452 Accuracy 0.3327\n",
            "Epoch 116 Batch 0 Loss 0.2054 Accuracy 0.3398\n",
            "Epoch 116 Batch 50 Loss 0.2399 Accuracy 0.3337\n",
            "Epoch 116 Loss 0.2444 Accuracy 0.3328\n",
            "Epoch 117 Batch 0 Loss 0.2126 Accuracy 0.3379\n",
            "Epoch 117 Batch 50 Loss 0.2405 Accuracy 0.3332\n",
            "Epoch 117 Loss 0.2442 Accuracy 0.3327\n",
            "Epoch 118 Batch 0 Loss 0.2050 Accuracy 0.3391\n",
            "Epoch 118 Batch 50 Loss 0.2384 Accuracy 0.3335\n",
            "Epoch 118 Loss 0.2425 Accuracy 0.3332\n",
            "Epoch 119 Batch 0 Loss 0.2061 Accuracy 0.3404\n",
            "Epoch 119 Batch 50 Loss 0.2378 Accuracy 0.3335\n",
            "Epoch 119 Loss 0.2417 Accuracy 0.3330\n",
            "Epoch 120 Batch 0 Loss 0.2107 Accuracy 0.3364\n",
            "Epoch 120 Batch 50 Loss 0.2390 Accuracy 0.3334\n",
            "Epoch 120 Loss 0.2423 Accuracy 0.3326\n",
            "Epoch 121 Batch 0 Loss 0.2237 Accuracy 0.3391\n",
            "Epoch 121 Batch 50 Loss 0.2396 Accuracy 0.3336\n",
            "Epoch 121 Loss 0.2435 Accuracy 0.3329\n",
            "Epoch 122 Batch 0 Loss 0.2052 Accuracy 0.3398\n",
            "Epoch 122 Batch 50 Loss 0.2362 Accuracy 0.3343\n",
            "Epoch 122 Loss 0.2404 Accuracy 0.3333\n",
            "Epoch 123 Batch 0 Loss 0.2068 Accuracy 0.3391\n",
            "Epoch 123 Batch 50 Loss 0.2380 Accuracy 0.3333\n",
            "Epoch 123 Loss 0.2419 Accuracy 0.3328\n",
            "Epoch 124 Batch 0 Loss 0.2131 Accuracy 0.3389\n",
            "Epoch 124 Batch 50 Loss 0.2358 Accuracy 0.3336\n",
            "Epoch 124 Loss 0.2406 Accuracy 0.3328\n",
            "Epoch 125 Batch 0 Loss 0.2025 Accuracy 0.3364\n",
            "Epoch 125 Batch 50 Loss 0.2351 Accuracy 0.3338\n",
            "Epoch 125 Loss 0.2403 Accuracy 0.3331\n",
            "Epoch 126 Batch 0 Loss 0.2023 Accuracy 0.3434\n",
            "Epoch 126 Batch 50 Loss 0.2355 Accuracy 0.3336\n",
            "Epoch 126 Loss 0.2401 Accuracy 0.3331\n",
            "Epoch 127 Batch 0 Loss 0.1954 Accuracy 0.3412\n",
            "Epoch 127 Batch 50 Loss 0.2354 Accuracy 0.3337\n",
            "Epoch 127 Loss 0.2391 Accuracy 0.3331\n",
            "Epoch 128 Batch 0 Loss 0.1944 Accuracy 0.3439\n",
            "Epoch 128 Batch 50 Loss 0.2364 Accuracy 0.3333\n",
            "Epoch 128 Loss 0.2402 Accuracy 0.3327\n",
            "Epoch 129 Batch 0 Loss 0.2017 Accuracy 0.3418\n",
            "Epoch 129 Batch 50 Loss 0.2346 Accuracy 0.3336\n",
            "Epoch 129 Loss 0.2390 Accuracy 0.3329\n",
            "Epoch 130 Batch 0 Loss 0.1938 Accuracy 0.3442\n",
            "Epoch 130 Batch 50 Loss 0.2341 Accuracy 0.3340\n",
            "Epoch 130 Loss 0.2375 Accuracy 0.3333\n",
            "Epoch 131 Batch 0 Loss 0.1912 Accuracy 0.3433\n",
            "Epoch 131 Batch 50 Loss 0.2339 Accuracy 0.3335\n",
            "Epoch 131 Loss 0.2374 Accuracy 0.3329\n",
            "Epoch 132 Batch 0 Loss 0.2265 Accuracy 0.3313\n",
            "Epoch 132 Batch 50 Loss 0.2340 Accuracy 0.3336\n",
            "Epoch 132 Loss 0.2383 Accuracy 0.3327\n",
            "Epoch 133 Batch 0 Loss 0.2031 Accuracy 0.3398\n",
            "Epoch 133 Batch 50 Loss 0.2326 Accuracy 0.3338\n",
            "Epoch 133 Loss 0.2371 Accuracy 0.3331\n",
            "Epoch 134 Batch 0 Loss 0.2051 Accuracy 0.3394\n",
            "Epoch 134 Batch 50 Loss 0.2332 Accuracy 0.3338\n",
            "Epoch 134 Loss 0.2377 Accuracy 0.3330\n",
            "Epoch 135 Batch 0 Loss 0.1901 Accuracy 0.3409\n",
            "Epoch 135 Batch 50 Loss 0.2327 Accuracy 0.3338\n",
            "Epoch 135 Loss 0.2369 Accuracy 0.3331\n",
            "Epoch 136 Batch 0 Loss 0.1991 Accuracy 0.3410\n",
            "Epoch 136 Batch 50 Loss 0.2331 Accuracy 0.3337\n",
            "Epoch 136 Loss 0.2364 Accuracy 0.3330\n",
            "Epoch 137 Batch 0 Loss 0.1964 Accuracy 0.3439\n",
            "Epoch 137 Batch 50 Loss 0.2310 Accuracy 0.3337\n",
            "Epoch 137 Loss 0.2348 Accuracy 0.3333\n",
            "Epoch 138 Batch 0 Loss 0.2026 Accuracy 0.3419\n",
            "Epoch 138 Batch 50 Loss 0.2325 Accuracy 0.3335\n",
            "Epoch 138 Loss 0.2364 Accuracy 0.3330\n",
            "Epoch 139 Batch 0 Loss 0.1899 Accuracy 0.3451\n",
            "Epoch 139 Batch 50 Loss 0.2310 Accuracy 0.3341\n",
            "Epoch 139 Loss 0.2354 Accuracy 0.3332\n",
            "Epoch 140 Batch 0 Loss 0.1960 Accuracy 0.3415\n",
            "Epoch 140 Batch 50 Loss 0.2308 Accuracy 0.3341\n",
            "Epoch 140 Loss 0.2351 Accuracy 0.3332\n",
            "Epoch 141 Batch 0 Loss 0.1894 Accuracy 0.3439\n",
            "Epoch 141 Batch 50 Loss 0.2301 Accuracy 0.3340\n",
            "Epoch 141 Loss 0.2343 Accuracy 0.3333\n",
            "Epoch 142 Batch 0 Loss 0.1997 Accuracy 0.3362\n",
            "Epoch 142 Batch 50 Loss 0.2295 Accuracy 0.3337\n",
            "Epoch 142 Loss 0.2339 Accuracy 0.3329\n",
            "Epoch 143 Batch 0 Loss 0.2133 Accuracy 0.3367\n",
            "Epoch 143 Batch 50 Loss 0.2278 Accuracy 0.3339\n",
            "Epoch 143 Loss 0.2331 Accuracy 0.3331\n",
            "Epoch 144 Batch 0 Loss 0.1934 Accuracy 0.3374\n",
            "Epoch 144 Batch 50 Loss 0.2286 Accuracy 0.3339\n",
            "Epoch 144 Loss 0.2331 Accuracy 0.3330\n",
            "Epoch 145 Batch 0 Loss 0.2118 Accuracy 0.3353\n",
            "Epoch 145 Batch 50 Loss 0.2282 Accuracy 0.3340\n",
            "Epoch 145 Loss 0.2334 Accuracy 0.3331\n",
            "Epoch 146 Batch 0 Loss 0.1952 Accuracy 0.3382\n",
            "Epoch 146 Batch 50 Loss 0.2288 Accuracy 0.3336\n",
            "Epoch 146 Loss 0.2322 Accuracy 0.3331\n",
            "Epoch 147 Batch 0 Loss 0.2071 Accuracy 0.3334\n",
            "Epoch 147 Batch 50 Loss 0.2274 Accuracy 0.3337\n",
            "Epoch 147 Loss 0.2320 Accuracy 0.3331\n",
            "Epoch 148 Batch 0 Loss 0.1994 Accuracy 0.3371\n",
            "Epoch 148 Batch 50 Loss 0.2277 Accuracy 0.3337\n",
            "Epoch 148 Loss 0.2323 Accuracy 0.3331\n",
            "Epoch 149 Batch 0 Loss 0.2096 Accuracy 0.3388\n",
            "Epoch 149 Batch 50 Loss 0.2278 Accuracy 0.3340\n",
            "Epoch 149 Loss 0.2319 Accuracy 0.3333\n",
            "Epoch 150 Batch 0 Loss 0.1855 Accuracy 0.3430\n",
            "Epoch 150 Batch 50 Loss 0.2290 Accuracy 0.3336\n",
            "Epoch 150 Loss 0.2327 Accuracy 0.3330\n",
            "Epoch 151 Batch 0 Loss 0.1920 Accuracy 0.3427\n",
            "Epoch 151 Batch 50 Loss 0.2276 Accuracy 0.3342\n",
            "Epoch 151 Loss 0.2310 Accuracy 0.3335\n",
            "Epoch 152 Batch 0 Loss 0.2090 Accuracy 0.3376\n",
            "Epoch 152 Batch 50 Loss 0.2261 Accuracy 0.3338\n",
            "Epoch 152 Loss 0.2306 Accuracy 0.3330\n",
            "Epoch 153 Batch 0 Loss 0.1928 Accuracy 0.3392\n",
            "Epoch 153 Batch 50 Loss 0.2275 Accuracy 0.3338\n",
            "Epoch 153 Loss 0.2315 Accuracy 0.3330\n",
            "Epoch 154 Batch 0 Loss 0.1904 Accuracy 0.3410\n",
            "Epoch 154 Batch 50 Loss 0.2264 Accuracy 0.3336\n",
            "Epoch 154 Loss 0.2297 Accuracy 0.3331\n",
            "Epoch 155 Batch 0 Loss 0.2001 Accuracy 0.3367\n",
            "Epoch 155 Batch 50 Loss 0.2254 Accuracy 0.3341\n",
            "Epoch 155 Loss 0.2300 Accuracy 0.3333\n",
            "Epoch 156 Batch 0 Loss 0.1939 Accuracy 0.3400\n",
            "Epoch 156 Batch 50 Loss 0.2267 Accuracy 0.3340\n",
            "Epoch 156 Loss 0.2302 Accuracy 0.3332\n",
            "Epoch 157 Batch 0 Loss 0.1919 Accuracy 0.3394\n",
            "Epoch 157 Batch 50 Loss 0.2253 Accuracy 0.3342\n",
            "Epoch 157 Loss 0.2286 Accuracy 0.3334\n",
            "Epoch 158 Batch 0 Loss 0.1869 Accuracy 0.3439\n",
            "Epoch 158 Batch 50 Loss 0.2244 Accuracy 0.3343\n",
            "Epoch 158 Loss 0.2289 Accuracy 0.3334\n",
            "Epoch 159 Batch 0 Loss 0.2007 Accuracy 0.3373\n",
            "Epoch 159 Batch 50 Loss 0.2237 Accuracy 0.3342\n",
            "Epoch 159 Loss 0.2282 Accuracy 0.3332\n",
            "Epoch 160 Batch 0 Loss 0.2034 Accuracy 0.3404\n",
            "Epoch 160 Batch 50 Loss 0.2242 Accuracy 0.3342\n",
            "Epoch 160 Loss 0.2289 Accuracy 0.3331\n",
            "Epoch 161 Batch 0 Loss 0.1940 Accuracy 0.3418\n",
            "Epoch 161 Batch 50 Loss 0.2249 Accuracy 0.3338\n",
            "Epoch 161 Loss 0.2290 Accuracy 0.3332\n",
            "Epoch 162 Batch 0 Loss 0.1859 Accuracy 0.3439\n",
            "Epoch 162 Batch 50 Loss 0.2248 Accuracy 0.3337\n",
            "Epoch 162 Loss 0.2280 Accuracy 0.3332\n",
            "Epoch 163 Batch 0 Loss 0.1932 Accuracy 0.3370\n",
            "Epoch 163 Batch 50 Loss 0.2236 Accuracy 0.3341\n",
            "Epoch 163 Loss 0.2278 Accuracy 0.3332\n",
            "Epoch 164 Batch 0 Loss 0.1876 Accuracy 0.3410\n",
            "Epoch 164 Batch 50 Loss 0.2234 Accuracy 0.3339\n",
            "Epoch 164 Loss 0.2266 Accuracy 0.3333\n",
            "Epoch 165 Batch 0 Loss 0.1885 Accuracy 0.3425\n",
            "Epoch 165 Batch 50 Loss 0.2230 Accuracy 0.3343\n",
            "Epoch 165 Loss 0.2272 Accuracy 0.3334\n",
            "Epoch 166 Batch 0 Loss 0.1968 Accuracy 0.3406\n",
            "Epoch 166 Batch 50 Loss 0.2223 Accuracy 0.3343\n",
            "Epoch 166 Loss 0.2259 Accuracy 0.3334\n",
            "Epoch 167 Batch 0 Loss 0.1877 Accuracy 0.3427\n",
            "Epoch 167 Batch 50 Loss 0.2210 Accuracy 0.3344\n",
            "Epoch 167 Loss 0.2261 Accuracy 0.3333\n",
            "Epoch 168 Batch 0 Loss 0.1889 Accuracy 0.3430\n",
            "Epoch 168 Batch 50 Loss 0.2225 Accuracy 0.3343\n",
            "Epoch 168 Loss 0.2268 Accuracy 0.3336\n",
            "Epoch 169 Batch 0 Loss 0.1950 Accuracy 0.3388\n",
            "Epoch 169 Batch 50 Loss 0.2224 Accuracy 0.3337\n",
            "Epoch 169 Loss 0.2266 Accuracy 0.3331\n",
            "Epoch 170 Batch 0 Loss 0.1845 Accuracy 0.3466\n",
            "Epoch 170 Batch 50 Loss 0.2223 Accuracy 0.3343\n",
            "Epoch 170 Loss 0.2266 Accuracy 0.3331\n",
            "Epoch 171 Batch 0 Loss 0.1789 Accuracy 0.3441\n",
            "Epoch 171 Batch 50 Loss 0.2217 Accuracy 0.3342\n",
            "Epoch 171 Loss 0.2254 Accuracy 0.3334\n",
            "Epoch 172 Batch 0 Loss 0.2082 Accuracy 0.3403\n",
            "Epoch 172 Batch 50 Loss 0.2215 Accuracy 0.3343\n",
            "Epoch 172 Loss 0.2251 Accuracy 0.3334\n",
            "Epoch 173 Batch 0 Loss 0.1962 Accuracy 0.3358\n",
            "Epoch 173 Batch 50 Loss 0.2223 Accuracy 0.3339\n",
            "Epoch 173 Loss 0.2257 Accuracy 0.3333\n",
            "Epoch 174 Batch 0 Loss 0.2048 Accuracy 0.3409\n",
            "Epoch 174 Batch 50 Loss 0.2202 Accuracy 0.3346\n",
            "Epoch 174 Loss 0.2242 Accuracy 0.3335\n",
            "Epoch 175 Batch 0 Loss 0.1873 Accuracy 0.3453\n",
            "Epoch 175 Batch 50 Loss 0.2216 Accuracy 0.3337\n",
            "Epoch 175 Loss 0.2253 Accuracy 0.3331\n",
            "Epoch 176 Batch 0 Loss 0.1873 Accuracy 0.3344\n",
            "Epoch 176 Batch 50 Loss 0.2214 Accuracy 0.3341\n",
            "Epoch 176 Loss 0.2250 Accuracy 0.3334\n",
            "Epoch 177 Batch 0 Loss 0.1857 Accuracy 0.3344\n",
            "Epoch 177 Batch 50 Loss 0.2197 Accuracy 0.3347\n",
            "Epoch 177 Loss 0.2237 Accuracy 0.3338\n",
            "Epoch 178 Batch 0 Loss 0.1862 Accuracy 0.3424\n",
            "Epoch 178 Batch 50 Loss 0.2201 Accuracy 0.3343\n",
            "Epoch 178 Loss 0.2247 Accuracy 0.3335\n",
            "Epoch 179 Batch 0 Loss 0.1801 Accuracy 0.3410\n",
            "Epoch 179 Batch 50 Loss 0.2190 Accuracy 0.3343\n",
            "Epoch 179 Loss 0.2227 Accuracy 0.3336\n",
            "Epoch 180 Batch 0 Loss 0.2015 Accuracy 0.3391\n",
            "Epoch 180 Batch 50 Loss 0.2193 Accuracy 0.3343\n",
            "Epoch 180 Loss 0.2230 Accuracy 0.3335\n",
            "Epoch 181 Batch 0 Loss 0.1897 Accuracy 0.3388\n",
            "Epoch 181 Batch 50 Loss 0.2205 Accuracy 0.3343\n",
            "Epoch 181 Loss 0.2244 Accuracy 0.3332\n",
            "Epoch 182 Batch 0 Loss 0.2043 Accuracy 0.3308\n",
            "Epoch 182 Batch 50 Loss 0.2184 Accuracy 0.3344\n",
            "Epoch 182 Loss 0.2222 Accuracy 0.3335\n",
            "Epoch 183 Batch 0 Loss 0.1946 Accuracy 0.3373\n",
            "Epoch 183 Batch 50 Loss 0.2190 Accuracy 0.3342\n",
            "Epoch 183 Loss 0.2232 Accuracy 0.3333\n",
            "Epoch 184 Batch 0 Loss 0.1954 Accuracy 0.3385\n",
            "Epoch 184 Batch 50 Loss 0.2190 Accuracy 0.3341\n",
            "Epoch 184 Loss 0.2230 Accuracy 0.3335\n",
            "Epoch 185 Batch 0 Loss 0.1773 Accuracy 0.3385\n",
            "Epoch 185 Batch 50 Loss 0.2178 Accuracy 0.3345\n",
            "Epoch 185 Loss 0.2220 Accuracy 0.3334\n",
            "Epoch 186 Batch 0 Loss 0.1874 Accuracy 0.3361\n",
            "Epoch 186 Batch 50 Loss 0.2182 Accuracy 0.3343\n",
            "Epoch 186 Loss 0.2221 Accuracy 0.3334\n",
            "Epoch 187 Batch 0 Loss 0.1816 Accuracy 0.3395\n",
            "Epoch 187 Batch 50 Loss 0.2166 Accuracy 0.3345\n",
            "Epoch 187 Loss 0.2209 Accuracy 0.3335\n",
            "Epoch 188 Batch 0 Loss 0.1978 Accuracy 0.3350\n",
            "Epoch 188 Batch 50 Loss 0.2164 Accuracy 0.3345\n",
            "Epoch 188 Loss 0.2206 Accuracy 0.3337\n",
            "Epoch 189 Batch 0 Loss 0.1812 Accuracy 0.3415\n",
            "Epoch 189 Batch 50 Loss 0.2177 Accuracy 0.3342\n",
            "Epoch 189 Loss 0.2216 Accuracy 0.3334\n",
            "Epoch 190 Batch 0 Loss 0.2062 Accuracy 0.3337\n",
            "Epoch 190 Batch 50 Loss 0.2166 Accuracy 0.3345\n",
            "Epoch 190 Loss 0.2207 Accuracy 0.3335\n",
            "Epoch 191 Batch 0 Loss 0.1955 Accuracy 0.3379\n",
            "Epoch 191 Batch 50 Loss 0.2176 Accuracy 0.3344\n",
            "Epoch 191 Loss 0.2213 Accuracy 0.3335\n",
            "Epoch 192 Batch 0 Loss 0.1888 Accuracy 0.3367\n",
            "Epoch 192 Batch 50 Loss 0.2175 Accuracy 0.3343\n",
            "Epoch 192 Loss 0.2213 Accuracy 0.3336\n",
            "Epoch 193 Batch 0 Loss 0.1958 Accuracy 0.3394\n",
            "Epoch 193 Batch 50 Loss 0.2157 Accuracy 0.3345\n",
            "Epoch 193 Loss 0.2196 Accuracy 0.3337\n",
            "Epoch 194 Batch 0 Loss 0.1928 Accuracy 0.3400\n",
            "Epoch 194 Batch 50 Loss 0.2173 Accuracy 0.3345\n",
            "Epoch 194 Loss 0.2210 Accuracy 0.3335\n",
            "Epoch 195 Batch 0 Loss 0.1938 Accuracy 0.3385\n",
            "Epoch 195 Batch 50 Loss 0.2163 Accuracy 0.3348\n",
            "Epoch 195 Loss 0.2201 Accuracy 0.3335\n",
            "Epoch 196 Batch 0 Loss 0.1968 Accuracy 0.3383\n",
            "Epoch 196 Batch 50 Loss 0.2157 Accuracy 0.3349\n",
            "Epoch 196 Loss 0.2196 Accuracy 0.3339\n",
            "Epoch 197 Batch 0 Loss 0.1955 Accuracy 0.3416\n",
            "Epoch 197 Batch 50 Loss 0.2156 Accuracy 0.3347\n",
            "Epoch 197 Loss 0.2196 Accuracy 0.3338\n",
            "Epoch 198 Batch 0 Loss 0.1927 Accuracy 0.3377\n",
            "Epoch 198 Batch 50 Loss 0.2167 Accuracy 0.3343\n",
            "Epoch 198 Loss 0.2201 Accuracy 0.3336\n",
            "Epoch 199 Batch 0 Loss 0.1948 Accuracy 0.3379\n",
            "Epoch 199 Batch 50 Loss 0.2161 Accuracy 0.3344\n",
            "Epoch 199 Loss 0.2202 Accuracy 0.3335\n",
            "Epoch 200 Batch 0 Loss 0.1708 Accuracy 0.3481\n",
            "Epoch 200 Batch 50 Loss 0.2154 Accuracy 0.3346\n",
            "Epoch 200 Loss 0.2194 Accuracy 0.3337\n",
            "CPU times: user 11min 24s, sys: 1min 52s, total: 13min 16s\n",
            "Wall time: 20min 36s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EShPJcIS0pEe"
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZDenScKnYUQ",
        "outputId": "b4490690-804d-46b1-c577-b02dd09f3833"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    start_token = [1]\n",
        "    end_token = [2]\n",
        "  \n",
        "    sentence = preprocess_sentence(inp_sentence)\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    \n",
        "    encoder_input = tf.expand_dims(inputs, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "    decoder_input = [1]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_length_targ):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "    sentence = inp_lang_tokenizer.encode(sentence)\n",
        "  \n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "\n",
        "        # plot the attention weights\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "        fontdict = {'fontsize': 10}\n",
        "\n",
        "        ax.set_xticks(range(len(sentence)+2))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "\n",
        "        ax.set_ylim(len(result)-1.5, -0.5)\n",
        "\n",
        "        ax.set_xticklabels(\n",
        "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
        "            fontdict=fontdict, rotation=90)\n",
        "\n",
        "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
        "                            if i < tokenizer_en.vocab_size], \n",
        "                           fontdict=fontdict)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def translate(sentence, plot=''):\n",
        "    result, attention_weights = evaluate(sentence)\n",
        "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
        "        \n",
        "translate(\"good morning.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: good morning.\n",
            "Predicted translation: ['<start>', 'доброе', 'утро', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGbH0YwH0qHU",
        "outputId": "4a5128fd-c19a-4615-8a45-120c83a50f0f"
      },
      "source": [
        "translate('how are you?')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: how are you?\n",
            "Predicted translation: ['<start>', 'как', 'живёшь', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuUuuco30wVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da11e7c2-c0ce-4ef3-b980-143f8570d82b"
      },
      "source": [
        "!pip install --upgrade google-cloud-texttospeech"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-texttospeech in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-texttospeech) (21.0)\n",
            "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-texttospeech) (1.26.3)\n",
            "Requirement already satisfied: proto-plus>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-texttospeech) (1.19.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (3.17.3)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (1.35.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (1.53.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (57.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (1.41.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (0.2.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-texttospeech) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-texttospeech) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PoUf5kZ6maS",
        "outputId": "546bcea3-8aeb-41af-bd1f-9e2b24a9adc2"
      },
      "source": [
        "!pip install gTTS"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gTTS in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gTTS) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from gTTS) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gTTS) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8LTG9VC6p8m"
      },
      "source": [
        "import gtts # Импорт библиотеки\n",
        "from gtts import gTTS # Основной интерфейс для перевода текста в аудио\n",
        "from IPython.display import Audio # Используем для проигрывания аудио"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHZwGGcq8ohQ"
      },
      "source": [
        "def translate(sentence, plot=''):\n",
        "    result, attention_weights = evaluate(sentence)\n",
        "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))\n",
        "    text = \" \".join(predicted_sentence[1:])\n",
        "    tts = gTTS(text, lang='ru') # Преобразовываем текст в аудио\n",
        "    tts.save('text.mp3') # Сохраняем в формате мр3\n",
        "\n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdmCIBR886F7",
        "outputId": "ad986a8f-2f77-4440-a110-7b7d778386e8"
      },
      "source": [
        "translate('where are you from?')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: where are you from?\n",
            "Predicted translation: ['<start>', 'где', 'вы', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "zzwf3HBK6wTZ",
        "outputId": "bf09bc45-ead5-4b97-cb1a-3c1fdf7a0052"
      },
      "source": [
        "Audio('text.mp3')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "                <audio controls=\"controls\" >\n",
              "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykKTE_Am6ygt",
        "outputId": "58c6abd4-7d56-4ea8-e1fb-75ff4628cffb"
      },
      "source": [
        "translate('there is a cat on the table')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: there is a cat on the table\n",
            "Predicted translation: ['<start>', 'там', 'кошка', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "RK2hGYSd9Kwp",
        "outputId": "7d2ba7e6-ce5b-4545-860f-a98a9f2f1018"
      },
      "source": [
        "Audio('text.mp3')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "                <audio controls=\"controls\" >\n",
              "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZwPQOHi9MfW",
        "outputId": "58331619-ad43-4e43-d1d8-288cf59c73a7"
      },
      "source": [
        "translate('how are you?')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: how are you?\n",
            "Predicted translation: ['<start>', 'как', 'живёшь', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "id": "QCIfc9GM9alc",
        "outputId": "c2c5b502-0283-413a-9342-94cfe94b9dfa"
      },
      "source": [
        "Audio('text.mp3')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "                <audio controls=\"controls\" >\n",
              "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JZOMiwl9Tvu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}